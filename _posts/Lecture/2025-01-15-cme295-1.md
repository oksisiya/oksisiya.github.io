---
title: "Stanford CME 295: Transformers & Large Language Models"
date: 2026-01-15 09:40:00 +0900
categories: [Lecture]
---

&nbsp;

## NLP Overview

<br>

![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 9.37.06.png)

<br>

NLP (Natural Language Processing)는 텍스트를 다루는 분야로 텍스트를 가지고 계산을 수행한다.

<br>

![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 9.38.26.png)|![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 9.38.46.png)|![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 9.39.00.png)

<br>

NLP 작업들(tasks)은 기본적으로 세 가지 범주(buckets)로 분류할 수 있다.

* Classification (분류)

분류는 입력으로 텍스트가 주어졌을 때 무엇인가를 예측하는(predict) 것이다. 예를 들어 

* Multi Classification (다중 분류)

다중 분류에서도 입력으로 텍스트가 주어지지만 이번에는 하나 이상의 것을 예측한다.










<br>

## Tokenization

<br>

![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 11.13.03.png)|![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 11.20.14.png)

<br>

우리가 하고 싶은 것은 텍스트를 다룰 수 있는 모델을 갖는 것이다. 하지만 모델들은 숫자를 이해한다(they understand numbers). 모델들은 텍스트를 이해하지는 못한다(they don't really understand text). 그래서 우리는 그 텍스트에 대해서 어떤 작업을 해서 모델이 이해할 수 있는 더 정량적인(quantifiable) 형태로 만들어야 한다. 예를 들어 "A cute teddy bear is reading."이라는 문장이 있다. 이 문장을 어떻게 잘라서(cut) 모델에 전달할(pass) 수 있을까? 이 과정을 토큰화(tokenization)이라고 부른다.

<br>

![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 11.20.40.png)|![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 11.20.51.png)|![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 11.21.12.png)|![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 11.21.28.png)

<br>



토큰화는 텍스트를 어떤 임의의 텍스트 단위(some arbitrary unit of text)를 기준으로 자르는 것이다. 토큰화를 수행하는 방법은 여러 가지가 있다. 첫 번째 방법은 완전히 임의적으로(arbitrarily) 자르는 것이다. 예를 들어 여기서는 "a", "cute", "teddy bear"가 텍스트 단위가 될 수 있다. 이러한 텍스트 단위를 토큰(token)이라고 부른다.

또 다른 방법은 그냥 단어 단위로 분리하는(separate) 것이다. 하지만 항상 장단점(always pros and cons)이 존재한다. 우리가 달성하고자(achieve) 하는 목표 중 하나는 이 토큰들을 의미 있는(meaningful) 방식으로 표현할 수 있도록 하는 것이다. 그래서 단어 단위로 이렇게 처리했을 대의 한 가지 단점은 겉보기에는 비슷한 단어들이 실제로는 다른 토큰들로 취급되는 일이 생긴다는 것이다. 그리고 여기서의 한계는 이렇게 비슷하지만 다른 토큰들에 대해 임베딩을 각각 계산해야 하고(compute embeddings for these similar yet different tokens) 그 임베딩들이 서로 비슷해지도록 해야 한다는 것이다(make their embeddings similar). 예를 들어 "bear"라는 단어가 있다고 가정해 보자. 그리고 이것의 복수형(plural form)인 "bears"라는 단어가 있다. 이 두 단어는 매우 비슷하다. 하나는 단수형(sinular)이고 다른 하나는 복수형(plural)일 뿐이다. 만약 우리가 단어 단위(word level) 토큰화를 한다면(go ahead) 이 둘은 그냥 두 개의 다른 개체(entities)로 끝나게 되고(end up) 기본적으로(basically) 완전히 다른 것으로 취급된다(are considered). "run"과 "runs" 같은 동사의 변화형(variations)들도 마찬가지다.

이러한 이유로 사람들은 subword tokenizer를 연구하게 되었다. 서브워드 토큰나이저는 단어의 어근(roots of words)을 활용해서(leveraging) 이러한 단어들 사이에서 공통된 어근이 무엇인지를 찾는 방식이다. 예를 들어 "bear"와 "bears"의 경우 공통으로 사용되는 "bear" 부분(particle)이 있을 것이다. 그래서 장점(pro)은 단어의 어근을 사용할 수 있다는 것이다. 하지만 여기서의 단점(con)은 시퀀스(sequnece)가 더 길어진다는 것이다. 그리고 이것이 왜 단점인지에 대해 보게 될 것이다. 미리 살짝 말하자면 이러한 모델들의 복잡도는(complexity of these models) 시퀀스 길이의 함수(a function of the sequence length)이기도 하다. 그래서 처리해야 할 토큰이 많을수록 모델이 학습하는 데 더 많은 시간이 걸리게 된다. 왜냐하면 기본적으로 이 모든 토큰들을 처리해야 하기 때문이다. 그래서 장점은 단어의 어근을 활용할 수 있다는 것이다. 단점은 시퀀스를 더 길게 만든다는 것이다.

마지막 토큰화 방법은 문자(character) 단위로 처리하는 것이다. 이것은 문자 하나하나를 사용하는 방식이다 (just like taking our characters). 여기서 우리가 문자를 쓸 때 가끔 오타(misspellings)를 낸다는 점을 생각해보자. 서브워드 방식의 토큰화에서는 잘못 철자가 쓰인(misspelled) 단어를 인식하지(recognize) 못할 수도 있다. 이것은 문자 단위 토크나이저(character-level tokenizer)가 고려할 수 있는(take into consideration) 부분이다. 하지만 여기서의 문제는 시퀀스의 길이가 훨씬 훨씬(much much) 더 길어진다는 것이다. 이것은 모델이 이 시퀀스를 처리하는 데 더 많은 시간이 들게 만든다.그리고 또 다른 단점은 각 토큰을 표현하려고 할 때 각각의 문자(a letter)의 표현이 정말로 무엇을 의미하는지 매우 알기 어렵다.예를 들어 문자 "U"의 표현(representation of the letter U)이 무엇을 의미하는지 생각해보라. 이것은 매우 어렵다.

<br>

![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-15 오전 11.21.37.png)

<br>

간단히 정리해 보자(I have just a quick recap). word-level은 매우 순진(super naive)하고 간단한(super simple) 방식으로 텍스트를 임의의 단위(arbitrary units)로 나누는 것이다. 하지만 문제는 우리가 언급했듯이 단어의 어근을 활용하지 못한다는 점이다 (do not leverage the root of words). 그리고 아까 이걸 언급하지 않았는데 우리가 무엇인가를 것을 자르고 나서 추론 시점에(at inference time) 예측을 하고 싶을 때 하나의 전제 조건(a term)이 있다. 하나의 전제조건은(prerequisite) 훈련 시점에 보았던 토큰(the token that you saw at training time)이 훈련 데이터셋 안에 존재해야 한다는 것이다(need to have it in your training sets). 그리고 문제는 추론 시점에(at inference time) 텍스트를 단어 단위로 자른다고 가정할 때 (cut your text into words) 훈련 중에(at training time) 한번도 보지 못한 단어가 있다면 그 단어를 알 수 없음(unknown)으로 표시해야 한다는 것이다. 그래서 이것을 OOV(Out Of Vocabulary)라고 부른다. 다행히도, subword-level tokenizer는 이 문제를 완화해준다(mitigates). 그래서 OOV가 발생할 위험이 더 낮아지지만 (a lower risk of OOV), 그래도 여전히 발생할 수는 있다. 그리고 우리가 언급했듯이, 장점 측면에서는 단어의 어근을 활용할 수 있다. 그리고 character-level은 오타(misspellings)나 대소문자 오류(casing errors)에 강하다. 하지만 문제는 계산이 훨씬 느려진다는 점이다. 그리고 시퀀스가 매우 매우 길어지게 되는데 이것은 또한 추론 시간도 훨씬 더 길어지게 만든다. 이것이 텍스트를 다루는 방법(how to handle things with text)에 대한 정말 기초적인 토대(really the foundation)이다.

그래서 이제 우리가 한 것은 입력 텍스트(input text)를 가져와서 그것을 토큰이라고 하는 여러 부분들로 나눈 것(cut it into parts)이다. 그래서 우리의 모델이 이 토큰들을 이해할 수 있도록 하려면, 우리는 각 토큰에 대한 표현(representation)을 찾아야 한다. 이것을 단어 표현(word representation)이라고 부르거나 또는 좀 더 정확하게 말하자면 토큰 표현(token representation)이라고 부른다. 그래서 우리가 원하는 것은 이 각각의 토큰들을 표현할 방법을 찾는 것이다.

<br>

## Word Representation

<br>

![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-19 오전 9.07.18.png)

<br>

![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-19 오전 9.07.30.png)

<br>

그래서 이것을 하는 가장 단순하고 순진한 방법(simple and naive way)은 각 단어(또는 토큰)에 대해 원-핫 벡터(one hot vector)를 할당하는(assign) 것이다. 예를 들어 세 개의 토큰 book, soft, 그리고 teddy bears 으로 이루어진 어휘(vocabulary)가 있다고 가정해 보자. 그러면 예를 들어 soft는 1, 0, 0 벡터가 될 것이다. teddy bear는 0, 1, 0 벡터가 될 것이다. book은 0, 0, 1 벡터가 될 것이다. 그래서 이것을 원핫인코딩(One-hot encoding, OHE)이라고 부른다. 이것은 토큰을 표현하는 하나의 방법이다. 하지만 사람들이 실제로 하고 싶은 것은 이러한 토큰들을 비교해서 어떤 것들이 다른 것들과 더 비슷한지를 보는(see) 것이다. 그래서 사람들이 자주 사용하는 유사도 측정 방법(common similarity measure)은 코사인 유사도(cosine similarity)라고 불리는 것이다. 이것은 n차원 공간(n dimensional space)에서 이 벡터들이 이루는 각도를 보는 것이라고 생각할 수 있다. 그리고 만약 이 벡터들이 같은 방향을 가리키고 있다면 그것들은 아마도 유사할(similar) 것이다. 만약 서로 직교한다면(orthogonal) 아마도 서로 독립적일(independent) 것이다. 그리고 만약 완전히 반대 방향이라면(completely opposite) 아마도 서로 반대일(opposite) 것이다. 이것이 우리가 갖고 싶은(go into) 기본적인 직관적 모델(mental model)이다. 그런데 문제는 만약 토큰을 원핫 방식으로(in a one hot fashion) 표현한다면 모든 벡터들이 서로 직교하게(orthogonal to one another) 된다는 점이다. 그래서 이상적으로는(So ideally) 우리는 같거나 유사한 의미를 가지는(mean the same or similar) 토큰들은 높은 유사도(high similarity)를 가지도록 하고 싶다. 그리고 서로 비슷하지 않은 토큰들 즉 전혀 다른 것에 관한 토큰들은 더 직교에 가깝게 만들고 싶다. 그래서 여기서 설명을 위한 예로, "Teddy bears are soft." 그래서 teddy bears와 soft는 높은 유사도를 가지도록 하고 싶다. 그리고 예를 들어 teddy bear와 book처럼 서로 독립적인 것들은 0에 가까운 값을 가지기를 원한다. 그래서 이것이 우리가 원하는 것이다(so that's what you want). 이것이(화살표 기준 왼쪽) 원핫 인코딩(one-hot encoding)에서 얻는 결과이다. 그리고 이것이(화살표 기준 오른쪽) 우리가 원하는 결과이다.

<br>

![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-19 오전 9.07.44.png)

<br>

![Screenshot](/assets/img/2026-01-15/스크린샷 2026-01-19 오전 9.34.06.png)

<br>

그래서 이제 원핫인코딩이 토큰을 표현하는 좋은 방법이 아니라는 것을 알게 되었다. 우리가 하고 싶은 것은 데이터로부터 이러한 임베딩들을 학습하는 것이다. 2010년대에 나온 논문이 하나 있는데 - 아마 2013년이었을 것이다. - 그 논문의 이름은 Word2Vec이다. 그리고 이것이 그렇게 인기 있었던 이유는 임베딩을 매우 직관적이고(intuitive) 해석 가능한(interpretable) 방식으로 보여주었기 때문이다. 왜냐하면 그들은 다음과 같은 식의 말을 했기 때문이다. king이 queen에게 대응되는 방식은, Paris가 France에 대응되는 방식과 같고, Berlin이 Germany에게 대응되는 방식과 같다. 그래서 임베딩을 이해할 수 있는 방법이 기본적으로(basically) 존재했던 것이다. 그래서 이제 질문은 이것이다. 그들은 이것을 어떻게 했을까(how did they do that)? 그들은 이 임베딩들을 계산하는 두 가지 방법을 제시했다. 한 가지 방법은 연속적 단어 묶음(continuous bag of words)이라고 불렸다. 다른 하나는 스킵 그랩(skip-gram)이라고 불렸다. 하지만 두 방법 모두 같은 아이디어에 기반한다(rely on). 문맥을 바탕으로(based on context) 우리가 가진 텍스트를 활용해서(leverage texts that we have) 그 텍스트의 일부를 예측해보자는(try to predict something that is part of the text) 것이다. 예를 들어 continuous bag of words의 경우 주어진 목표 단어(target word) 주변에 있는 단어들을 고려한다. 그리고 목표는 그 목표 단어를 예측하는 것이다. 그리고 스킵 그램은 그 반대이다. 목표 단어에서 출발해서 그 주변에 있는 단어들을 예측하려고 한다. 그래서 이 작업은 보통(commonly) 프록시 작업(proxy task)이라고 불린다. 왜냐하면 우리가 진짜로 관심 있는 것은 다음 단어를 예측하는 것 자체가 아니기 때문이다, 적어도 아직은 그렇다. 우리의 목표는 의미 있는 단어 표현을 학습하는 것이다(learn a representation of these words that are meaningful). 그래서 여기서의 아이디어는 만약 어떤 모델이 다음 단어를 예측할 수 있다면(somehow knows how to predict the next word) 그것은 그 모델이 언어가 어떻게 작동하는지에 대해 어느 정도 이해하고 있다는(has some understanding of how language works) 뜻이다. 그리고 그것이 우리가 원하는 바이다. 우리는 언어의 구조를 반영하는(reflective of what language is) 임베딩을 원한다. 즉 king과 queen의 관계라든지, Paris와 France의 관계처럼 이것이 수도(capital)라는 관계 말이다. 우리는 이런 연관성들이 표현 안에 내재되어 있기를 원한다(want to have these associations embedded in the representation).

<br>

그리고 이것이 어떤 모습인지(what that looks like)에 대한 아주 간단한 예제를 하나 살펴보자. 그래서 여기 예제에서는 프록시 작업이 다음 단어를 예측하는 것이라고 가정해 보자. 여기서 우리가 사용하는 것은 아주 기본적인(vanilla) 신경망 모델이다. 이 모델은 크기 v의 벡터(a vector of size v)를 입력으로 받고, 몇 번의 곱셈(some multiplication)과 바이어스 항(bias term)을 거쳐 히든 상태를 얻는다. 그리고 또다른 곱셈을 거쳐(another set of multiplications) 최종 벡터(final vector)를 얻는다. 그래서 이것은 매우 단순한 신경망이다. 입력은 크기 v를 가진다(So the input is of size v). 히든 레이어는 크기 d를 가지는데, 이 값은 보통 어휘 크기보다 훨씬 작다(which is typically much smaller thatn the vocabulary). 어휘(vocabulary)는 보통 수만 개(tens of thousands) 또는 수십만(hundreds of thousands) 개 정도이다. 그래서 d는 보통 수백(hundreds) (예를 들면 768) 차원이다. 그래서 이것은 훨씬 훨씬 작은 차원이다. 그래서 우리가 하려는 것은 이 프록시 작업을 통해 단어 표현을 학습하는 것(learn the word representation through this proxy task)이다. 그리고 우리가 할 일은 단어를 입력으로(words as inputs) 받아서 다음 단어를 예측하는 것이다. 그래서 시퀀스의 첫 번째 단어부터 가보자(let's go with the first word of the sequence). 참고로 나는 토큰(token)과 단어(word)를 서로 바꿔서(interchangeably) 사용하고 있다.

그래서 "a"라는 단어가 있다고 가정하고 그 다음 단어인 "cute"를 예측하고 싶다고 하자. 그래서 우리가 하는 것은 단어 "a"를 가져와서(take) 그것의 원핫 인코딩 표현(one-hot encoding representation)을 사용해서(take) 네트워크에 넣는 것(pass it through the network)이다. 그래서 여기서 행렬과 벡터 사이의 곱셈(a multiplication between a matrix and this vector)이 일어난다. 그래서 크기 d의 히든 상태 표현(hidden state representation)을 얻게 된다. 여기서 예를 들어 0.2와 0.9라고 가정하자. 즉, d는 2이다(D=2). 그리고 다시 한번 다음 단계를 거친다. 그리고 소프트맥스(softmax) 이후에 다음 단어가 무엇일지에 대한 확률들의 집합(a set of probabilities which are around seeing what is the next word)을 얻게 된다. 이 예제에서 어휘 크기가 6(a vocabulary of size 6)이라고 가정하자. 그래서 첫 번째 단어는 확률 0.2로 예측되고, 두 번째 단어는 0.4, 그리고 나머지 단어들은 모두 0.1이다. 그래서 우리가 원하는 것은 두 번째 단어, 즉 0.4에 해당하는 단어의 확률을 최대화하는 것이다(So let's suppose that we want to somehow be able to maximize our prediction to be the second word of the vocabulary, which is the 0.4). 그래서 우리는 예측 결과를 어휘의 두 번째 단어에 대한 표현(representation of the second word of the vocabulary)인 0, 1, 0, 0, 0 벡터와 비교한다. 그리고나서 역전파(backpropagation)를 수행하고 가중치를 업데이트한다. 핵심 아이이디어는 예측을 얻은 다음에(once you obtain a prediction), 손실을 계산한다(compute the loss). 보통은 정답과 얼마나 차이가 나는지를 측정하는 손실인 크로스 엔트로피를 사용한다 (typically cross-entropy, which will determine how far off you are from the true answer). 그리고 그 차이를 기반으로 예측이 정답에 더 가까워지도록 가중치를 업데이트한다. 그리고 이러한 과정을 반복한다. 이제 "cute"라는 단어를 가져가 보자(Let's suppose you take the word "cute" as we said, is the second word in the vocabulary). 이 단어는 어휘에서 두 번째 단어라고 하자. 그래서 원핫 인코딩은 0, 1, 0, 0, 0이다. 이것을 네트워크에 넣으면(이것이 네트워크를 지나면?? so you go through that network)  예를 들어 0.8과 0.4 벡터인 히든 상태(hidden state, like the vector is 0.8 and 0.4)가 나온다. 그리고 다시 같은 과정을 반복한다. 그리고 우리가 하고 싶은 것은 다음 토큰을 예측하는 것이다. 여기서는 "teddy bears"이다. 이 예제에서 모델은 다음 단어를 균등하게 예측하고 있지만 우리는 teddy bear에 대한 확률을 최대화하고 싶다 (And so you see now your model in this example is predicting the next word to be uniform, but you want to somehow maximize the probability for teddy bear). 그래서 이 과정을 모든 단어에 대해 계속 반복한다. 그리고 결국 다음 단어를 예측하는 방법 (이것이 바로 프록시 작업이다) 을 학습한 모델을 얻게 된다 obtain a model that learns how to predict the next word which is basically the proxy task. 그리고 우리가 실제로 할 일은 모델이 학습한 이 표현 (초록색으로 표시된 유닛들)을 가져오는 것이다 take the representation that the model learns, which is the green units. 그래서 지금 무슨 일이 일어나느냐 하면(So what happens now is) 어떤 단어가 주어지면 그 단어를 원핫인코딩으로 표현하고 이것을 가중치와 곱해서 초록색 표현(green representation)을 얻는다. 그리고 이것이 바로 단어 표현(word representation)이다.

<br>

그래서 이제 우리가 한 것은 토큰의 표현을 어떻게 학습할 수 있는지(how we could learn representations of tokens)를 본 것이다. 하지만 문장이나 텍스트 조각 전체에 대한 표현(representations of sentences or pieces of text)을 얻고 싶을 수도 있다. 그래서 우리가 앞에서 본 방법을 사용해서 아주 순진한 방식으로 이를 수행하자면 단어 표현들의 평균을 취하는(take something like the average of the word representations) 방법이 있다. 하지만 문제는 이렇게 하면 많은 의미를 잃게 된다(lose a lot of meaning). 순서(order)를 잃는다. 우리가 학습한 표현들은 어디에 위치해 있든 상관없이(regardless of where they're at) 토큰 자체에만 의존한다(token-specific). 이런 이유로 텍스트가 등장하는 순차적인(sequential) 특성을 포착하려는(aim at capturing the sequential nature of how text appears) 모델들의 한 부류(a class of models)가 존재한다. 그래서 우리는 RNN(Recurrent Neural Network)에 대해 이야기 할 것이다.

<br>

그래서 RNNs이 하는 일은 단어들을 한 번에 처리하는 대신에(instead of processing words one at a time) 지금까지의 문장을 숨겨진 표현(hidden representation of the sentence so far)으로 계속 유지하면서 토큰을 하나씩 고려하는 것이다(consider tokens one at a time). 앞에서 말했듯이 이 기법은 사실 꽤 오래 전인 1980년대에 도입되었다(introduced). 이 모델이 하는 것은 단어들이 또는 토큰들이 어떤 순서로 등장했는지를 고려(takes into consideration the order at which words appeared or tokens appeared)한다는 점이다. 그래서 이 예제에서는 문장의 아주 처음부터 처리하는 것을 시작한다. 어떤 dummy hidden states가 있는데 보통 A 또는 H로 표기한다. 이것은 hidden state, activation, 또는 어떤 경우에는 context vector라고도 부른다. 그리고 어떤 모듈(module)이 있는데 이 모듈은 지금까지의 hidden state와 시간 단계(time step) t에서의 (여기서는 시간 단계 1) 단어를 함께 고려한다. 즉 지금까지의 문장의 의미를 받아들이고 현재 등장한 단어를 함께 고려하는 것이다 takes in the meaning of the sentence so far and takes into consideration the word that is happening now. 그리고 그 결과로 하나의 출력 벡터(output vector)를 만들어내는데 이 벡터는 여기서 다음 단어를 예측하는 데 사용될 수 있다. 예를 들면 여기서 hidden state와 단어의 표현을 가지고 이 파란 박스 안에서 어떤 행렬곱(matrix multipulications)이 이루어진다. 그리고 그 결과로 나온 출력 벡터(output vector)를 이용해서 다음 단어를 예측하도록 훈련시킨다(train). 그 다음에도 같은 과정을 계속 반복하면서 hidden state를 계속 추적해 나간다(keeping track). 즉 이 과정을 반복하는 것이다. 그리고 이러한 hidden state는 지금까지 처리된 시퀀스에 대한 표현이라고 해석할 수 있다. 그래서 RNN의 좋은 점(the good thing with RNNs)은 단어의 순서가 중요해진다는(the word order matters) 것이고 또한 문장을 보다 자연스러운 방식으로 인코딩할 수 있다는(are able to encode the sentence in a more natural way) 점이다.

그래서 대락적으로 이것이 어떻게 동작하는지 보자. "A cute teddy bear is reading." 같은 예제를 사용하자. 먼저 토큰 "A"가 있다. 이 토큰을 원핫 인코딩 벡터로 만든다. 그걸 네트워크에 통과시킨다. hidden state를 계산한다. 그리고 "cute"라는 다음 단어를 예측하려고 한다. 하지만 그러면 hidden state를 계속 저장해둔다 (But then you keep track of the hidden state). 그리고 이 hidden state를 다른 모듈의 입력으로 넣는다. 그 다음에는 다음 단어들도 함께 고려한다. 즉 단순히 현재 단어만 고려하는 것이 아니라 지금까지 문장이 어떻게 흘러왔는지에 대한 hidden state도 함께 고려하는 것이다 consider not only the word itself, but also the hidden state of this sentence so far. 그리고 다시 다음 단어를 예측하고, 또 그 다음 단어를 예측하고, 이런 과정을 반복한다. 이것이 바로 RNN이다.


<br>

## References

[Lecture] <https://www.youtube.com/watch?v=Ub3GoFaUcds>

[Slide] <https://cme295.stanford.edu/slides/fall25-cme295-lecture1.pdf>