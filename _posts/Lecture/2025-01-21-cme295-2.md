---
title: "[Stanford][CME 295] Lecture #2 Transformer-Based Models & Tricks"
date: 2026-01-21 09:20:00 +0900
categories: [Lecture]
use_math: true
---

---

&nbsp;

## Lecture 1 Recap

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오전 10.46.13.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오전 10.46.25.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오전 10.46.35.png)

<br>

1강에서는 셀프 어텐션(self-attention) 개념에 대해 소개했다. 셀프 어텐션이란 각 토큰이 어텐션 메커니즘을 통해 시퀀스 내의 다른 모든 토큰들에 주의를 기울이는 것을 의미한다. 쿼리(query), 키(key), 밸류(value)라는 표기법(notation)을 사용한다. 여기서 핵심은 쿼리와 키를 비교함으로써 쿼리가 자신과 가장 유사한 토큰을 찾는다는 것이다. 그리고 그 과정이 완료되면 해당 토큰과 관련된 밸류를 가져오게 된다.

셀프 어텐션 메커니즘은 다음과 같은 공식으로 나타낼 수 있다. 이 공식은 대규모 행렬 곱(matrix multiplication)인데 하드웨어는 이러한 연산을 처리하는 데 매우 능숙하고(capable) 매우 최적화되어(optimized) 있다.

이 모든 것을 통해 트랜스포머(transformer) 아키텍처를 소개했다. 트랜스포머는 크게 두 가지 구성요소로 이루어져 있다. 인코더(encoder, the left side)와 디코더(decoder, the right side)이다. 트랜스포머는 기계 번역(machine translation) 분야에서 처음 도입되었다. 인코더는 입력 텍스트를 원래 언어(source language)(예를 들면 영어)로 처리하는 역할을 수행한다. 디코더는 번역된 내용(translation)을 대상 언어(target language)(예를 들면 프랑스어)로 해독하는 역할을 수행한다. 멀티 헤드 어텐션(Multi-Head Attention) 레이어에서 셀프 어텐션 메커니즘이 작동한다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 2.02.46.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 2.09.59.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 2.10.10.png)

<br>

멀티 헤드 어텐션에는 여러 개의 헤드(head)가 있다. 그게 무엇을 의미하는 걸까? 트랜스포머 논문(Attention Is All You Need) 속 그림에서 각각의 헤드를 나타내고 있다. 그리고 각 헤드는 모델이 입력을 쿼리, 키, 또는 밸류로 투영하는 한 가지 방법을 학습할 수 있는 기회(an opportunity for the model to learn one way of projecting the input into being a query, a key, or a value)라고 생각할 수 있다.

예를 들면 쿼리와 키의 경우 이것은 각각의 헤드를 나타낸다. 따라서 이 작은 상자들(그림에서 보라색 박스)의 개수는 헤드의 개수($\text{h}$)와 같다.

이것이 무엇을 의미하는지 더 잘 시각화하고 이해하기 위해 논문에서 각 헤드가 어떤 역할을 하는지 해석하는 방법을 참고한다. 어텐션 맵(attention map)은 각 쿼리(dot product query)의 밸류을 나타낸다. 왼쪽의 어텐션 맵에서 우리는 토큰 "its"와 가장 유사한 다른 토큰이 무엇인지 살펴보고자 한다. 그래서 우리는 양(quantities), 즉 "its"를 나타내는 쿼리와 다른 모든 키들의 내적(dot product)을 살펴본다. 그리고 우리는 쿼리와 키의 내적을 높은 밸류로 이끄는(leading) 다른 키들을 찾는다. 그러면 논문에서처럼 "Law"와 "application" 두 단어가 높은 어텐션 가중치(attention weight)로 강조된 것을 확인할 수 있다. 여기서 어텐션 가중치란 쿼리 "its"와 다른 토큰들의 키의 내적(dot product of the query "its" and the key for each of these tokens)이다. 이러한 단어들을 해석하는 방법도 있다. 여기서 강조 표시된 토큰은 "Law"와 "application"인데 토큰 "its"가 "Law"를 지칭하기 때문에 당연한 결과이다. 따라서 모델은 이러한 단어들을 이전에 발생한 사건과 연결하는(associate) 방법을 학습해야 한다.그리고 "its"는 "application"을 의미하기도 하는데 이것은 또한 그러한 이유를 설명하는 또다른 방식(which is also another way of explaining why that is the case)이다. 그래서 저자들은 이러한 값들을(밸류들을?) 각기 다른 헤드들의 함수로 나타내기로 했다. 예를 들어 왼쪽은 헤드의 강도(intensity), 즉 첫 번째 헤드에 대한 강도이다. 그리고 두 번째 헤드는 "Law"에 대한 강도가 매우 높다는 것을 보여준다. 그래서 간단히 말하면 이러한 헤드들은 어떤 단어가 중요한지(what words matters) 파악하는 다양한 방법을 학습할 수 있다.

<br>

Q. 이러한 모든 계산을 수행할 때 각 계산은 서로 다른 MLP를 거치는가? (Are they going through different MLPs when we're doing all these computations?)
A. 헤드마다 서로 다른 투영 행렬을 사용한다. (We're going to have different projection metrices for each of them.)

우리는 실제로 각 헤드가 자체적인 투영을 수행하는 과정에 대한 자세한 예를 다뤘다.


간단히 말하면 고도로 병렬화 되어 있다. 여기에는 행렬 곱과 소프트맥스(softmax) 연산이 포함되어 있다.



And in parallel, you're going to have that computation that's going to happen. So each head is going to have one result here, that is then going to be concatenated and then projected once again when the output matrix. So yeah, long story short, it's highly parallelized. And it's basically just like projections. And here you have some matrix multiplication and softmax.

어텐션 맵을 살펴보면 어텐션 헤드의 기능(what they do)을 이해하는 데 도움이 될 것이다.

<br>

이와 관련해서 트랜스포머 논문인 \<Attention Is All You Need\>를 읽기를 강력히 추천한다. 몇 페이지 분량밖에 되지 않지만 내용이 매우 알차다. 1강에서 다룬 내용을 통해 충분히 이해할 수 있을 것이다.

<br>

## Position Embeddings

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 4.12.25.png)

<br>

2017년에 도입된 트랜스포머 아키텍처는 수년 동안 여전히 유효한 아키텍처이다. 그리고 몇 가지 구성요소가 약간 변경되었다(slightly changed). 그래서 약간의 차이(slight variations)가 있다. 하지만 전반적으로 오늘날의 모델들은 거의 대부분 초기(initial) 트랜스포머 아키텍처를 기반으로 하고 있다.

강의는 두 파트로 나누어 진행된다. 첫 번째 파트에서는 트랜스포머에서 중요하고 몇 가지 변화가 있었던 부분을 다룬다. 두 번째 파트에서는 오늘날 모델의 명칭(nomenclature)과 그것들과 초기 트랜스포머(original transformer)와의 관계에 대해 설명한다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 4.12.35.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 4.12.46.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 4.12.56.png)

<br>

트랜스포머 아키텍처에서 중요한 첫 번째 개념은 위치 임베딩(position embedding)이다. 토큰들은 다른 모든 토큰들과 서로 직접적으로(in a direct fashion) 상호작용한다. 즉 직접적인 연결(direct links)이 있다. 하지만 RNN처럼 각 토큰을 순차적으로 처리

트랜스포머에서는 

즉 위치 정보(position information)를 잃게 된다.



그래서 초기 트랜스포머 논문의 저자들은 전용 임베딩(dedicated embedding)을 사용하기로 했다. 여기서 전용(dedicated)은 각 위치에 하나의 임베딩이 있다(each position has one embedding)는 것을 의미한다. 예를 들면 첫 번째 위치에 하나의 임베딩이 있고 두 번째 위치에 하나의 임베딩이 있다. 그래서 저자들은 입력 토큰 임베딩(input token embedding)에 해당 임베딩을 추가했다. 예를 들어 "A cute teddy bear is reading."이라는 문장의 여기에 첫 번째 위치(first position)를 나타내는 나타내는 임베딩을 더하는 것이다.

Ok, so let's start with the first important concept that's in this architecture. And this is the position embedding. So if you remember, here we're letting tokens interact with all other tokens in a direct fashion. So they have direct links. But contrary to things like RNNs, where you have a sequential dependency were you process each token one at a time, here you're basically losing this idea of a token being processed before another one. So you can lose this position information. So as a result of that, we need to somehow quantify tokens at each position and try to inject that information when the transformer is processing the inputs. So how are we going to do that? So the original transofrmer paper authors, they chose to have a dedicated embedding. And when I say dedicated, what that means is each position has one embedding. So position 1 has one embedding, position 2 has one embedding, et cetera, et cetera. And what they chose to do is to add that embedding to the input-token embedding. So for instance, if I say a cute teddy bear is reading, which is position number 1, representing the token a, plus the embedding representing the first position.

<br>

Q. 위치 임베딩은 학습되나요 아니면 고정적인가요? (Are the position embeddings learned or static?)

A. 둘 다. (Both.)

저자들이 두 가지 방법을 모두 시도했기 때문이다. 두 번째 방법이 무엇인지 살펴보겠지만 여기서는 임베딩이 학습되었다고(they are learned) 가정해 보자. 이게 무슨 뜻일까? 각 위치에 대한 임베딩을 학습해야 한다(need to learn embeddings for each position)는 뜻이다. 하지만 이 접근법의 문제는 학습 데이터셋(training set)에 매우 의존적(much dependent)이라는 것이다.

예를 들어 여기처럼 항상 두 번째 위치에서 어떤 일이 일어나는 텍스트가 있다면 학습된 임베딩(learned embeddings)은 그러한 편향이 학습될 수 있다. 이것이 첫 번째 한계이다.

두 번째 한계는 학습할 수 있는 위치의 수는 학습 데이터셋에 있는 최대 위치 개수만큼이다 (Can only learn positions up to the max number of position that is in your training set). 예를 들어 트랜스포머를 최대 512개의 시퀀스로 학습시킨다고(train your transformer on sequences that are up to 512) 가정해보자. 우리는 해당 위치까지의 위치 임베딩만 학습할 수 있다(can only learn position embeddings up to that position).

어떻게 파라미터화할 수 있을까?(How do you parameterize that?) 위치 1부터 위치 512 사이의 위치에 대한 학습 가능한 위치 임베딩의 자리 표시자를(have a kind of placeholder of a learnable position embedding between 1 and 512) 사용하는 것이다. 그리고 학습을 진행할 때는(when you do your training) 일반적인 경사하강법(gradient descent)을 통해 가중치를 학습시킨다. 이게 첫 번째 방법이다. 하지만 앞서 언급했듯이 이 방법에는 한계가 있다. 훈련 데이터셋(training set)에 존재하는 최대 위치까지의 위치 임베딩만 학습할(only learn embeddings of positions up to the max position that is present in the training set) 수 있기 때문이다. 예를 들어 추론할 때 훈련 데이터셋에 있던 위치를 벗어난 위치(a position that is beyond the position that was in the training set)가 있다면 그 위치는 학습되지 않은 상태이다. 그래서 값(밸류?)을 추론하는 방법을 찾아야 한다. 이것이 두 번째 한계점이다.

<br>

But yeah, but on the pro side, I guess you're just letting your model learn. And we've seen that the gradient descent does wonders when it comes to just learning from the data. And for these reasons, these methods was someting that the authors said that was performing well, along with the second method, which is different, which is around having an arbitrary formula for each dimension corresponding to a position embedding.

<br>

## References

[Lecture] <https://www.youtube.com/watch?v=yT84Y5zCnaA>

[Slide] <https://cme295.stanford.edu/slides/fall25-cme295-lecture2.pdf>