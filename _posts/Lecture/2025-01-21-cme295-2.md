---
title: "[Stanford][CME 295] Lecture #2 Transformer-Based Models & Tricks"
date: 2026-01-21 09:20:00 +0900
categories: [Lecture]

published: true
use_math: true
---

---

&nbsp;

## Lecture 1 Recap

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오전 10.46.13.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오전 10.46.25.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오전 10.46.35.png)

<br>

1강에서는 셀프 어텐션(self-attention) 개념에 대해 소개했다. 셀프 어텐션이란 각 토큰이 어텐션 메커니즘을 통해 시퀀스 내의 다른 모든 토큰들에 주의를 기울이는 것을 의미한다. 쿼리(query, $q$), 키(key, $k$), 밸류(value, $v$)라는 표기법(notation)을 사용한다. 여기서 핵심은 쿼리와 키를 비교함으로써 쿼리가 자신과 가장 유사한 토큰을 찾는다는 것이다. 그리고 그 과정이 완료되면 해당 토큰과 관련된 밸류를 가져오게 된다.

셀프 어텐션 메커니즘은 다음과 같은 공식으로 나타낼 수 있다. 이 공식은 대규모 행렬 곱(matrix multiplication)인데 하드웨어는 이러한 연산을 처리하는 데 매우 능숙하고(capable) 매우 최적화되어(optimized) 있다.

이 모든 것을 통해 트랜스포머(transformer) 아키텍처를 소개했다. 트랜스포머는 크게 두 가지 구성요소로 이루어져 있다. 인코더(encoder, the left side)와 디코더(decoder, the right side)이다. 트랜스포머는 기계 번역(machine translation) 분야에서 처음 도입되었다. 인코더는 입력 텍스트를 원래 언어(source language)(예를 들면 영어)로 처리하는 역할을 수행한다. 디코더는 번역된 내용(translation)을 대상 언어(target language)(예를 들면 프랑스어)로 해독하는 역할을 수행한다. 멀티 헤드 어텐션(Multi-Head Attention) 레이어에서 셀프 어텐션 메커니즘이 작동한다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 2.02.46.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 2.09.59.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 2.10.10.png)

<br>

멀티 헤드 어텐션에는 여러 개의 헤드(head)가 있다. 그게 무엇을 의미하는 걸까? 트랜스포머 논문(Attention Is All You Need) 속 그림에서 이러한 헤드를 나타내고 있다. 각 헤드는 모델이 입력을 쿼리, 키, 또는 밸류로 투영하는 한 가지 방법을 학습할 수 있는 기회(an opportunity for the model to learn one way of projecting the input into being a query, a key, or a value)라고 생각할 수 있다.

예를 들면 쿼리와 키의 경우 이것(작은 흰색 박스)은 각각의 헤드를 나타낸다. 따라서 이 상자들(보라색 박스)의 개수는 헤드의 개수($h$)와 같다.

이것이 무엇을 의미하는지 더 잘 시각화하고 이해하기 위해 논문에서 각 헤드가 어떤 역할을 하는지 해석하는 방법을 참고한다. 어텐션 맵(attention map)은 각 쿼리(dot product query)의 밸류를 나타낸다. 우리는 왼쪽의 어텐션 맵에서 토큰 "its"와 가장 유사한 다른 토큰이 무엇인지 찾고자 한다. 그래서 양(quantities), 즉 "its"를 나타내는 쿼리와 다른 모든 키들의 내적(dot product)을 살펴본다. 그리고 쿼리와 키의 내적을 높은 밸류로 이끄는(leading) 다른 키들을 찾는다. 그러면 논문에서처럼 "Law"와 "application" 두 단어가 높은 어텐션 가중치(attention weight)로 강조된 것을 확인할 수 있다. 여기서 어텐션 가중치란 쿼리 "its"와 다른 토큰들의 키의 내적이다. 이러한 단어들을 해석하는 방법도 있다. 여기서 강조 표시된 토큰은 "Law"와 "application"인데 토큰 "its"가 "Law"를 지칭하기(referring) 때문에 당연한 결과이다. 따라서 모델은 이러한 단어들을 이전에 발생한 사건과 연결하는 방법(how to associate these words with what happended before)을 학습해야 한다. 그리고 "its"는 "application"을 의미하기도 하는데 이것은 그러한 이유를 설명하는 또 다른 경우이다. 그래서 저자들은 이러한 밸류들을 각기 다른 헤드들의 함수로 나타내기로 했다. 예를 들어 왼쪽은 헤드의 강도(intensity), 즉 첫 번째 헤드에 대한 강도이다. 그리고 두 번째 헤드는 "Law"에 대한 강도가 매우 높다는 것을 보여준다. 그래서 간단히 말하면 이러한 헤드들은 어떤 단어가 중요한지(what words matters) 파악하는 다양한 방법을 학습할 수 있다.

<br>

Q. 이러한 모든 계산을 수행할 때 각 계산은 서로 다른 MLP를 거치는가? (Are they going through different MLPs when we're doing all these computations?)

A. 헤드마다 서로 다른 투영 행렬을 사용한다. (We're going to have different projection metrices for each of them.)

우리는 실제로 각 헤드가 자체적인 투영(its own projection)을 수행하는 과정에 대한 자세한 예를 다뤘다. 그리고 동시에(in parallel) 해당 계산이 진행된다. 각 헤드는 하나의 결과를 생성한다. 그러면 이 결과들은 연결된(concatenated) 다음 다시 한번 투영되어(projected once again) 출력 행렬(output matrix)이 된다. 간단히 말하면 고도로 병렬화(highly parallelized) 되어 있다. 여기에는 행렬 곱과 소프트맥스(softmax) 연산이 포함된다.

<br>

## Position Embeddings

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 4.12.25.png)

<br>

2017년에 도입된 트랜스포머 아키텍처는 수년 동안 여전히 유효한 아키텍처이다. 몇 가지 구성요소가 약간 변경되었다. 하지만 오늘날의 모델들은 거의 대부분 초기(initial) 트랜스포머 아키텍처를 기반으로 한다.

트랜스포머 아키텍처에서 중요한 첫 번째 개념은 위치 임베딩(position embedding)이다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 4.12.35.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 4.12.46.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 4.12.56.png)

<br>

토큰들은 다른 모든 토큰들과 서로 직접적으로(in a direct fashion) 상호작용한다. 즉 직접적인 연결(direct links)이 있다. 하지만 각 토큰을 순차적으로 처리하는 RNN과 달리 트랜스포머에서는 어떤 토큰이 다른 토큰보다 먼저 처리된다는 개념이 사라지게 된다. 즉 위치 정보(position information)를 잃게 된다.

결과적으로 우리는 각 위치의 토큰을 정량화하고(quantify) 트랜스포머가 입력을 처리할 때 그 정보를 주입해야(inject) 한다. 그래서 초기 트랜스포머 논문의 저자들은 전용 임베딩(dedicated embedding)을 사용하기로 했다. 여기서 전용(dedicated)은 각 위치에 하나의 임베딩이 있다(each position has one embedding)는 것을 의미한다. 1번 위치에 한 개의 임베딩이 있고 2번 위치에도 한 개의 임베딩이 있는 식이다. 저자들은 입력 토큰 임베딩(input token embedding)에 위치 임베딩을 추가했다. 예를 들어 "A cute teddy bear is reaing." 문장에서 토큰 "A"를 나타내는 첫 번째 위치에 첫 번째 위치를 나타내는 임베딩(embedding representing the first position)이 더해진다.

<br>

Q. 위치 임베딩은 학습되나요 아니면 고정인가요? (Are the position embeddings learned or static?)

A. 학습되기도 하고 고정이기도 하다. (Both.)

저자들이 두 가지 방법을 모두 시도했기 때문이다. 두 번째 방법이 무엇인지 살펴보겠지만 여기서는 임베딩이 학습되었다고(they are learned) 가정해 보자. 이는 각 위치에 대한 임베딩을 학습하는(learn embeddings for each position) 것이다. 하지만 이 접근 방식의 문제점은 학습 데이터셋(training set)에 매우 의존적(dependent)이라는 것이다. 예를 들어 여기에서처럼 항상 두 번째 위치에서 어떤 일이 일어나는 텍스트가 있다면 학습된 임베딩(learned embeddings)은 그러한 편향(bias)이 학습될 수 있다. 이것이 첫 번째 한계이다. 두 번째 한계는 학습할 수 있는 위치의 개수가 학습 데이터셋에 있는 최대 위치 개수만큼이라는 것이다(only learn positions up to the max number of position that is in your training set). 예를 들어 트랜스포머를 최대 512개의 시퀀스(sequences that are up to 512)로 학습시킨다고 가정해 보자. 우리는 해당 위치까지의(up to that position) 위치 임베딩만 학습할 수 있다.

<br>

Q. 어떻게 위치 임베딩을 파라미터화할 수 있을까? (How do we parameterize that?)

A. 위치 1부터 위치 512 사이에 학습 가능한 위치 임베딩에 대한 일종의 자리 표시자(a kind of placeholder of a learnable position embedding between 1 and 512)를 두는 것이다. 그리고 경사하강법(gradient descent) 등을 통해 이러한 가중치를 학습시킨다. 이것이 첫 번째 방법이다. 하지만 앞서 언급했듯 이 방법에는 한계가 있다. 학습 데이터셋에 존재하는 최대 위치까지의 위치 임베딩만 학습할 수 있기 때문이다. 예를 들어 추론할 때 훈련 데이터셋에 있던 위치를 벗어난(beyond the position that was in the training set) 위치가 있다면 그 위치는 학습되지 않은 상태이다. 그래서 그 값을 추론하는 방법을 찾아야 한다. 이것이 두 번째 한계점이다. 하지만 장점으로는 모델이 스스로 학습하도록 내버려둔다는 것이다. 그리고 우리는 경사하강법이 데이터를 통해 학습하는 데 있어서 놀라운 성능을 보인다는 것을 확인했다. 이러한 이유로 저자들은 이러한 방법이 두 번째 방법과 함께 좋은 성능을 보인다고 말했다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.09.08.png)

<br>

두 번째 방법은 위치 임베딩에 해당하는 각 차원에 대해(for each dimension corresponding to a position embedding) 임의의 공식을 취하는 것이다. 첫 번째 방법은 위치 당 하나의 임베딩(one embedding per position)을 사용하고 그 임베딩을 학습하는 것이었다. 두 번째 방법은 위치 당 하나의 임베딩을 사용하지만 그 임베딩을 학습하는 것은 아니다. 사전에 정해진(predetermined) 것을 사용하게 된다. 저자들은 사인 함수와 코사인 함수를 사용한 공식을 선택했다. 여기서 핵심은 주어진 위치 $\text{m}$에 대해 크기가 $\text{d}$인 벡터 모델(a vector of size d model)을 만드는 것이다. 토큰들을 더해야(adding) 하기 때문에 $\text{d}$는 토큰 임베딩의 차원과 일치해야 한다. 그리고 모든 인덱스에 대해 이 공식들에 대한 값을 계산하게 된다. 첫 번째 공식은 어떤 것과 $\text{m}$의 곱에 사인 함수를 취한 것이다. 두 번째 공식은 어떤 것과 $\text{m}$의 곱에 코사인 함수를 취한 것이다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.09.10.png)

<br>

표기법을 단순화하자. $\omega_i=10000^{-\frac{2i}{d_{\text{model}}}}$라 가정하자. 즉 $\omega$가 $i$와 $m$의 곱에 대한 함수라고(as a function of i times m) 가정하자.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.09.24.png)

<br>

우리는 서로 가까이 있는 단어일수록 관련성이 높고 서로 멀리 있는 단어일수록 관련성이 낮다는(words that are close together are liekly to be more relevant, as opposed to words that are further together) 사실을 반영하며 위치를 나타내고자 한다. 즉 두 단어가 한 개의 위치만 떨어져 있는 경우와 만 개의 위치가 떨어져 있는 경우가 있다면 한 개의 위치만 떨어진 경우가 더 유사해야(similar) 한다. 공식이 타당한지 살펴보자. 두 개의 위치 임베딩이 있다. 하나는 위치 $m$에 하나는 위치 $n$에 있다. 그리고 사전에 정해진 공식을 사용해 모든 값을 계산했다고 가정해 보자.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.09.33.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.10.11.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.10.22.png)

<br>

삼각함수 공식(trigonometry formulas)을 기억한다면 $\cos(a-b)=\cos(a)\cos(b)+\sin(a)\sin(b)$이다.

$\cos(\omega_i(m-n))$으로 표현하면 다음과 같은 결과를 얻는다.

알고 보니 $\cos(\omega_i(m-n))$는 두 위치 임베딩의 내적을 계산할 때 나타나는 요소 중 하나(one component)일뿐이다.

위치 $m$과 위치 $n$을 내적할 때 첫 번째 위치를 가져와서(take), 그들을 곱하고(multiply), 두 번째 위치에 값을 더하고(plus), 다시 그들을 곱하는 식으로 계속한다. 그리고 이것이 $\cos(\omega_i(m-n))$임을 알 수 있다. 결국 이러한 임베딩들의 내적을 수행하면 $m$과 $n$ 사이의 상대 거리에 대한 함수인 코사인의 합(a sum of cosine that are a function of the relative distance between m and n) 얻게 된다.

<br>

Q. 두 임베딩이 가까울수록 더 유사하다는 것인가? (The closer they are, the more similar they are?)

A. 직관적으로 말하면 이러한 임베딩 공식화 방법은(way of formulating the embeddings) 근사화하거나(trying to approximate) 모방하려는(trying to mimic) 것이다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.10.29.png)

<br>

그래서 내적 $\left\langle \text{PE}_m, \text{PE}_n \right\rangle$을 얻게 되는데 이는 $m$과 $n$, 즉 두 임베딩 사이의 상대 거리에 대한 함수이다.

다시 한번 상기하자면 왜 내적을 고려해야 할까? 임베딩 세계에서(in the embedding world) 두 임베딩 간의 유사성(similarity)을 정량화할(quantify) 때 주로 두 임베딩의 내적을 수반한다(involving). 일반적으로 코사인 유사도(cosine similarity)를 사용한다. 코사인 유사도는 각 임베딩의 노름(norm)에 대한 내적이다. 즉 기본적으로 내적이다. 그래서 우리가 내적에 관심을 갖는 것이다. 그리고 여기서 알 수 있듯 이것은 두 위치 사이의 상대 거리에 대한 함수이다.

특히 $\cos(0)=1$이다. $\omega_i(m-n)$가 클수록 $\cos(\omega_i(m-n))$은 작아진다. 물론 주기성이 있기(periodic) 때문에 이 말이 반드시 맞는 것은 아니다. $\pi$를 지나면 방향은 완전히 반대가 된다. 하지만 말하고자 하는 것은 $m$과 $n$이 같을 때 $\cos(0)$에 대한 합을 얻게 된다는 것이다. 그리고 그 값은 $\left\langle \text{PE}_m, \text{PE}_n \right\rangle$이 최대(maximum)가 되는 값이다. 즉 $m$과 $n$이 같을 때 $\left\langle \text{PE}_m, \text{PE}_n \right\rangle$이 최대가 된다. 위치 자체만 놓고 보면 가장 유사하다는 것이다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.10.38.png)

<br>

임베딩 값을 그래프로 나타내면 다음과 같다. 왼쪽 그래프에서 y축은 50개 위치 각각에 대한 모든 임베딩을 나타낸다. x축은 주어진 벡터의 여러 차원에 걸친(across several dimentions) 값들을 나타낸다. 예를 들어 첫 번째 행은 벡터를 어떻게 인덱싱 하는지에 따라(depending on how you index your vector) 첫 번째 위치(first position) 또는 0번째 위치(number 0 position) 임베딩의 모든 값들을 나타내고 있다. 보이는 것과 같이 차원이 낮을수록 이 값은 매우 빈번하게 오르내린다(goes up and down very frequently). 즉 더 높은 주파수(high frequency)이다. 그리고 차원이 높을수록 값이 오르내리는 데 많은 시간이 걸린다. 즉 더 낮은 주파수(low frequency)이다.

이것은 앞서 언급했던 $\omega_i$와 관련있다. $\omega_i$는 차원 $i$의 값이 작을 때 매우 높다(very high). 그리고 $i$의 값이 클 때 매우 낮다(very low). 즉 $\omega_i$는 코사인과 사인 값이 얼마나 빨리 변하는지(how quickly cosine and sine vary)를 결정한다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.10.47.png)

<br>

이것이 원저자들(original authors)이 시도했던 것이다. 그리고 이러한 방법들을 사용하면 학습된 방법(learned one)과 비슷한(comparable) 결과가 나온다는 점에 주목했다. 하지만 이 방법은 큰 장점이 있다. 훈련 시점에 관찰했던 시퀀스 길이 뿐만 아니라(not just a sequence length that you saw at training time) 어떤 시퀀스 길이에도(to any sequence length) 적용할 수 있기 때문이다. 이것이 바로 이 방식이 더 바람직한(preferable) 이유 중 하나이다. 이것이 저자들이 선택한 방식(what the authors chose)이다.

2025년으로 시간을 되돌려 보자. 아직도 이 방법을 사용하고 있는지("Are we still using that?") 물을 수도 있다. 어느 정도는 그렇다("Kind of"). 우리는 여전히 멀리 떨어진(far) 토큰일수록 가까운(closer) 토큰보다 유사성이 낮다는(less similar) 아이디어를 사용하고 있다. 하지만 우리는 저자들이 한 것처럼 임베딩을 삽입하지는(injecting) 않는다. 그 이유를 알아보자.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.10.53.png)|![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-21 오후 4.12.46.png)

<br>

우리가 중요하게 생각하는 것은 셀프 어텐션 계산에서 토큰들이 얼마나 유사한지(how similar tokens are)를 결정하는 것이다. 셀프 어텐션 계산은 어텐션 레이어에서 이루어진다. 우리는 임베딩을 계산해서 여기에(입력에) 더한다고(add) 했다. 하지만 사실 우리가 원하는 건 어텐션 레이어에 이 유사성을 반영하는(reflect) 것이다.

<br>

Q. 입력 특징에 더해지는 것인가? (Is it added to the input feature?)

A. 그렇다. 첫 번째 방법에서는 그냥 더하면 된다. (Yes, so in this first method. Just add.)

<br>

하지만 우리는 멀티 헤드 어텐션 레이어에서 그 직관(intuition)이 그대로 적용되기를 바란다. 이것이 사람들이 다양한 변형을 시도한 이유(why people have tried different variations) 중 하나이며 특히 이러한 위치 임베딩이 입력이 아닌 어텐션 레이어에 직접 적용되도록(intervene directly) 했다. 입력에 적용하면 어텐션 레이어에 어느 정도(roughly) 영향을 미치겠지만 간접적인(indirect) 방식이기 때문이다. 따라서 가까운 토큰일수록 멀리 있는 토큰보다 더 유사하도록 어텐션 공식을 직접 수정한다(directly do something about the attention formula).

<br>

그리고 그 방법은 셀프 어텐션 레이어가 qk 전치 함수에 d 곱하기 v의 제곱근을 곱한 값에 대한 소프트맥스(the softmax of qk transpose over square root of d times v)라는 것이다. 그래서 우리는 소프트맥스 함수 안에(inside that softmax) 무언가를 추가하고자 한다. 소프트맥스함수는 토큰이 다른 토큰과 얼마나 유사한지를 정량화한다. So what we want is to add a little something inside that softmax, which is basically where you quantify how similar a token is to another token. 어떤 토큰은 다른 토큰에 비해 특정 토큰과 더 유사하다는 사실(the fact that some tokens supposed to be more similar compared to that token, compared to others)을 반영하는 무언가(a little something)를 추가하는 것이다.

그래서 이와 유사한 여러 가지 방법들(some variation of that)이 시도되어 왔다. T5 논문에서는 위의 공식에 있는 편향 항(bias term)을 학습함으로써 상대적 위치 편향(relative position bias)을 시도했다. 위치 m과 n 사이의 거리가 주어졌다고 가정해보자. 그들의 아이디어는 bias(m, n)을 학습하는 것이었다. 모든 m-n 값을 몇 개의 버킷으로 나눈다(bucketize). 그리고 모델이 이러한 양(quantities)들을 학습하도록 하고 그 학습된 양들을 소프트맥스 함수에 적용한다(infected into softmax).

<br>

Q. 최종 확률에 대한 편향이 문제가 되지 않을까요? 합이 1이 되어야 하잖아요. (Does that pose a problem that the bias is here with respect to the probability at the end? Because it has to sum to 1.)

A. 소프트맥스 안에서는 어떤 작업을 하든 상관없다. 소프트맥스 자체가 어차피 정규화를 해주기 때문이다. (You can do whatever you want inside the softmax. Because the softmax is going to normalize it anyways.)

<br>

즉 편향을 가까이 있는 것들에 비해 멀리 있는 것들에 대해 더 부정적인 반응을 보이는 것(being something that is more negative for things that are far apart compared to things that are closer together)이라고 생각할 수 있다. T5는 이것들을 학습하자고 주장한다. ALiBi라는 방법을을 소개한 "Train Short, Test Long" 논문에서 또다른 메시지를 전하고 있다. ALiBi는 Attention with Linear Bias의 약자이다. 이 방법은 그러한 편향을 학습하는 대신 두 위치 사이의 상대적 차이에 대한 함수로서 결정론적인 공식(deterministic formula, which is as a function of the relative difference between those two positions)을 사용하는 것이었다. 그렇게 해서 몇 가지 결과를 얻었다. 이 모든 논문들은 항상 서로 비교해서(compare based on one another) 어느 것이 더 나은 성능을 보이는지 판단한다.

<br>

But the reality is that in today's models, most models actually use another kind of position embedding method. And we're going to see it now. So this method relies on rotating the query and the key vector by some angle. And I guess you can think of it as, you have your query, you have your key, let's suppose in a 2D space. So what you're going to do is rotate your query by some angle that is a function of its position. And you're going to rotate the key vector by some angle that is a function of its position n. And I guess how do you do that, by the way? So let's suppose you have a vector, by the way. And you want to rotate that vector, because I had the answer on the slide. But how would you go about this, I guess, for people who just want to talk about this with intuition? So who here has done, I don't know, rotations in space? Matrix multiplication. And you're going to use a quantity that's called rotation matrix. And the rotation matrix is expressed as follows. So it's basically a 2 by 2 matrix in the 2D plane that has cosine of this angle, minus sine of this angle. sine of this angle, cosine of this angle. I'm looking at the time. It's quite simple to just show that it works, but we may run out of time. Do you want me to quickly show you that it's indeed a way to rotate the vector? Okay so here as a reminder whay we're trying to show is that we can use a matrix multiplication to rotate a vector in 2D space. (판서 따라 그려볼 것) So let's suppose we have the following vector. That is something that you can quantify with two dimensions, let's say x and y. You can express your vector in 2D space with this. But I guess this, if you note, are the norm of the vector, and phi, the angle with respect to the x-axis. You can also write v as R with vector cosine of phi and sine of phi. So if you multiply the rotation matrix with this phi, what you're going to obtain is some multiplication of cosine minus sine, sine and cosine of this and that. And I will leave this exercise for you. But you can show that rotation times this v can be expressed as R of cosine of theta plus phi, and sine of theta plus phi. So this is a quick proof. So I'll just leave you the multiplication of the rotation matrix and v. But you will obtain these trigonometric identities that will, I guess, lead you to this formula. This basically shows that if you multiply this matrix and this vector, you're basically rotating the vector by this angle.

<br>

So I guess here, just going back to this method, what we want to do is to quantify the similarity between tokens, and have close tokens be more similar compared to tokens that are more a far. So the problem that we had with the previous methods was -- so in the first method, this learned embedding, you always had this overfitting issue. Because basically when you learn these biases, it always depends on which training set you have. So maybe your dataset is in a way that, let's say, tokens that are close are similar, but in a different way compared to what you see at inference time.

And this ALiBi method, it didn't have that learnable component, but it was quite restrictive, because it's a very simple formula after all, just the relative difference between n and m. So I guess people have tried different ways of coming up with something that tells you that, I guess, an embedding that reflects the fact that you want further positions to be less similar than closer ones. So in this method, we're going back to the sine and cosine world from what the author had proposed. And think about similarity from the lens of cosine and sine functions. So this is a little intro.

<br>

And their method is called RoFE. So I'm not sure if you've heard of it. So it stands for Rotary Position Embeddings. And we're going to see that this method -- so why do we care about this method? So this method has two great things. So the first one is that if you rotate the query and the key, you will end up with a quantity that will be a function of the relative distance between the two. That's going to be very nice, and that's why I wrote this thing on the blackboard. Not sure if you can see, by the way, but we won't have time to go into the mathematical detail. But if you want to just express these things at home, this is just the foundation.

And so in particular, if you remember your attention formula, so you have query times key transpose. So if you rotate the query by an angle m and the key by an angle n, what you're going to end up is a formula that has the rotation matrix of angle theta and, I guess, n minus m. And this is greate because this is a function of the relative distance between these two positions. OK, so why do I talk about this in detail? Well, it turns out that most models these days, they use RoFE, which is why it's important. And I would say another thing, it's maybe a little bit hard to get the intuition as to why that works. But hopefully, the explanation that I gave regarding the sine and cosine at the very beginning can help you just build that intuition.

<br>

그리고 그와 관련해서 말하자면, 쿼리와 키에 의해 결정되는 어텐션 가중치의 상한값이 장기적으로 감소하는 양상을 보인다는 사실이 밝혀졌다 the upper bound of the attention weight given by the query and the key is such that we observe a lone-term decay. 즉 n-m이 커질수록(n-m is large) 상한값이 점점 작아지는 것(the upper bound that gets smaller and smaller)을 확인할 수 있다. 물론 이러한 작은 진동(oscillations)이 나타나긴 한다. 완벽한 결과는 아니다. 하지만 장기적으로 상한값이 감소하는 것(upper bounds decaying over the long-term)에 대한 수학적 결과를 얻을 수 있었다.

<br>

Q. 상대 거리가 회전 행렬에 포착되는가? (Is the relative distance captured in the rotation matrix?)

A. 그렇다. (Yes.)

<br>

Q. 세타는 무엇인가요? (What is theta?)

A. 세타는 실제로 고정된(actually fixed) 값이다. 앞에서 언급했던 오메가(omega)를 기억하는가? 이것은 i와 d의 함수이다. 아주 간단하게 설명했다. 내가 보였던 것은 2차원 공간에서였다. But what I showed you is in the 2D space. 하지만 여기서는 d차원 공간(d-dimensional space)에 있는데 이는 2차원보다 크다(greater than 2). 그래서 제가 아주 간단하게 설명했죠. 이 방법을 확장하는 방법은 2차원 공간을 블록 단위로 나누는(having these 2D space by block) 것이다. 하지만 세타는 일반적으로(typically) 고정된 어떤 값의 함수(a function of typically something that you fix)이기도 하지만 차원 i의 함수(a function of i which is the dimension)이기도 하다. 기본적으로 1과 d 사이의 값, d/2이다. It's basically between 1 and d, d over 2. 이것은 i의 함수이기도 하고 d의 함수이기도 하다. 이 세타는 대략 omega_i와 같다는(roughly euqual) 것을 알게 될 것이다. 거의 그렇다(more or less).

<br>

Q. 잠재 차원과 동일한 차원을 가져야 한다는 것인가요? (So that it has the same dimension as the latent dimension?)

A. 여기서는 행렬의 곱(product of matrices)을 사용하고 있다. 따라서 차원이 일치해야(match) 한다. 답은 "네"이다.

<br>

Q. 이 곡선은 어떻게 구한 건가요? (How do you obtain this curve?)

A. 이 곡선은 수학적으로 표현된 것이다. 좀 복잡하다. 그 공식을 써내려가지는 않을 것이다. 하지만 관심있다면 RoFormer 논문에 수학적으로 어떤 양에 의해 상한이 정해진다는(upper bounded by some quantity) 것을 보여주는 부록이 있다. 이 그림이 바로 그 내용을 보여주는 것이다.

<br>

## Layer Normalization

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 1.13.45.png)

<br>

위치 임베딩은 트랜스포머에서 약간 변형된 부분 중 하나이다. 지금까지 어떻게 바뀌었는지, 그리고 왜 바뀌었는지 살펴보았다. 트랜스포머에서 약간 변경된 또 다른 구성요소도 살펴보겠다. 바로 레이어 정규화(layer normalization)이다.

<br>

트랜스포머 아키텍처는 인코더, 디코더, 그리고 그 안에 있는 구성요소들(components)로 이루어져 있다. 여기 Add & Norm 이라고 적힌 상자들(boses that say add and norm)이 있다. 여기서 하는 일은 이 서브레이어(sublayer)(작은 화살표 세 개)의 입력과 출력을 가져오는 것이다(take the input, as well as the output of this sublayer). 이 값들을 더하고(add) 정규화(normalize)한다. 이것은 저자들이 사용한 작은 비법(trick)이다. 실제로 수렴 속도를 향상시키고(improve convergence) 수렴 과정을 더 빠르게 만드는 것(make the convergence be quicker)으로 나타났다.

아이디어는 다음과 같다. 벡터가 주어졌을 때 벡터의 구성요소들(the components of the vector)이 때로는 매우 클 수도(super large) 있고 매우 작을 수도(super small) 있다. 여기서 핵심은 벡터의 구성요소들을 특정 범위, 특 정규화된 범위 내에서(within some range, some normalized range) 정규화하는(normalize) 것이다. 먼저 벡터에서 계산된 평균(기본적으로 구성요소의 합)을 빼고 표준 편차로 정규화한다 you're going to take your vector, and then subtract it by the computed mean, which is basically the sum of its components, and normalize it by basically its standard deviation.
그리고 두 가지 변수(양)(quantities)을 학습하게 된다. 첫 번째는 감마(gamma)로 이는 스케일링 계수(rescaling factor) 역할을 하고 두 번째는 베타(beta)인데 이것 또한 우리가 학습하게 될 항(another term as well that you learn)이다. 이 두 가지 변수를 모델이 학습하도록 할 것이다 let these two quantities be learned by your model.

실제로 앞서 언급했듯이 이것은 훈련 안정성(training stability)과 수렴 시간(convergence time)에 도움이 된다.

그래서 이것이 원래 트랜스포머 논문에서 사용된 기술(a technique that was used in the original transformer paper)이었다.

<br>

I just want to call out that there has been some changes since then. So we went from normalizing the input plus the output of the sublayer, to having a sum of the input and the sublayer of the normalized input. So in other words, what we've done is to change where the normalization is located. So here in the transformer paper, it was what we now call a post-norm version. And nowadays, we use a pre-norm version, which basically consists of having the Layer Norm right before the vector goes into the sublayer. And sublayer here can be either the attention layer or the FFN.

<br>

But not only that, there is also another change. So nowadays, people, they do not use LayerNorm. They use something else called RMSNorm -- RMS, Root Mean Square, Normalization, which is basically a variation of what you've seen before.

So instead of computing this, basically what people do is they just normalize x by the root mean square of the components of x, and they learn gamma, only gamma. So why do they do that? Basically, they show that the convergence properties, they're basically comparable. But here you have fewer parameters to learn. So it's basically quicker.

<br>

Q. So the question is, what is the intuition behind normalizing?

A. So the intuition is that, if you look at your model, you have several layers. In some layers your vector -- your activation, to be more precies -- so do you know the vector that you see that goes from here to here is basically called activation. Sometimes the activation has extreme values in one part of its component, sometimes in another part. And the model is typically having trouble in learning the weights in each of these layers if these activations, they vary too much. So the idea is to bring the values of the components of the activation to some range that is not too far off in some direction. So in case you're interested, there is this key word, internal covariate shift, which is basically the term that is given to the phenomenon that I'm describing here. So yeah, that's the intuition.

<br>

Q. What is the difference between this and batch normalization?

A. So batch normalization is normalization across the other dimension, which is the dimension of the batch. So let's suppose you have a bunch of vectors. What you do is you normalize each component with respect to all the other components for the same dimension, but of the other vectors. So you can think of it as just another way of normalizing. But that being said, when it comes to these transformer-based models, typically people use LayerNorm, probably because empirically it works better. But also because BatchNorm, you're basically also dependent on the batch. And it can introduce, I guess, differences between training and then inference. So that's basically the reason why.

<br>

## Attention Approximation

<br>

So we've seen position embeddings. We've seen Layer Normalization. So now we're going to see a third important component of the transformer, which is the attention.

<br>

And in particular, I guess there is something I've not really emphasized. But when you do self-attention, you're basically letting every token interact with all other tokens. So when you look at, let's say a matrix that shows all the interactions, so you have n, the sequence length. And the sequence length, you basically have o of n suqared somplexity, which is a lot, especially as n grows longer. So people have tried to approximate this o of n suqared into something that is a little bit more tractable, but does not lose the performance. So there is this paper in 2020 that came out, "LongFormer." So what it did was just try different versions of the attention by restricting the window at which it operates. So here instead of letting each token interact with everyone, each token only interacts with its neighborhood.

<br>

Q. Do you do that after the attention matrix computation? You mean the softmax, right, softmax?

A. So you raise a greate point, which is, if you do the softmax of everything, why would you do this? So in practice there is a bunch of implementations that does a clever -- so it's called tiling -- a bunch of clever operations that do not involve this huge matrix operation that you see in the softmax. Again, I guess, like against the softmax of qk transpose over d. You're not going to compute the whole thing. You're going to have some cleverness in how you compute that, basically.

<br>

Q. Can it be something that's comparable to convolutions?

And we're going to see that in a second. But you have some similarities with the vision world. And we're going to see that in a second.

<br>

So nowadays when you have local attention like this, people use the term sliding window attention. So when they use that term, they mean this, which is basically just restricting the attention to neighboring tokens. And nowadays what people do is in some layers they will have local attention. In some others, they will have global attention. And they interleave these layers. So depending on the model, they typically try different combinations. So there's not a set recipe. But it's typically something that is used nowadays. So the window here, I mean, for, illustrative, purposes, the window is super small in my illustration. But I guess nowadays with the sequence lengths that gan be very big, you can think of this window as being, o of several, thousands.

<br>

And just to give you another example. So back to the convolution comparison thay you mentioned. So you have some architectures. And here I'm going to take the example of a mistral that has these sliding window attention at every layer. But then when you think about it, the token here can attend up to the token here, but then the token here can attend up to the token here, et cetera, et cetera.

<br>

So I guess, if you think about it, it's similar to the idea of the receptive field in computer vision. So I'm not sure if you're familiar or from the computer vision world. But if you are, what this means is just taking one token and trying to think what other tokens has this token effectively interacted with, which is basically the question that people also sometimes ask. When they do convolution, they're like, OK, so this value, what are the values did it actually see? So you can also think it this way.

<br>

OK, so first variation is, instead of doing the full n by n attention, people sometimes do local attention. The second variation that is orthogonal to all of this is to not have one projection mtrix per head, but to share projection matrices across heads. So here the idea is you have, let's say, h heads. The idea is you're going to have some number of projection matrices for the query. But then what you're going to do is to group the projection matrices for the key and the value that you're going to share across several heads. Now you may ask, why do you share projection matrices for the keys and the values, but not for the queries? Is this the question that you're wondering? Well here, I guess, people do things to just try out if something works or not. I guess here you can intuitively think that the query is basically wondering if something is similar to another thing. So I guess you can ask yourself this question in different ways. So it may make sense to kepp that diversity. But one of the core reasons why we choose to group projection matrices for the keys and the values and not the query is because, when you decode, you perform attention between the current word and all the words before. So in other words, every time you generate a new world, you're going to attend that word to all the words before. So the keys and the values they're going to come up a lot. So every time you want to decode something, you need to attend to all other things, again and agian. So we're going to see in, I think, next, lecture there's something called the KV cache, which basically saves the values of the keys and values. And one thing that we want is for that cache to not become too big. So if you share projection matrices across heads, just allows you to save a little bit of space, a little bit of memory. That's the why. Does that roughly make sense?

<br>

So speaking of that, there are some variations as to, I guess, just how many projection matrices to share. So you have the extreme example where all h heads, they all share the same projection matrices for v and k. And in that case, this method is called MAQ, Multi-Query Attention.

You have the in-between case where you share g, where g is the number of groups g projections matrcies for v and k. And then you have groups of size, let's say, h over g. So this one is called Group Query Attention, GQA.

And then you have the case that you are very familiar with from the transformer, which is every head has its own query projection, key projection, and value projection matrices. And this is the standard, multi-head attention.

<br>

So I'm going to just take a pause to see if what I mentioned makes sense. I just want to make sure that what I discussed is making sense, roughly.

<br>

Q. Do we apply this to self-attention and cross-attention?

A. I don't want to spoil the show. So I'm going to talk about something later on. So if you take a look at the transformer architecture, we're going to see soon that what we care about most is what is in the decoder, because we actually forego of the encoder in nowadays' models. We have not seen this yet, but I'm just telling you. And so this typically comes into play for the masked self-attention in the decoder. But this technique can be applied in all attention layers. But I guess what I'm telling you is modern LLMs, their decoder only models which basically only have the decoder part of the transformer. We're going to see this in a second, which is the masked self-attention. So I'm not going to say too much because Shervine is going to cover that. But yeah, just a quick peek of what we're going to wee.

<br>

Q. When do you know which one you should use?

A. I would say the choice is always driven by a few factors. One is how well it performs. Second is how much do you care about things like latency costs. And so it really depends how big is your model, how much you want to save on, let's say, compute, how is your input length, for instance if you have a shorter input length. So here what we want to do is to avoid having to do all these things for the whole o of n squared thing. So I guess all of these come into play. So I would way it's not straightforward as to what the answer should be. But I would say a lot of recent models, they tend to share projection matrices. So typically, I would say GQA is what you sould see. But it's not necesaarily the case for all models.

<br>

## Transformer-Based Models

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 3.57.58.png)

<br>

트랜스포머 아키텍처가 사용되는 다양한 모델 유형에 대해 좀 더 자세히 살펴보겠다. 그리고 분류 설정(classification settings)에 매우 유용한 특정 아키텍처 하나를 자세히 살펴보겠다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 3.58.08.png)

<br>

먼저 인코더와 디코더 두 가지 구성요소를 모두 포함하는 전통적인 트랜스포머 아키텍처로 돌아가 보자. 2017년에 발표된 최초의 트랜스포머 논문에서 이러한 아키텍처를 제시했다. 그리고 이후에 이 아키텍처를 기반으로 더 많은 아키텍처들이 등장했다.

T5 모델 계열(T5 family of models)이 있다. T5는 여러 개의 T를 줄인 약어(abbreviation of multiple Ts)이다. 첫 번째는 transfer, 그 다음부터는 text-to-text transformer이다. T5라는 이름은 여기서 유래했다. 그리고 여러 버전으로 파생되었다(derived). T5는 기본 논문(vanilla paper)이라고 할 수 있다. 그리고 mT5가 있는데, m은 다국어(multilingual)를 의미하며 학습에 사용된 데이터와 어휘(vocabulary)에 대한 추가적인 작업이 이루어졌다. 그리고 토큰화 과정을 생략하는(tokenizer-free) ByT5라는 방식이 있다. 그리고 그 대신에 바이트(byte) 단위로 연산을 수행한다. 그러니까 'By'는 바이트와 같은 의미이다. 기본적으로 어휘 크기가 훨씬 작아진다. o of 30k 대신 2의 8 제곱(2 to the power of 8)을 사용하는 것이다. 1 바이트는 8비트(bit)이다. 그러면 모든 문자를 바이트로 표현할 수 있다.

T5 계열(T5 family)에 대해 한 가지 알아야 할 점은 목적함수(objective function)가 기존의 트랜스포머와 약간 다르다는 것이다. 초기 트랜스포머는 학습 과정에서 다음 토큰에 대한 예측(next token prediction)을 수행했다. 하지만 T5 계열은 소위(so-called) 스팬 손상(span corruption) 작업을 수행했다. 즉 기본적으로 문장을 인코더에 입력으로(sentence as an input to the encoder) 준다. 그리고 모든 부분을 인코더에 입력하는 대신(instead of putting everything to the encoder) 일부 구간을 공백으로 남겨둔다(leave blanks). 이것을 스팬 손상이라고 부른다. 그리고 스팬 손상은 하나 또는 여러 개의 토큰이 누락된 경우일 수 있다(could be one or multiple tokens missing). 예를 들어 "My teddy bear is cute and reading." 이라는 문장에서 "My teddy bear" 스팬이 손상되어(corrupted) "is reading"이라는 메시지가 나타날 수 있다. 이것이 하나의 잠재적인(potential) 인코더 입력일 수 있다. 그리고 최대 n개의 스팬이 손상될 수 있다. 여기서 n은 손상된 스팬의 개수를 나타내는 매개변수(parameterization of the number of spans that are corrupted)이다. 그래서 여기에 그러한 토큰이 n개 있게 된다(n such tokens here). T5 계열에서는 이를 센티널 토큰(sentinel token)이라고 부른다. 센티널 토큰은 손상된 토큰들의 구간(a span of corrupted tokens)을 나타낸다. 인코더에는 이러한 센티널 토큰들이 있다. 그리고 디코더는 이러한 구간들을 순차적으로 찾아내는 역할을 한다. 먼저 첫 번째 손상된 구간을 나타내는(denotes) 토큰으로 시작한다. 그런 다음 디코딩 프로세스를 시작해서 다음 센티널 토큰을 예측하는 n+1, 여기서 연속된 두 센티널 토큰 사이에서 디코딩 되는 토큰은 복구된 손상된 구간에 해당한다. And then you start the decoding process until it hits a prediction that predicts the next sentinel token up to the n plus 1, 1, where the tokens that are decoded between two consecutive sentinel tokens corresponds to the corrupted spans that are recovered. 따라서 이 괄호 안의 내용(parentheses)은 다음 토큰 예측 목적 함수와의 차이를 나타낸다. So these parentheses is basically a shift from the next token prediction objective function.

<br>

Q. 디코딩 과정을 좀 더 자세히 설명해주세요. (Can you elaborate with the decoding process?)

누락된 텍스트를 나타내는 센티널 토큰(sentinel tokens that denote some missing text)이 있다. 디코딩하려는 것은 바로 그 누락된 텍스트(missing text)이다. 따라서 디코더의 출력은 재구성된(reconstructed) 각 스팬과 정확히 일치한다. 학습 과정이 궁금하다면 디코더에 모든 데이터를 입력한 다음, 한 번에 전체를 재구성하는(reconstruct everything at once) 교사 강제 방식(teacher forcing mechanism)을 사용한다.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 3.58.18.png)

<br>

이제 또 다른 유형의 트랜스포머에 대해 이야기해보자. 이 트랜스포머는 인코더/디코더 구조를 가지고 있으며 디코더는 고려하지 않고(forget about the decoder) 인코더만 사용한다(just deal with the encoder). 이걸로는 생성 작업을 할 수 없다고(cannot do generation) 생각할 수도 있다. 바로 그것이 핵심이다. 그래서 이 모델은 분류에 더 특화된 작업에 사용할 수 있는 인코더 표현(representations that can be used for tasks that might be more geared towards classification)을 가지고 있다. 감정 추출(sentiment extraction), 토큰 분류(token classification)와 같이 특정 언어 모델들을 사용했던 작업들을 이제 트랜스포머의 인코더 부분을 사용해 수행할 수 있다. 나중에 3키 인코더만 사용하는 모델(a three-key encoder only models)에 대해 좀 더 자세히 살펴보겠다. BERT는 이 분야에서 중심적인(central) 역할을 한다고 할 수 있다. 그리고 DistilBERT와 RoBERTa는 개선의 축을 중심으로 연구되고 있다(investicating on axis of improvements).

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 3.58.29.png)

<br>

그리고 한 가지 마지막 유형이 있다. 오늘날의 LLM은 인코더 부분을 아예 제거한다(remove). 인코더가 없으면 스택된 인코더의 끝에 있는 인코더 임베딩이 없어져서 크로스 어텐션에 입력할 수 없게된다. And then when you have no encoder, you don't have your encoder embeddings at the end of your stacked encoders that could be fed to the cross-attention. 따라서 이 모듈은 완전히 사라진다. 그래서 스택된 디코더 각각에는(each decoder that is stacked) 마스크된 셀프 어텐션(masked self-attention)과 FFN이 포함되어 있다. And yeah, that's basically something that has caught up since then.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 3.58.44.png)

<br>

이러한 모델들의 선호도를 살펴보면 초기에는 인코더 부분이 디코더 표현을 얻는데 매우 유용하다는(the encoder part is very useful for getting to the decoder's representation) 가설에 기반한 트랜스포머와 같은(transformer-like architecture) 아키텍처가 인기였다. 하지만 시간이 지나면서 사람들은 컴퓨팅 자원(compute budget)을 디코더에만 투자하는(invested) 것이 가장 효율적이라는 것을 깨달았다. 그리고 나서 확장(scale up)과 일반화(generalize)가 가장 쉬운 작업(tasks), 즉 다음 단어 예측(next word prediction)과 같은 작업에 더 많은 투자(investment)가 이루어졌다. 앞서 언급한 T5 작업처럼 맞춤형(more bespoke) 작업이 필요한 경우와는 대조적이다. And then there has been more investment into the kind of tasks that is, I would say, easiest to scale up and generalize -- next-world prediction -- as opposed to the task I just methioned for T5, which might be more bespoke. 그래서 무엇인가를 손상시켜야(corrupt things) 한다. 그래서 더 복잡하다. 반면에 다음 단어 예측은 가장 간단한 것이다. 그리고 이것은 놀라운 놀라운 결과를 보여주었고 오늘날의 애플리케이션들이 주로 사용하는 유용한 챗봇 역할에 매우 잘 부합했다. Whereas next-word prediction is, I would say, the simplest thing you can do. And it proved to be wonders and well-aligned with the task of being a helpful kind of chatbot, which is like twday's applications mainly.

<br>

![Screenshot](/assets/img/2026-01-21/스크린샷 2026-01-22 오후 3.58.53.png)

<br>

그리고 디코더만 있는(decoder-only) 아키텍처에 대해서는 이번 시간에 다루지 않겠다. 하지만 앞으로 진행될 강의에서 LLM에 대해 더 자세히 다룰 때 이것이 핵십 주제(central part)가 될 것이다.

<br>

## BERT Deep Dive

<br>

이제 인코더만 있는(encoder-only) 아키텍처와 BERT에 대해 자세히 살펴보겠다.

<br>

먼저 BERT가 무엇을 의미하는지 알아보자. BERT는 Bidirectional Encoder Representations from Transformers의 약어(acronym)이다. 그리고 이 약어의 각 부분이 무엇에 해당하는지(what does it correspond to?) 알아보자.

<br>

가장 이해하기 쉬운 인코더 부분부터 시작하자.

<br>

앞서 말한 것처럼 디코더를 제거했다(dropped). 따라서 트랜스포머로부터의 이 인코더는 말 그대로 인코더의 역할을 한다. So this encoder from transformer is basically exactly what it means.

<br>

이제 다른 부분으로 넘어가서(now on the other part) 왜 양방향성(bidirectionality)에 대해 이야기하는 걸까? 이 논문의 결과가 주목할 만한(remarkable) 이유는 주어진 입력으로부터 각 토큰에 대한 모든 요소를 고려한(attended to everything for each token) 출력 표현(output representations)을 얻을 수 있기 때문이다.

<br>

이것이 바로 그 이유(case)이다. 인코더만 있기 때문에 다른 모든 토큰에 진정으로 주의를 기울이는(truly attends) 이 셀프 어텐션 레이어를 가지고 있다.

<br>

그리고 이는 마스크가 어텐션 메커니즘을 인과적으로(causal) 만든다고 했던 마스크드 셀프 어텐션(masked self-attention)과는 대조적(contrast)이다. 즉 모든 토큰은 자기 자신과 그 이전의 토큰들에 주의를 기울일 수 있다(attend). 그리고 이것은 논문에서 저자들이 많이 논하는 내용인데 GPT가 처음 나왔을 때는 진정한 양방향성이 아니었다(GPT they're not truly bidirectional). 하지만 분류 작업(classification tasks)에 사용할 수 있는 이러한 인코딩 방식은 진정한 양방향성을 갖는다.

<br>

Q. 마스크가 없을 때 각 토큰이 서로 상호작용할 수 있나요? (Can each token attend to each other when you don't have the mask?)

A. 맞습니다. (That's exactly right.) 그리고 마스크는 토큰이 그 뒤에 오는 토큰으로 연결되는 것을 막기(prevent links from tokens to those that come after them) 위해 존재하는 것입니다.

<br>

이 논문을 맥락에 맞춰 설명하겠다. 당시 자연어 처리(NLP) 분야는 호황을 누리고(booming) 있었다. 같은 해에 ELMo, 즉 Embeddings from Language Models라는 획기적인(landmark) 논문이 발표되었다. 나는 그 논문의 발표 시기가 좀 아쉬웠다고(unfortunate) 생각한다. 왜냐하면 양방향 표현(bidirectional representations)을 구축하는 데 있어 정말 새로운 통찰력을 제공했기 때문이다. 하지만 알고 보니 ELMo는 트랜스포머와 같은 해에 트랜스포머 기반의 유사한 연구 분야에서 나왔다. But it just turns out that it came the same year as the transformer, as the kind of transformer-based same line of work. 그래서 트랜스포머에 가려져 잘 알려지지 않았다. ELMo의 핵심적인 특징은 여러 층의 레이어가 겹겹이 쌓인(multiple layers, each stacked on top of each other) 양방향 LSTM을 기반으로 한다는 것이다. 이 아키텍처 덕분에 각 단어에 대한 양방향 표현을 구축할 수 있었다. 그렇다면 왜 BERT만큼 인기를 얻지 못했을까? 그 이유는 이전 모델들과 마찬가지로(as the previous models) 이러한 반복성(반복되는 패턴)(recurrence) 때문에 확장성이 떨어진다는(hard to scale) 단점(same downsides)이 있기 때문이다. 그리고 많은 사람들이 ELMo와 BERT를 생각할 때 처음에는 이러한 논문들을 떠올리지 않을 거라고 생각한다. 왜냐하면 그들은 Sesame Street의 캐릭터이기 때문이다.

<br>

## References

🎥 (Lecture) <https://www.youtube.com/watch?v=yT84Y5zCnaA>

📄 (Slide) <https://cme295.stanford.edu/slides/fall25-cme295-lecture2.pdf>