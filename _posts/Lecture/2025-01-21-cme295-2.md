---
title: "[Stanford][CME 295] Lecture #2 Transformer-Based Models & Tricks"
date: 2026-01-21 09:20:00 +0900
categories: [Lecture]

published: true
use_math: true
---

---

&nbsp;

## Lecture 1 Recap

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤ì „ 10.46.13.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤ì „ 10.46.25.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤ì „ 10.46.35.png)

<br>

1ê°•ì—ì„œëŠ” ì…€í”„ ì–´í…ì…˜(self-attention) ê°œë…ì— ëŒ€í•´ ì†Œê°œí–ˆë‹¤. ì…€í”„ ì–´í…ì…˜ì´ë€ ê° í† í°ì´ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì‹œí€€ìŠ¤ ë‚´ì˜ ë‹¤ë¥¸ ëª¨ë“  í† í°ë“¤ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì¿¼ë¦¬(query, $q$), í‚¤(key, $k$), ë°¸ë¥˜(value, $v$)ë¼ëŠ” í‘œê¸°ë²•(notation)ì„ ì‚¬ìš©í•œë‹¤. ì—¬ê¸°ì„œ í•µì‹¬ì€ ì¿¼ë¦¬ì™€ í‚¤ë¥¼ ë¹„êµí•¨ìœ¼ë¡œì¨ ì¿¼ë¦¬ê°€ ìì‹ ê³¼ ê°€ì¥ ìœ ì‚¬í•œ í† í°ì„ ì°¾ëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ê·¸ ê³¼ì •ì´ ì™„ë£Œë˜ë©´ í•´ë‹¹ í† í°ê³¼ ê´€ë ¨ëœ ë°¸ë¥˜ë¥¼ ê°€ì ¸ì˜¤ê²Œ ëœë‹¤.

ì…€í”„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ê³µì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì´ ê³µì‹ì€ ëŒ€ê·œëª¨ í–‰ë ¬ ê³±(matrix multiplication)ì¸ë° í•˜ë“œì›¨ì–´ëŠ” ì´ëŸ¬í•œ ì—°ì‚°ì„ ì²˜ë¦¬í•˜ëŠ” ë° ë§¤ìš° ëŠ¥ìˆ™í•˜ê³ (capable) ë§¤ìš° ìµœì í™”ë˜ì–´(optimized) ìˆë‹¤.

ì´ ëª¨ë“  ê²ƒì„ í†µí•´ íŠ¸ëœìŠ¤í¬ë¨¸(transformer) ì•„í‚¤í…ì²˜ë¥¼ ì†Œê°œí–ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” í¬ê²Œ ë‘ ê°€ì§€ êµ¬ì„±ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. ì¸ì½”ë”(encoder, the left side)ì™€ ë””ì½”ë”(decoder, the right side)ì´ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ê¸°ê³„ ë²ˆì—­(machine translation) ë¶„ì•¼ì—ì„œ ì²˜ìŒ ë„ì…ë˜ì—ˆë‹¤. ì¸ì½”ë”ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì›ë˜ ì–¸ì–´(source language)(ì˜ˆë¥¼ ë“¤ë©´ ì˜ì–´)ë¡œ ì²˜ë¦¬í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. ë””ì½”ë”ëŠ” ë²ˆì—­ëœ ë‚´ìš©(translation)ì„ ëŒ€ìƒ ì–¸ì–´(target language)(ì˜ˆë¥¼ ë“¤ë©´ í”„ë‘ìŠ¤ì–´)ë¡œ í•´ë…í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. ë©€í‹° í—¤ë“œ ì–´í…ì…˜(Multi-Head Attention) ë ˆì´ì–´ì—ì„œ ì…€í”„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì‘ë™í•œë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 2.02.46.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 2.09.59.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 2.10.10.png)

<br>

ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ í—¤ë“œ(head)ê°€ ìˆë‹¤. ê·¸ê²Œ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ” ê±¸ê¹Œ? íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸(Attention Is All You Need) ì† ê·¸ë¦¼ì—ì„œ ì´ëŸ¬í•œ í—¤ë“œë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ê° í—¤ë“œëŠ” ëª¨ë¸ì´ ì…ë ¥ì„ ì¿¼ë¦¬, í‚¤, ë˜ëŠ” ë°¸ë¥˜ë¡œ íˆ¬ì˜í•˜ëŠ” í•œ ê°€ì§€ ë°©ë²•ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê¸°íšŒ(an opportunity for the model to learn one way of projecting the input into being a query, a key, or a value)ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.

ì˜ˆë¥¼ ë“¤ë©´ ì¿¼ë¦¬ì™€ í‚¤ì˜ ê²½ìš° ì´ê²ƒ(ì‘ì€ í°ìƒ‰ ë°•ìŠ¤)ì€ ê°ê°ì˜ í—¤ë“œë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë”°ë¼ì„œ ì´ ìƒìë“¤(ë³´ë¼ìƒ‰ ë°•ìŠ¤)ì˜ ê°œìˆ˜ëŠ” í—¤ë“œì˜ ê°œìˆ˜($h$)ì™€ ê°™ë‹¤.

ì´ê²ƒì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ë” ì˜ ì‹œê°í™”í•˜ê³  ì´í•´í•˜ê¸° ìœ„í•´ ë…¼ë¬¸ì—ì„œ ê° í—¤ë“œê°€ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ í•´ì„í•˜ëŠ” ë°©ë²•ì„ ì°¸ê³ í•œë‹¤. ì–´í…ì…˜ ë§µ(attention map)ì€ ê° ì¿¼ë¦¬(dot product query)ì˜ ë°¸ë¥˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ìš°ë¦¬ëŠ” ì™¼ìª½ì˜ ì–´í…ì…˜ ë§µì—ì„œ í† í° "its"ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¤ë¥¸ í† í°ì´ ë¬´ì—‡ì¸ì§€ ì°¾ê³ ì í•œë‹¤. ê·¸ë˜ì„œ ì–‘(quantities), ì¦‰ "its"ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¿¼ë¦¬ì™€ ë‹¤ë¥¸ ëª¨ë“  í‚¤ë“¤ì˜ ë‚´ì (dot product)ì„ ì‚´í´ë³¸ë‹¤. ê·¸ë¦¬ê³  ì¿¼ë¦¬ì™€ í‚¤ì˜ ë‚´ì ì„ ë†’ì€ ë°¸ë¥˜ë¡œ ì´ë„ëŠ”(leading) ë‹¤ë¥¸ í‚¤ë“¤ì„ ì°¾ëŠ”ë‹¤. ê·¸ëŸ¬ë©´ ë…¼ë¬¸ì—ì„œì²˜ëŸ¼ "Law"ì™€ "application" ë‘ ë‹¨ì–´ê°€ ë†’ì€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜(attention weight)ë¡œ ê°•ì¡°ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë€ ì¿¼ë¦¬ "its"ì™€ ë‹¤ë¥¸ í† í°ë“¤ì˜ í‚¤ì˜ ë‚´ì ì´ë‹¤. ì´ëŸ¬í•œ ë‹¨ì–´ë“¤ì„ í•´ì„í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤. ì—¬ê¸°ì„œ ê°•ì¡° í‘œì‹œëœ í† í°ì€ "Law"ì™€ "application"ì¸ë° í† í° "its"ê°€ "Law"ë¥¼ ì§€ì¹­í•˜ê¸°(referring) ë•Œë¬¸ì— ë‹¹ì—°í•œ ê²°ê³¼ì´ë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì€ ì´ëŸ¬í•œ ë‹¨ì–´ë“¤ì„ ì´ì „ì— ë°œìƒí•œ ì‚¬ê±´ê³¼ ì—°ê²°í•˜ëŠ” ë°©ë²•(how to associate these words with what happended before)ì„ í•™ìŠµí•´ì•¼ í•œë‹¤. ê·¸ë¦¬ê³  "its"ëŠ” "application"ì„ ì˜ë¯¸í•˜ê¸°ë„ í•˜ëŠ”ë° ì´ê²ƒì€ ê·¸ëŸ¬í•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ëŠ” ë˜ ë‹¤ë¥¸ ê²½ìš°ì´ë‹¤. ê·¸ë˜ì„œ ì €ìë“¤ì€ ì´ëŸ¬í•œ ë°¸ë¥˜ë“¤ì„ ê°ê¸° ë‹¤ë¥¸ í—¤ë“œë“¤ì˜ í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚´ê¸°ë¡œ í–ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì™¼ìª½ì€ í—¤ë“œì˜ ê°•ë„(intensity), ì¦‰ ì²« ë²ˆì§¸ í—¤ë“œì— ëŒ€í•œ ê°•ë„ì´ë‹¤. ê·¸ë¦¬ê³  ë‘ ë²ˆì§¸ í—¤ë“œëŠ” "Law"ì— ëŒ€í•œ ê°•ë„ê°€ ë§¤ìš° ë†’ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. ê·¸ë˜ì„œ ê°„ë‹¨íˆ ë§í•˜ë©´ ì´ëŸ¬í•œ í—¤ë“œë“¤ì€ ì–´ë–¤ ë‹¨ì–´ê°€ ì¤‘ìš”í•œì§€(what words matters) íŒŒì•…í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

<br>

Q. ì´ëŸ¬í•œ ëª¨ë“  ê³„ì‚°ì„ ìˆ˜í–‰í•  ë•Œ ê° ê³„ì‚°ì€ ì„œë¡œ ë‹¤ë¥¸ MLPë¥¼ ê±°ì¹˜ëŠ”ê°€? (Are they going through different MLPs when we're doing all these computations?)

A. í—¤ë“œë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ íˆ¬ì˜ í–‰ë ¬ì„ ì‚¬ìš©í•œë‹¤. (We're going to have different projection metrices for each of them.)

ìš°ë¦¬ëŠ” ì‹¤ì œë¡œ ê° í—¤ë“œê°€ ìì²´ì ì¸ íˆ¬ì˜(its own projection)ì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì— ëŒ€í•œ ìì„¸í•œ ì˜ˆë¥¼ ë‹¤ë¤˜ë‹¤. ê·¸ë¦¬ê³  ë™ì‹œì—(in parallel) í•´ë‹¹ ê³„ì‚°ì´ ì§„í–‰ëœë‹¤. ê° í—¤ë“œëŠ” í•˜ë‚˜ì˜ ê²°ê³¼ë¥¼ ìƒì„±í•œë‹¤. ê·¸ëŸ¬ë©´ ì´ ê²°ê³¼ë“¤ì€ ì—°ê²°ëœ(concatenated) ë‹¤ìŒ ë‹¤ì‹œ í•œë²ˆ íˆ¬ì˜ë˜ì–´(projected once again) ì¶œë ¥ í–‰ë ¬(output matrix)ì´ ëœë‹¤. ê°„ë‹¨íˆ ë§í•˜ë©´ ê³ ë„ë¡œ ë³‘ë ¬í™”(highly parallelized) ë˜ì–´ ìˆë‹¤. ì—¬ê¸°ì—ëŠ” í–‰ë ¬ ê³±ê³¼ ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax) ì—°ì‚°ì´ í¬í•¨ëœë‹¤.

<br>

## Position Embeddings

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 4.12.25.png)

<br>

2017ë…„ì— ë„ì…ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” ìˆ˜ë…„ ë™ì•ˆ ì—¬ì „íˆ ìœ íš¨í•œ ì•„í‚¤í…ì²˜ì´ë‹¤. ëª‡ ê°€ì§€ êµ¬ì„±ìš”ì†Œê°€ ì•½ê°„ ë³€ê²½ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ ì˜¤ëŠ˜ë‚ ì˜ ëª¨ë¸ë“¤ì€ ê±°ì˜ ëŒ€ë¶€ë¶„ ì´ˆê¸°(initial) íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤.

íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì—ì„œ ì¤‘ìš”í•œ ì²« ë²ˆì§¸ ê°œë…ì€ ìœ„ì¹˜ ì„ë² ë”©(position embedding)ì´ë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 4.12.35.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 4.12.46.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 4.12.56.png)

<br>

í† í°ë“¤ì€ ë‹¤ë¥¸ ëª¨ë“  í† í°ë“¤ê³¼ ì„œë¡œ ì§ì ‘ì ìœ¼ë¡œ(in a direct fashion) ìƒí˜¸ì‘ìš©í•œë‹¤. ì¦‰ ì§ì ‘ì ì¸ ì—°ê²°(direct links)ì´ ìˆë‹¤. í•˜ì§€ë§Œ ê° í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” RNNê³¼ ë‹¬ë¦¬ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” ì–´ë–¤ í† í°ì´ ë‹¤ë¥¸ í† í°ë³´ë‹¤ ë¨¼ì € ì²˜ë¦¬ëœë‹¤ëŠ” ê°œë…ì´ ì‚¬ë¼ì§€ê²Œ ëœë‹¤. ì¦‰ ìœ„ì¹˜ ì •ë³´(position information)ë¥¼ ìƒê²Œ ëœë‹¤.

ê²°ê³¼ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ê° ìœ„ì¹˜ì˜ í† í°ì„ ì •ëŸ‰í™”í•˜ê³ (quantify) íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì…ë ¥ì„ ì²˜ë¦¬í•  ë•Œ ê·¸ ì •ë³´ë¥¼ ì£¼ì…í•´ì•¼(inject) í•œë‹¤. ê·¸ë˜ì„œ ì´ˆê¸° íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì˜ ì €ìë“¤ì€ ì „ìš© ì„ë² ë”©(dedicated embedding)ì„ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆë‹¤. ì—¬ê¸°ì„œ ì „ìš©(dedicated)ì€ ê° ìœ„ì¹˜ì— í•˜ë‚˜ì˜ ì„ë² ë”©ì´ ìˆë‹¤(each position has one embedding)ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. 1ë²ˆ ìœ„ì¹˜ì— í•œ ê°œì˜ ì„ë² ë”©ì´ ìˆê³  2ë²ˆ ìœ„ì¹˜ì—ë„ í•œ ê°œì˜ ì„ë² ë”©ì´ ìˆëŠ” ì‹ì´ë‹¤. ì €ìë“¤ì€ ì…ë ¥ í† í° ì„ë² ë”©(input token embedding)ì— ìœ„ì¹˜ ì„ë² ë”©ì„ ì¶”ê°€í–ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "A cute teddy bear is reaing." ë¬¸ì¥ì—ì„œ í† í° "A"ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì²« ë²ˆì§¸ ìœ„ì¹˜ì— ì²« ë²ˆì§¸ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ë² ë”©(embedding representing the first position)ì´ ë”í•´ì§„ë‹¤.

<br>

Q. ìœ„ì¹˜ ì„ë² ë”©ì€ í•™ìŠµë˜ë‚˜ìš” ì•„ë‹ˆë©´ ê³ ì •ì¸ê°€ìš”? (Are the position embeddings learned or static?)

A. í•™ìŠµë˜ê¸°ë„ í•˜ê³  ê³ ì •ì´ê¸°ë„ í•˜ë‹¤. (Both.)

ì €ìë“¤ì´ ë‘ ê°€ì§€ ë°©ë²•ì„ ëª¨ë‘ ì‹œë„í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ë‘ ë²ˆì§¸ ë°©ë²•ì´ ë¬´ì—‡ì¸ì§€ ì‚´í´ë³´ê² ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì„ë² ë”©ì´ í•™ìŠµë˜ì—ˆë‹¤ê³ (they are learned) ê°€ì •í•´ ë³´ì. ì´ëŠ” ê° ìœ„ì¹˜ì— ëŒ€í•œ ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ”(learn embeddings for each position) ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ì ‘ê·¼ ë°©ì‹ì˜ ë¬¸ì œì ì€ í•™ìŠµ ë°ì´í„°ì…‹(training set)ì— ë§¤ìš° ì˜ì¡´ì (dependent)ì´ë¼ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì—¬ê¸°ì—ì„œì²˜ëŸ¼ í•­ìƒ ë‘ ë²ˆì§¸ ìœ„ì¹˜ì—ì„œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ëŠ” í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ í•™ìŠµëœ ì„ë² ë”©(learned embeddings)ì€ ê·¸ëŸ¬í•œ í¸í–¥(bias)ì´ í•™ìŠµë  ìˆ˜ ìˆë‹¤. ì´ê²ƒì´ ì²« ë²ˆì§¸ í•œê³„ì´ë‹¤. ë‘ ë²ˆì§¸ í•œê³„ëŠ” í•™ìŠµí•  ìˆ˜ ìˆëŠ” ìœ„ì¹˜ì˜ ê°œìˆ˜ê°€ í•™ìŠµ ë°ì´í„°ì…‹ì— ìˆëŠ” ìµœëŒ€ ìœ„ì¹˜ ê°œìˆ˜ë§Œí¼ì´ë¼ëŠ” ê²ƒì´ë‹¤(only learn positions up to the max number of position that is in your training set). ì˜ˆë¥¼ ë“¤ì–´ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ìµœëŒ€ 512ê°œì˜ ì‹œí€€ìŠ¤(sequences that are up to 512)ë¡œ í•™ìŠµì‹œí‚¨ë‹¤ê³  ê°€ì •í•´ ë³´ì. ìš°ë¦¬ëŠ” í•´ë‹¹ ìœ„ì¹˜ê¹Œì§€ì˜(up to that position) ìœ„ì¹˜ ì„ë² ë”©ë§Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

<br>

Q. ì–´ë–»ê²Œ ìœ„ì¹˜ ì„ë² ë”©ì„ íŒŒë¼ë¯¸í„°í™”í•  ìˆ˜ ìˆì„ê¹Œ? (How do we parameterize that?)

A. ìœ„ì¹˜ 1ë¶€í„° ìœ„ì¹˜ 512 ì‚¬ì´ì— í•™ìŠµ ê°€ëŠ¥í•œ ìœ„ì¹˜ ì„ë² ë”©ì— ëŒ€í•œ ì¼ì¢…ì˜ ìë¦¬ í‘œì‹œì(a kind of placeholder of a learnable position embedding between 1 and 512)ë¥¼ ë‘ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ê²½ì‚¬í•˜ê°•ë²•(gradient descent) ë“±ì„ í†µí•´ ì´ëŸ¬í•œ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. ì´ê²ƒì´ ì²« ë²ˆì§¸ ë°©ë²•ì´ë‹¤. í•˜ì§€ë§Œ ì•ì„œ ì–¸ê¸‰í–ˆë“¯ ì´ ë°©ë²•ì—ëŠ” í•œê³„ê°€ ìˆë‹¤. í•™ìŠµ ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ëŠ” ìµœëŒ€ ìœ„ì¹˜ê¹Œì§€ì˜ ìœ„ì¹˜ ì„ë² ë”©ë§Œ í•™ìŠµí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì¶”ë¡ í•  ë•Œ í›ˆë ¨ ë°ì´í„°ì…‹ì— ìˆë˜ ìœ„ì¹˜ë¥¼ ë²—ì–´ë‚œ(beyond the position that was in the training set) ìœ„ì¹˜ê°€ ìˆë‹¤ë©´ ê·¸ ìœ„ì¹˜ëŠ” í•™ìŠµë˜ì§€ ì•Šì€ ìƒíƒœì´ë‹¤. ê·¸ë˜ì„œ ê·¸ ê°’ì„ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì„ ì°¾ì•„ì•¼ í•œë‹¤. ì´ê²ƒì´ ë‘ ë²ˆì§¸ í•œê³„ì ì´ë‹¤. í•˜ì§€ë§Œ ì¥ì ìœ¼ë¡œëŠ” ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ë„ë¡ ë‚´ë²„ë ¤ë‘”ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” ê²½ì‚¬í•˜ê°•ë²•ì´ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•˜ëŠ” ë° ìˆì–´ì„œ ë†€ë¼ìš´ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ ì €ìë“¤ì€ ì´ëŸ¬í•œ ë°©ë²•ì´ ë‘ ë²ˆì§¸ ë°©ë²•ê³¼ í•¨ê»˜ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ê³  ë§í–ˆë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.09.08.png)

<br>

ë‘ ë²ˆì§¸ ë°©ë²•ì€ ìœ„ì¹˜ ì„ë² ë”©ì— í•´ë‹¹í•˜ëŠ” ê° ì°¨ì›ì— ëŒ€í•´(for each dimension corresponding to a position embedding) ì„ì˜ì˜ ê³µì‹ì„ ì·¨í•˜ëŠ” ê²ƒì´ë‹¤. ì²« ë²ˆì§¸ ë°©ë²•ì€ ìœ„ì¹˜ ë‹¹ í•˜ë‚˜ì˜ ì„ë² ë”©(one embedding per position)ì„ ì‚¬ìš©í•˜ê³  ê·¸ ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ì—ˆë‹¤. ë‘ ë²ˆì§¸ ë°©ë²•ì€ ìœ„ì¹˜ ë‹¹ í•˜ë‚˜ì˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì§€ë§Œ ê·¸ ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ì‚¬ì „ì— ì •í•´ì§„(predetermined) ê²ƒì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ì €ìë“¤ì€ ì‚¬ì¸ í•¨ìˆ˜ì™€ ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ê³µì‹ì„ ì„ íƒí–ˆë‹¤. ì—¬ê¸°ì„œ í•µì‹¬ì€ ì£¼ì–´ì§„ ìœ„ì¹˜ $\text{m}$ì— ëŒ€í•´ í¬ê¸°ê°€ $\text{d}$ì¸ ë²¡í„° ëª¨ë¸(a vector of size d model)ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤. í† í°ë“¤ì„ ë”í•´ì•¼(adding) í•˜ê¸° ë•Œë¬¸ì— $\text{d}$ëŠ” í† í° ì„ë² ë”©ì˜ ì°¨ì›ê³¼ ì¼ì¹˜í•´ì•¼ í•œë‹¤. ê·¸ë¦¬ê³  ëª¨ë“  ì¸ë±ìŠ¤ì— ëŒ€í•´ ì´ ê³µì‹ë“¤ì— ëŒ€í•œ ê°’ì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤. ì²« ë²ˆì§¸ ê³µì‹ì€ ì–´ë–¤ ê²ƒê³¼ $\text{m}$ì˜ ê³±ì— ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì·¨í•œ ê²ƒì´ë‹¤. ë‘ ë²ˆì§¸ ê³µì‹ì€ ì–´ë–¤ ê²ƒê³¼ $\text{m}$ì˜ ê³±ì— ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì·¨í•œ ê²ƒì´ë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.09.10.png)

<br>

í‘œê¸°ë²•ì„ ë‹¨ìˆœí™”í•˜ì. $\omega_i=10000^{-\frac{2i}{d_{\text{model}}}}$ë¼ ê°€ì •í•˜ì. ì¦‰ $\omega$ê°€ $i$ì™€ $m$ì˜ ê³±ì— ëŒ€í•œ í•¨ìˆ˜ë¼ê³ (as a function of i times m) ê°€ì •í•˜ì.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.09.24.png)

<br>

ìš°ë¦¬ëŠ” ì„œë¡œ ê°€ê¹Œì´ ìˆëŠ” ë‹¨ì–´ì¼ìˆ˜ë¡ ê´€ë ¨ì„±ì´ ë†’ê³  ì„œë¡œ ë©€ë¦¬ ìˆëŠ” ë‹¨ì–´ì¼ìˆ˜ë¡ ê´€ë ¨ì„±ì´ ë‚®ë‹¤ëŠ”(words that are close together are liekly to be more relevant, as opposed to words that are further together) ì‚¬ì‹¤ì„ ë°˜ì˜í•˜ë©° ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ê³ ì í•œë‹¤. ì¦‰ ë‘ ë‹¨ì–´ê°€ í•œ ê°œì˜ ìœ„ì¹˜ë§Œ ë–¨ì–´ì ¸ ìˆëŠ” ê²½ìš°ì™€ ë§Œ ê°œì˜ ìœ„ì¹˜ê°€ ë–¨ì–´ì ¸ ìˆëŠ” ê²½ìš°ê°€ ìˆë‹¤ë©´ í•œ ê°œì˜ ìœ„ì¹˜ë§Œ ë–¨ì–´ì§„ ê²½ìš°ê°€ ë” ìœ ì‚¬í•´ì•¼(similar) í•œë‹¤. ê³µì‹ì´ íƒ€ë‹¹í•œì§€ ì‚´í´ë³´ì. ë‘ ê°œì˜ ìœ„ì¹˜ ì„ë² ë”©ì´ ìˆë‹¤. í•˜ë‚˜ëŠ” ìœ„ì¹˜ $m$ì— í•˜ë‚˜ëŠ” ìœ„ì¹˜ $n$ì— ìˆë‹¤. ê·¸ë¦¬ê³  ì‚¬ì „ì— ì •í•´ì§„ ê³µì‹ì„ ì‚¬ìš©í•´ ëª¨ë“  ê°’ì„ ê³„ì‚°í–ˆë‹¤ê³  ê°€ì •í•´ ë³´ì.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.09.33.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.11.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.22.png)

<br>

ì‚¼ê°í•¨ìˆ˜ ê³µì‹(trigonometry formulas)ì„ ê¸°ì–µí•œë‹¤ë©´ $\cos(a-b)=\cos(a)\cos(b)+\sin(a)\sin(b)$ì´ë‹¤.

$\cos(\omega_i(m-n))$ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤.

ì•Œê³  ë³´ë‹ˆ $\cos(\omega_i(m-n))$ëŠ” ë‘ ìœ„ì¹˜ ì„ë² ë”©ì˜ ë‚´ì ì„ ê³„ì‚°í•  ë•Œ ë‚˜íƒ€ë‚˜ëŠ” ìš”ì†Œ ì¤‘ í•˜ë‚˜(one component)ì¼ë¿ì´ë‹¤.

ìœ„ì¹˜ $m$ê³¼ ìœ„ì¹˜ $n$ì„ ë‚´ì í•  ë•Œ ì²« ë²ˆì§¸ ìœ„ì¹˜ë¥¼ ê°€ì ¸ì™€ì„œ(take), ê·¸ë“¤ì„ ê³±í•˜ê³ (multiply), ë‘ ë²ˆì§¸ ìœ„ì¹˜ì— ê°’ì„ ë”í•˜ê³ (plus), ë‹¤ì‹œ ê·¸ë“¤ì„ ê³±í•˜ëŠ” ì‹ìœ¼ë¡œ ê³„ì†í•œë‹¤. ê·¸ë¦¬ê³  ì´ê²ƒì´ $\cos(\omega_i(m-n))$ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ê²°êµ­ ì´ëŸ¬í•œ ì„ë² ë”©ë“¤ì˜ ë‚´ì ì„ ìˆ˜í–‰í•˜ë©´ $m$ê³¼ $n$ ì‚¬ì´ì˜ ìƒëŒ€ ê±°ë¦¬ì— ëŒ€í•œ í•¨ìˆ˜ì¸ ì½”ì‚¬ì¸ì˜ í•©(a sum of cosine that are a function of the relative distance between m and n) ì–»ê²Œ ëœë‹¤.

<br>

Q. ë‘ ì„ë² ë”©ì´ ê°€ê¹Œìš¸ìˆ˜ë¡ ë” ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì¸ê°€? (The closer they are, the more similar they are?)

A. ì§ê´€ì ìœ¼ë¡œ ë§í•˜ë©´ ì´ëŸ¬í•œ ì„ë² ë”© ê³µì‹í™” ë°©ë²•ì€(way of formulating the embeddings) ê·¼ì‚¬í™”í•˜ê±°ë‚˜(trying to approximate) ëª¨ë°©í•˜ë ¤ëŠ”(trying to mimic) ê²ƒì´ë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.29.png)

<br>

ê·¸ë˜ì„œ ë‚´ì  $\left\langle \text{PE}_m, \text{PE}_n \right\rangle$ì„ ì–»ê²Œ ë˜ëŠ”ë° ì´ëŠ” $m$ê³¼ $n$, ì¦‰ ë‘ ì„ë² ë”© ì‚¬ì´ì˜ ìƒëŒ€ ê±°ë¦¬ì— ëŒ€í•œ í•¨ìˆ˜ì´ë‹¤.

ë‹¤ì‹œ í•œë²ˆ ìƒê¸°í•˜ìë©´ ì™œ ë‚´ì ì„ ê³ ë ¤í•´ì•¼ í• ê¹Œ? ì„ë² ë”© ì„¸ê³„ì—ì„œ(in the embedding world) ë‘ ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ì„±(similarity)ì„ ì •ëŸ‰í™”í• (quantify) ë•Œ ì£¼ë¡œ ë‘ ì„ë² ë”©ì˜ ë‚´ì ì„ ìˆ˜ë°˜í•œë‹¤(involving). ì¼ë°˜ì ìœ¼ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„(cosine similarity)ë¥¼ ì‚¬ìš©í•œë‹¤. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ê° ì„ë² ë”©ì˜ ë…¸ë¦„(norm)ì— ëŒ€í•œ ë‚´ì ì´ë‹¤. ì¦‰ ê¸°ë³¸ì ìœ¼ë¡œ ë‚´ì ì´ë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ê°€ ë‚´ì ì— ê´€ì‹¬ì„ ê°–ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì—¬ê¸°ì„œ ì•Œ ìˆ˜ ìˆë“¯ ì´ê²ƒì€ ë‘ ìœ„ì¹˜ ì‚¬ì´ì˜ ìƒëŒ€ ê±°ë¦¬ì— ëŒ€í•œ í•¨ìˆ˜ì´ë‹¤.

íŠ¹íˆ $\cos(0)=1$ì´ë‹¤. $\omega_i(m-n)$ê°€ í´ìˆ˜ë¡ $\cos(\omega_i(m-n))$ì€ ì‘ì•„ì§„ë‹¤. ë¬¼ë¡  ì£¼ê¸°ì„±ì´ ìˆê¸°(periodic) ë•Œë¬¸ì— ì´ ë§ì´ ë°˜ë“œì‹œ ë§ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. $\pi$ë¥¼ ì§€ë‚˜ë©´ ë°©í–¥ì€ ì™„ì „íˆ ë°˜ëŒ€ê°€ ëœë‹¤. í•˜ì§€ë§Œ ë§í•˜ê³ ì í•˜ëŠ” ê²ƒì€ $m$ê³¼ $n$ì´ ê°™ì„ ë•Œ $\cos(0)$ì— ëŒ€í•œ í•©ì„ ì–»ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ê·¸ ê°’ì€ $\left\langle \text{PE}_m, \text{PE}_n \right\rangle$ì´ ìµœëŒ€(maximum)ê°€ ë˜ëŠ” ê°’ì´ë‹¤. ì¦‰ $m$ê³¼ $n$ì´ ê°™ì„ ë•Œ $\left\langle \text{PE}_m, \text{PE}_n \right\rangle$ì´ ìµœëŒ€ê°€ ëœë‹¤. ìœ„ì¹˜ ìì²´ë§Œ ë†“ê³  ë³´ë©´ ê°€ì¥ ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.38.png)

<br>

ì„ë² ë”© ê°’ì„ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ì™¼ìª½ ê·¸ë˜í”„ì—ì„œ yì¶•ì€ 50ê°œ ìœ„ì¹˜ ê°ê°ì— ëŒ€í•œ ëª¨ë“  ì„ë² ë”©ì„ ë‚˜íƒ€ë‚¸ë‹¤. xì¶•ì€ ì£¼ì–´ì§„ ë²¡í„°ì˜ ì—¬ëŸ¬ ì°¨ì›ì— ê±¸ì¹œ(across several dimentions) ê°’ë“¤ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì²« ë²ˆì§¸ í–‰ì€ ë²¡í„°ë¥¼ ì–´ë–»ê²Œ ì¸ë±ì‹± í•˜ëŠ”ì§€ì— ë”°ë¼(depending on how you index your vector) ì²« ë²ˆì§¸ ìœ„ì¹˜(first position) ë˜ëŠ” 0ë²ˆì§¸ ìœ„ì¹˜(number 0 position) ì„ë² ë”©ì˜ ëª¨ë“  ê°’ë“¤ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ë³´ì´ëŠ” ê²ƒê³¼ ê°™ì´ ì°¨ì›ì´ ë‚®ì„ìˆ˜ë¡ ì´ ê°’ì€ ë§¤ìš° ë¹ˆë²ˆí•˜ê²Œ ì˜¤ë¥´ë‚´ë¦°ë‹¤(goes up and down very frequently). ì¦‰ ë” ë†’ì€ ì£¼íŒŒìˆ˜(high frequency)ì´ë‹¤. ê·¸ë¦¬ê³  ì°¨ì›ì´ ë†’ì„ìˆ˜ë¡ ê°’ì´ ì˜¤ë¥´ë‚´ë¦¬ëŠ” ë° ë§ì€ ì‹œê°„ì´ ê±¸ë¦°ë‹¤. ì¦‰ ë” ë‚®ì€ ì£¼íŒŒìˆ˜(low frequency)ì´ë‹¤.

ì´ê²ƒì€ ì•ì„œ ì–¸ê¸‰í–ˆë˜ $\omega_i$ì™€ ê´€ë ¨ìˆë‹¤. $\omega_i$ëŠ” ì°¨ì› $i$ì˜ ê°’ì´ ì‘ì„ ë•Œ ë§¤ìš° ë†’ë‹¤(very high). ê·¸ë¦¬ê³  $i$ì˜ ê°’ì´ í´ ë•Œ ë§¤ìš° ë‚®ë‹¤(very low). ì¦‰ $\omega_i$ëŠ” ì½”ì‚¬ì¸ê³¼ ì‚¬ì¸ ê°’ì´ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ë³€í•˜ëŠ”ì§€(how quickly cosine and sine vary)ë¥¼ ê²°ì •í•œë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.47.png)

<br>

ì´ê²ƒì´ ì›ì €ìë“¤(original authors)ì´ ì‹œë„í–ˆë˜ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ë°©ë²•ë“¤ì„ ì‚¬ìš©í•˜ë©´ í•™ìŠµëœ ë°©ë²•(learned one)ê³¼ ë¹„ìŠ·í•œ(comparable) ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ì ì— ì£¼ëª©í–ˆë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ í° ì¥ì ì´ ìˆë‹¤. í›ˆë ¨ ì‹œì ì— ê´€ì°°í–ˆë˜ ì‹œí€€ìŠ¤ ê¸¸ì´ ë¿ë§Œ ì•„ë‹ˆë¼(not just a sequence length that you saw at training time) ì–´ë–¤ ì‹œí€€ìŠ¤ ê¸¸ì´ì—ë„(to any sequence length) ì ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì´ê²ƒì´ ë°”ë¡œ ì´ ë°©ì‹ì´ ë” ë°”ëŒì§í•œ(preferable) ì´ìœ  ì¤‘ í•˜ë‚˜ì´ë‹¤. ì´ê²ƒì´ ì €ìë“¤ì´ ì„ íƒí•œ ë°©ì‹(what the authors chose)ì´ë‹¤.

2025ë…„ìœ¼ë¡œ ì‹œê°„ì„ ë˜ëŒë ¤ ë³´ì. ì•„ì§ë„ ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€("Are we still using that?") ë¬¼ì„ ìˆ˜ë„ ìˆë‹¤. ì–´ëŠ ì •ë„ëŠ” ê·¸ë ‡ë‹¤("Kind of"). ìš°ë¦¬ëŠ” ì—¬ì „íˆ ë©€ë¦¬ ë–¨ì–´ì§„(far) í† í°ì¼ìˆ˜ë¡ ê°€ê¹Œìš´(closer) í† í°ë³´ë‹¤ ìœ ì‚¬ì„±ì´ ë‚®ë‹¤ëŠ”(less similar) ì•„ì´ë””ì–´ë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ì €ìë“¤ì´ í•œ ê²ƒì²˜ëŸ¼ ì„ë² ë”©ì„ ì‚½ì…í•˜ì§€ëŠ”(injecting) ì•ŠëŠ”ë‹¤. ê·¸ ì´ìœ ë¥¼ ì•Œì•„ë³´ì.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.53.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 4.12.46.png)

<br>

ìš°ë¦¬ê°€ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€ ì…€í”„ ì–´í…ì…˜ ê³„ì‚°ì—ì„œ í† í°ë“¤ì´ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€(how similar tokens are)ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì´ë‹¤. ì…€í”„ ì–´í…ì…˜ ê³„ì‚°ì€ ì–´í…ì…˜ ë ˆì´ì–´ì—ì„œ ì´ë£¨ì–´ì§„ë‹¤. ìš°ë¦¬ëŠ” ì„ë² ë”©ì„ ê³„ì‚°í•´ì„œ ì—¬ê¸°ì—(ì…ë ¥ì—) ë”í•œë‹¤ê³ (add) í–ˆë‹¤. í•˜ì§€ë§Œ ì‚¬ì‹¤ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê±´ ì–´í…ì…˜ ë ˆì´ì–´ì— ì´ ìœ ì‚¬ì„±ì„ ë°˜ì˜í•˜ëŠ”(reflect) ê²ƒì´ë‹¤.

<br>

Q. ì…ë ¥ íŠ¹ì§•ì— ë”í•´ì§€ëŠ” ê²ƒì¸ê°€? (Is it added to the input feature?)

A. ê·¸ë ‡ë‹¤. ì²« ë²ˆì§¸ ë°©ë²•ì—ì„œëŠ” ê·¸ëƒ¥ ë”í•˜ë©´ ëœë‹¤. (Yes, so in this first method. Just add.)

<br>

í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ë ˆì´ì–´ì—ì„œ ê·¸ ì§ê´€(intuition)ì´ ê·¸ëŒ€ë¡œ ì ìš©ë˜ê¸°ë¥¼ ë°”ë€ë‹¤. ì´ê²ƒì´ ì‚¬ëŒë“¤ì´ ë‹¤ì–‘í•œ ë³€í˜•ì„ ì‹œë„í•œ ì´ìœ (why people have tried different variations) ì¤‘ í•˜ë‚˜ì´ë©° íŠ¹íˆ ì´ëŸ¬í•œ ìœ„ì¹˜ ì„ë² ë”©ì´ ì…ë ¥ì´ ì•„ë‹Œ ì–´í…ì…˜ ë ˆì´ì–´ì— ì§ì ‘ ì ìš©ë˜ë„ë¡(intervene directly) í–ˆë‹¤. ì…ë ¥ì— ì ìš©í•˜ë©´ ì–´í…ì…˜ ë ˆì´ì–´ì— ì–´ëŠ ì •ë„(roughly) ì˜í–¥ì„ ë¯¸ì¹˜ê² ì§€ë§Œ ê°„ì ‘ì ì¸(indirect) ë°©ì‹ì´ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ê°€ê¹Œìš´ í† í°ì¼ìˆ˜ë¡ ë©€ë¦¬ ìˆëŠ” í† í°ë³´ë‹¤ ë” ìœ ì‚¬í•˜ë„ë¡ ì–´í…ì…˜ ê³µì‹ì„ ì§ì ‘ ìˆ˜ì •í•œë‹¤(directly do something about the attention formula).

<br>

And the way we do that is, if you remember, the self-attention layer is basically the softmax of qk transpose over square root of d times v. So what we want is to add a little something inside that softmax, which is basically where you quantify how similar a token is to another token. You want to add a little something to reflect the fact that some tokens, they're supposed to be more similar compared to that token, compared to others.

So there's a few methods that have tried to have some variation of that. So for those of use who know the T5 paper, what we're going to see a little bit later, they have tried these relative position bias by learning the bias term, which is in the formula above. So what they did was, let's suppose you have a given distance between the positions m and n. So their idea was, let's learn that(bias(m, n)). Let's basically bucketize all m minus n into some buckets. And let's just have the model learn these quantities that are then going to be infected into softmax.

<br>

Q. So the question is, does that pose a problem that the bias is here with respect to the probability at the end? Because it has to sum to 1.
A. Well, you can do whatever you want inside the softmax, because the softmax is going to normalize it anyways.

<br>

So you can think of the bias as being something that is maybe more negative for things that are far apart, compared to things that are closer together. So T5 says, let's learn them. We have another message from, I guess, this "Train Short, Test Long" paper, which introduced this method called ALiBi. ALiBi stands for Attention with linear bias, I believe. And what they did was, say instead of learning those biases, let's actually have a deterministic formula, which is as a function of the relative difference between those two positions. And they said that. So they had some results. So all these papers, they always compare based on one another to see which one is, I guess, more performant.

<br>

But the reality is that in today's models, most models actually use another kind of position embedding method. And we're going to see it now. So this method relies on rotating the query and the key vector by some angle. And I guess you can think of it as, you have your query, you have your key, let's suppose in a 2D space. So what you're going to do is rotate your query by some angle that is a function of its position. And you're going to rotate the key vector by some angle that is a function of its position n. And I guess how do you do that, by the way? So let's suppose you have a vector, by the way. And you want to rotate that vector, because I had the answer on the slide. But how would you go about this, I guess, for people who just want to talk about this with intuition? So who here has done, I don't know, rotations in space? Matrix multiplication. And you're going to use a quantity that's called rotation matrix. And the rotation matrix is expressed as follows. So it's basically a 2 by 2 matrix in the 2D plane that has cosine of this angle, minus sine of this angle. sine of this angle, cosine of this angle. I'm looking at the time. It's quite simple to just show that it works, but we may run out of time. Do you want me to quickly show you that it's indeed a way to rotate the vector? Okay so here as a reminder whay we're trying to show is that we can use a matrix multiplication to rotate a vector in 2D space. (íŒì„œ ë”°ë¼ ê·¸ë ¤ë³¼ ê²ƒ) So let's suppose we have the following vector. That is something that you can quantify with two dimensions, let's say x and y. You can express your vector in 2D space with this. But I guess this, if you note, are the norm of the vector, and phi, the angle with respect to the x-axis. You can also write v as R with vector cosine of phi and sine of phi. So if you multiply the rotation matrix with this phi, what you're going to obtain is some multiplication of cosine minus sine, sine and cosine of this and that. And I will leave this exercise for you. But you can show that rotation times this v can be expressed as R of cosine of theta plus phi, and sine of theta plus phi. So this is a quick proof. So I'll just leave you the multiplication of the rotation matrix and v. But you will obtain these trigonometric identities that will, I guess, lead you to this formula. This basically shows that if you multiply this matrix and this vector, you're basically rotating the vector by this angle.

<br>

So I guess here, just going back to this method, what we want to do is to quantify the similarity between tokens, and have close tokens be more similar compared to tokens that are more a far. So the problem that we had with the previous methods was -- so in the first method, this learned embedding, you always had this overfitting issue. Because basically when you learn these biases, it always depends on which training set you have. So maybe your dataset is in a way that, let's say, tokens that are close are similar, but in a different way compared to what you see at inference time.

And this ALiBi method, it didn't have that learnable component, but it was quite restrictive, because it's a very simple formula after all, just the relative difference between n and m. So I guess people have tried different ways of coming up with something that tells you that, I guess, an embedding that reflects the fact that you want further positions to be less similar than closer ones. So in this method, we're going back to the sine and cosine world from what the author had proposed. And think about similarity from the lens of cosine and sine functions. So this is a little intro.

<br>

And their method is called RoFE. So I'm not sure if you've heard of it. So it stands for Rotary Position Embeddings. And we're going to see that this method -- so why do we care about this method? So this method has two great things. So the first one is that if you rotate the query and the key, you will end up with a quantity that will be a function of the relative distance between the two. That's going to be very nice, and that's why I wrote this thing on the blackboard. Not sure if you can see, by the way, but we won't have time to go into the mathematical detail. But if you want to just express these things at home, this is just the foundation.

And so in particular, if you remember your attention formula, so you have query times key transpose. So if you rotate the query by an angle m and the key by an angle n, what you're going to end up is a formula that has the rotation matrix of angle theta and, I guess, n minus m. And this is greate because this is a function of the relative distance between these two positions. OK, so why do I talk about this in detail? Well, it turns out that most models these days, they use RoFE, which is why it's important. And I would say another thing, it's maybe a little bit hard to get the intuition as to why that works. But hopefully, the explanation that I gave regarding the sine and cosine at the very beginning can help you just build that intuition.

<br>

And speaking of that, it turns out that the upper bound of the attention weight given by the query and the key is such that we observe a lone-term decay. Meaning that as n minus m is large, we do see the upper bound that gets smaller and smaller. Well, you see these little oscillations, it's not perfect either. But we do have some mathematical, I guess, results as to just the upper bounds decaying over the long-term.

<br>

Q. ìƒëŒ€ ê±°ë¦¬ê°€ íšŒì „ í–‰ë ¬ì— í¬ì°©ë˜ëŠ”ê°€? (Is the relative distance captured in the rotation matrix?)

A. ê·¸ë ‡ë‹¤. (Yes.)

<br>

Q. What is theta?
A. So theta is actually fixed. So do you remember this omega that I talked about here? So it's basically some function of i and d. I actually pass it quite quickly. But what I showed you is in the 2D space. But here we're in a d-dimensional space, which is greater than 2. So I glossed it very quickly. But the way you extend this method is by having these 2D space by block. But then the theta is a function of typically something that you fix, but a function of i, which is the dimension. It's basically between 1 and d, d over 2. It's a function of that, and it's a function of d as well. You will see this theta as being roughly euqual to omega_i, just this one, more or less, more or less.

Q. So the question is, so that it has the same dimension as the latent dimension? So, well, here you have a product of matrices. So you need to have the dimensions match. So I guess your answer -- yeah.


So this was position embeddings.

Q. So the question is about, how do you obtain this curve? So this is actually a curve that I believe is shown mathematically. So it's kind of complicated. We're not going to write down the formula. But if you're interested, so in this paper, in the RoFormer paper, there's an appendix where they show mathematically that it is upper bounded by some quantity. And this is what this is showing.

<br>

## Layer Normalization

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.13.45.png)

<br>

ìœ„ì¹˜ ì„ë² ë”©ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì•½ê°„ ë³€í˜•ëœ ë¶€ë¶„ ì¤‘ í•˜ë‚˜ì´ë‹¤. ì§€ê¸ˆê¹Œì§€ ì–´ë–»ê²Œ ë°”ë€Œì—ˆëŠ”ì§€, ê·¸ë¦¬ê³  ì™œ ë°”ë€Œì—ˆëŠ”ì§€ ì‚´í´ë³´ì•˜ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì•½ê°„ ë³€ê²½ëœ ë˜ ë‹¤ë¥¸ êµ¬ì„±ìš”ì†Œë„ ì‚´í´ë³´ê² ë‹¤. ë°”ë¡œ ë ˆì´ì–´ ì •ê·œí™”(layer normalization)ì´ë‹¤.

<br>

íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” ì¸ì½”ë”, ë””ì½”ë”, ê·¸ë¦¬ê³  ê·¸ ì•ˆì— ìˆëŠ” êµ¬ì„±ìš”ì†Œë“¤(components)ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. ì—¬ê¸° Add & Norm ì´ë¼ê³  ì íŒ ìƒìë“¤(boses that say add and norm)ì´ ìˆë‹¤. ì—¬ê¸°ì„œ í•˜ëŠ” ì¼ì€ ì´ ì„œë¸Œë ˆì´ì–´(sublayer)(ì‘ì€ í™”ì‚´í‘œ ì„¸ ê°œ)ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ë‹¤(take the input, as well as the output of this sublayer). ì´ ê°’ë“¤ì„ ë”í•˜ê³ (add) ì •ê·œí™”(normalize)í•œë‹¤. ì´ê²ƒì€ ì €ìë“¤ì´ ì‚¬ìš©í•œ ì‘ì€ ë¹„ë²•(trick)ì´ë‹¤. ì‹¤ì œë¡œ ìˆ˜ë ´ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ê³ (improve convergence) ìˆ˜ë ´ ê³¼ì •ì„ ë” ë¹ ë¥´ê²Œ ë§Œë“œëŠ” ê²ƒ(make the convergence be quicker)ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.

ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ë²¡í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë²¡í„°ì˜ êµ¬ì„±ìš”ì†Œë“¤(the components of the vector)ì´ ë•Œë¡œëŠ” ë§¤ìš° í´ ìˆ˜ë„(super large) ìˆê³  ë§¤ìš° ì‘ì„ ìˆ˜ë„(super small) ìˆë‹¤. ì—¬ê¸°ì„œ í•µì‹¬ì€ ë²¡í„°ì˜ êµ¬ì„±ìš”ì†Œë“¤ì„ íŠ¹ì • ë²”ìœ„, íŠ¹ ì •ê·œí™”ëœ ë²”ìœ„ ë‚´ì—ì„œ(within some range, some normalized range) ì •ê·œí™”í•˜ëŠ”(normalize) ê²ƒì´ë‹¤. ë¨¼ì € ë²¡í„°ì—ì„œ ê³„ì‚°ëœ í‰ê· (ê¸°ë³¸ì ìœ¼ë¡œ êµ¬ì„±ìš”ì†Œì˜ í•©)ì„ ë¹¼ê³  í‘œì¤€ í¸ì°¨ë¡œ ì •ê·œí™”í•œë‹¤ you're going to take your vector, and then subtract it by the computed mean, which is basically the sum of its components, and normalize it by basically its standard deviation.
ê·¸ë¦¬ê³  ë‘ ê°€ì§€ ë³€ìˆ˜(ì–‘)(quantities)ì„ í•™ìŠµí•˜ê²Œ ëœë‹¤. ì²« ë²ˆì§¸ëŠ” ê°ë§ˆ(gamma)ë¡œ ì´ëŠ” ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜(rescaling factor) ì—­í• ì„ í•˜ê³  ë‘ ë²ˆì§¸ëŠ” ë² íƒ€(beta)ì¸ë° ì´ê²ƒ ë˜í•œ ìš°ë¦¬ê°€ í•™ìŠµí•˜ê²Œ ë  í•­(another term as well that you learn)ì´ë‹¤. ì´ ë‘ ê°€ì§€ ë³€ìˆ˜ë¥¼ ëª¨ë¸ì´ í•™ìŠµí•˜ë„ë¡ í•  ê²ƒì´ë‹¤ let these two quantities be learned by your model.

ì‹¤ì œë¡œ ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ì´ê²ƒì€ í›ˆë ¨ ì•ˆì •ì„±(training stability)ê³¼ ìˆ˜ë ´ ì‹œê°„(convergence time)ì— ë„ì›€ì´ ëœë‹¤.

ê·¸ë˜ì„œ ì´ê²ƒì´ ì›ë˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ ê¸°ìˆ (a technique that was used in the original transformer paper)ì´ì—ˆë‹¤.

<br>



<br>

## Transformer-Based Models

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.57.58.png)

<br>

So we're going to continue this lecture with some deeper dive into the kinds of models that we have in the transformer landscape. And then we're going to do a deep dive into one specific architecture that is very useful for classification settings.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.58.08.png)

<br>

So first, we're going to come back to the architecture that we saw together last time, so this traditional encoder/decoder architecture where you have both components. And so you have the original transformer paper from 2017 that had this architecture. But also later on, you see more architectures that built on top of it.

So here we talk about the T5 family of models. So T5 is a paper that's an abbreviation of multiple Ts. So the first one is transfer, and then it's text-to-text, transformers. So this is where the T5 naming comes from. And then it's derived into multiple versions. So the T5 is like the vanilla paper. And then it had mT5, m stands for multilingual where there was some more work on the data it was trained on, as well as the vocabulary that it was computed over. And then you have ByT5, which is some sort of tokenizer-free method of all of this where you forego of the fact of tokenizing. And instead of that, you basically operate at the byte level. So By is like byte. And basically, you have a vocabulary size that is much smaller. So instead of having an o of 30k, you have 2 to the power of 8. So a like a byte is 8 bits. And then you can represent every character into bytes. So that's what they do.

And then one thing I want to mention regarding the T5 family is that the objective function changes a bit with what the original transformer did. So the original transformer did next token prediction for the training task. But the T5 family, what they did is that they operated on the so-called span corruption task. So basically you would have your sentence as an input to the encoder. And instead of putting everything to the encoder you would leave blanks. And this is what we call span corruption. And then the sapn corruption could be one or multiple tokens missing. So if I want to give an example, so for example, my teddy bear is cute and reading. so you could have my teddy bear span corrupted, and is reading. So that could be one potential encoder input. And then you could have up to n -- I mean, n is the parameterization of the number of spans that are corrupted. So you would have n such tokens here. And then the T5 families calls them sentinel tokens. So if you see sentinel tokens, they represent a span of corrupted tokens. So you have them here in the encoder. And then the decoder's work is to find each of these spans in series. So you start with a token that denotes the first corrupted span. And then you start the decoding process until it hits a prediction that predicts the next sentinel token up to the n plus 1, 1, where the tokens that are decoded between two consecutive sentinel tokens corresponds to the corrupted spans that are recovered. So these parentheses is basically a shift from the next token prediction objective function.

<br>

Q. So the question is, can you elaborate with the decoding process? So what you said regarding the reconstruction is exactly right. So you have sentinel tokens that denote some missing text. So what you want to decode is that missing text. So the decoder output will be exactly like each span reconstructed. And then if you want to know how training works, you do a teacher-forcing mechanism where you input everything in the decoder, and then try to reconstruct everything at once.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.58.18.png)

<br>

And then now we're going to talk about another class of transformers where basically you have this encoder/decoder structure, where you just forget about the decoder and then just deal with the encoder. So you might tell me, OK, hey, you cannot do generation with this. And then I would respond to you, yes, that's exactly the point. So it has encoder representations that can be used for tasks that might be more geared towards classification. So sentiment extraction, token classification, like all of these things that used to be done with specific language models, they can be done with the encoder part of the transformer. And we're going to dive deeper a bit later into a three-key encoder only models. So BERT, which is like the central one, I would say in this landscape, and then two other architectures, DistilBERT and RoBERTa, that are investicating on axis of improvements. So it's going to be good to see.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.58.29.png)

<br>

And then there is one last class. So as Afshine just mentioned, today's LLMs, they remove the encoder part altogether. And then when you have no encoder, you don't have your encoder embeddings at the end of your stacked encoders that could be fed to the cross-attention. So this module disappears altogether. So each decoder that is stacked just has masked self-attention and an FFN as part of it. And yeah, that's basically something that has caught up since then.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.58.44.png)

<br>

Because when you look at the popularity of each of these models, so you used to have these transformer-like architecture that was popular towards the beginning, where the main hypothesis was that the encoder part is very useful for getting to the decoder's representation. But as time went, people realized that your compute budget could be best invested in the decoder only. And then there has been more investment into the kind of tasks that is, I would say, easiest to scale up and generalize -- next-world prediction -- as opposed to the task I just methioned for T5, which might be more bespoke. So you need to corrupt things. So it's more complex. Whereas next-word prediction is, I would say, the simplest thing you can do. And it proved to be wonders and well-aligned with the task of being a helpful kind of chatbot, which is like twday's applications mainly.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.58.53.png)

<br>

And then the decoder-only architectures -- so I'm not going to talk about them today. But it's going to be the central part of the next lectures when we're going to talk about LLMs more and more.

<br>

## BERT Deep Dive

<br>

So now we can dive deep, as promised, into encoder-only architectures and with BERT.

<br>

So first we're going to start by seeing what does BERT mean. So BERT, it's an acronym that denotes Bidirectional Encoder Representations from Transformers. And we're going to see together how does each section of this acronym, what does it correspond to?

<br>

So let's just start with the encoder part, which is the easiest to grasp.

<br>

So as we said, we just dropped the decoder. So this encoder from transformer is basically exactly what it means.

<br>

Now on the other part, so why do we talk about bidirectionality? So it's a way, so the paper's result is remarkable, because we are able from a given input to get output representations that have attended to everything for each token.

<br>

And this is the case. Because since we only have the encoder, we have this self-attention layer that truly attends to every other token.

<br>

And this is in contrast with the masked self-attention that you have where we said that the mask is making the attention mechanism causal. So every token can attend to itself and to the tokens before it. And this is, by the way, something that the authors discuss a lot in the paper, saying that GPT came out, GPT they're not truly bidirectional. And then these encodings that can be used for classification tasks, they truly are.

<br>

Q. So the question is, when you don't have the mask, each token can attend to each other. That's exactly right. And then the mask is exactly there to prevent links from tokens to those that come after them.

<br>

So I just want to put this paper in context. And the field of NLP was booming back then. So you had another landmark paper that same year, which was called ELMo, so Embeddings from Language Models. And I would say that the timing of that paper was a bit unfortunate, because it truly had new insights of also building bidirectional representations. But it just turns out that it came the same year as the transformer, as the kind of transformer-based same line of work. So it got a bit masked by it. So ELMo, just to give the main lines, it was based on a bidirectional LSTM where you had multiple layers, each stacked on top of each other. And basically you were able to build a bidirectional representation for each word, thanks to this architecture. And so why didn't it become as popular as BERT? It's because you had the same downsides as the previous models, where basically it's hard to scale because of this recurrence. And I think a lot of you, when you think about ELMo and BERT, you don't think about these papers at first, because they are characters in Sesame Street.

<br>

## References

ğŸ¥ (Lecture) <https://www.youtube.com/watch?v=yT84Y5zCnaA>

ğŸ“„ (Slide) <https://cme295.stanford.edu/slides/fall25-cme295-lecture2.pdf>