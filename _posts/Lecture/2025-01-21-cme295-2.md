---
title: "[Stanford][CME 295] Lecture #2 Transformer-Based Models & Tricks"
date: 2026-01-21 09:20:00 +0900
categories: [Lecture]

published: false
use_math: true
---

---

&nbsp;

## Lecture 1 Recap

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤ì „ 10.46.13.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤ì „ 10.46.25.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤ì „ 10.46.35.png)

<br>

1ê°•ì—ì„œëŠ” ì…€í”„ ì–´í…ì…˜(self-attention) ê°œë…ì— ëŒ€í•´ ì†Œê°œí–ˆë‹¤. ì…€í”„ ì–´í…ì…˜ì´ë€ ê° í† í°ì´ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì‹œí€€ìŠ¤ ë‚´ì˜ ë‹¤ë¥¸ ëª¨ë“  í† í°ë“¤ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì¿¼ë¦¬(query), í‚¤(key), ë°¸ë¥˜(value)ë¼ëŠ” í‘œê¸°ë²•(notation)ì„ ì‚¬ìš©í•œë‹¤. ì—¬ê¸°ì„œ í•µì‹¬ì€ ì¿¼ë¦¬ì™€ í‚¤ë¥¼ ë¹„êµí•¨ìœ¼ë¡œì¨ ì¿¼ë¦¬ê°€ ìì‹ ê³¼ ê°€ì¥ ìœ ì‚¬í•œ í† í°ì„ ì°¾ëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ê·¸ ê³¼ì •ì´ ì™„ë£Œë˜ë©´ í•´ë‹¹ í† í°ê³¼ ê´€ë ¨ëœ ë°¸ë¥˜ë¥¼ ê°€ì ¸ì˜¤ê²Œ ëœë‹¤.

ì…€í”„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ê³µì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì´ ê³µì‹ì€ ëŒ€ê·œëª¨ í–‰ë ¬ ê³±(matrix multiplication)ì¸ë° í•˜ë“œì›¨ì–´ëŠ” ì´ëŸ¬í•œ ì—°ì‚°ì„ ì²˜ë¦¬í•˜ëŠ” ë° ë§¤ìš° ëŠ¥ìˆ™í•˜ê³ (capable) ë§¤ìš° ìµœì í™”ë˜ì–´(optimized) ìˆë‹¤.

ì´ ëª¨ë“  ê²ƒì„ í†µí•´ íŠ¸ëœìŠ¤í¬ë¨¸(transformer) ì•„í‚¤í…ì²˜ë¥¼ ì†Œê°œí–ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” í¬ê²Œ ë‘ ê°€ì§€ êµ¬ì„±ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. ì¸ì½”ë”(encoder, the left side)ì™€ ë””ì½”ë”(decoder, the right side)ì´ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ê¸°ê³„ ë²ˆì—­(machine translation) ë¶„ì•¼ì—ì„œ ì²˜ìŒ ë„ì…ë˜ì—ˆë‹¤. ì¸ì½”ë”ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì›ë˜ ì–¸ì–´(source language)(ì˜ˆë¥¼ ë“¤ë©´ ì˜ì–´)ë¡œ ì²˜ë¦¬í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. ë””ì½”ë”ëŠ” ë²ˆì—­ëœ ë‚´ìš©(translation)ì„ ëŒ€ìƒ ì–¸ì–´(target language)(ì˜ˆë¥¼ ë“¤ë©´ í”„ë‘ìŠ¤ì–´)ë¡œ í•´ë…í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. ë©€í‹° í—¤ë“œ ì–´í…ì…˜(Multi-Head Attention) ë ˆì´ì–´ì—ì„œ ì…€í”„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì‘ë™í•œë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 2.02.46.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 2.09.59.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 2.10.10.png)

<br>

ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ í—¤ë“œ(head)ê°€ ìˆë‹¤. ê·¸ê²Œ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ” ê±¸ê¹Œ? íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸(Attention Is All You Need) ì† ê·¸ë¦¼ì—ì„œ ê°ê°ì˜ í—¤ë“œë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ê·¸ë¦¬ê³  ê° í—¤ë“œëŠ” ëª¨ë¸ì´ ì…ë ¥ì„ ì¿¼ë¦¬, í‚¤, ë˜ëŠ” ë°¸ë¥˜ë¡œ íˆ¬ì˜í•˜ëŠ” í•œ ê°€ì§€ ë°©ë²•ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê¸°íšŒ(an opportunity for the model to learn one way of projecting the input into being a query, a key, or a value)ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.

ì˜ˆë¥¼ ë“¤ë©´ ì¿¼ë¦¬ì™€ í‚¤ì˜ ê²½ìš° ì´ê²ƒì€ ê°ê°ì˜ í—¤ë“œë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë”°ë¼ì„œ ì´ ì‘ì€ ìƒìë“¤(ê·¸ë¦¼ì—ì„œ ë³´ë¼ìƒ‰ ë°•ìŠ¤)ì˜ ê°œìˆ˜ëŠ” í—¤ë“œì˜ ê°œìˆ˜($\text{h}$)ì™€ ê°™ë‹¤.

ì´ê²ƒì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ë” ì˜ ì‹œê°í™”í•˜ê³  ì´í•´í•˜ê¸° ìœ„í•´ ë…¼ë¬¸ì—ì„œ ê° í—¤ë“œê°€ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ í•´ì„í•˜ëŠ” ë°©ë²•ì„ ì°¸ê³ í•œë‹¤. ì–´í…ì…˜ ë§µ(attention map)ì€ ê° ì¿¼ë¦¬(dot product query)ì˜ ë°¸ë¥˜ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì™¼ìª½ì˜ ì–´í…ì…˜ ë§µì—ì„œ ìš°ë¦¬ëŠ” í† í° "its"ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¤ë¥¸ í† í°ì´ ë¬´ì—‡ì¸ì§€ ì‚´í´ë³´ê³ ì í•œë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ì–‘(quantities), ì¦‰ "its"ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¿¼ë¦¬ì™€ ë‹¤ë¥¸ ëª¨ë“  í‚¤ë“¤ì˜ ë‚´ì (dot product)ì„ ì‚´í´ë³¸ë‹¤. ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” ì¿¼ë¦¬ì™€ í‚¤ì˜ ë‚´ì ì„ ë†’ì€ ë°¸ë¥˜ë¡œ ì´ë„ëŠ”(leading) ë‹¤ë¥¸ í‚¤ë“¤ì„ ì°¾ëŠ”ë‹¤. ê·¸ëŸ¬ë©´ ë…¼ë¬¸ì—ì„œì²˜ëŸ¼ "Law"ì™€ "application" ë‘ ë‹¨ì–´ê°€ ë†’ì€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜(attention weight)ë¡œ ê°•ì¡°ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë€ ì¿¼ë¦¬ "its"ì™€ ë‹¤ë¥¸ í† í°ë“¤ì˜ í‚¤ì˜ ë‚´ì (dot product of the query "its" and the key for each of these tokens)ì´ë‹¤. ì´ëŸ¬í•œ ë‹¨ì–´ë“¤ì„ í•´ì„í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤. ì—¬ê¸°ì„œ ê°•ì¡° í‘œì‹œëœ í† í°ì€ "Law"ì™€ "application"ì¸ë° í† í° "its"ê°€ "Law"ë¥¼ ì§€ì¹­í•˜ê¸° ë•Œë¬¸ì— ë‹¹ì—°í•œ ê²°ê³¼ì´ë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì€ ì´ëŸ¬í•œ ë‹¨ì–´ë“¤ì„ ì´ì „ì— ë°œìƒí•œ ì‚¬ê±´ê³¼ ì—°ê²°í•˜ëŠ”(associate) ë°©ë²•ì„ í•™ìŠµí•´ì•¼ í•œë‹¤.ê·¸ë¦¬ê³  "its"ëŠ” "application"ì„ ì˜ë¯¸í•˜ê¸°ë„ í•˜ëŠ”ë° ì´ê²ƒì€ ë˜í•œ ê·¸ëŸ¬í•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ëŠ” ë˜ë‹¤ë¥¸ ë°©ì‹(which is also another way of explaining why that is the case)ì´ë‹¤. ê·¸ë˜ì„œ ì €ìë“¤ì€ ì´ëŸ¬í•œ ê°’ë“¤ì„(ë°¸ë¥˜ë“¤ì„?) ê°ê¸° ë‹¤ë¥¸ í—¤ë“œë“¤ì˜ í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚´ê¸°ë¡œ í–ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì™¼ìª½ì€ í—¤ë“œì˜ ê°•ë„(intensity), ì¦‰ ì²« ë²ˆì§¸ í—¤ë“œì— ëŒ€í•œ ê°•ë„ì´ë‹¤. ê·¸ë¦¬ê³  ë‘ ë²ˆì§¸ í—¤ë“œëŠ” "Law"ì— ëŒ€í•œ ê°•ë„ê°€ ë§¤ìš° ë†’ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. ê·¸ë˜ì„œ ê°„ë‹¨íˆ ë§í•˜ë©´ ì´ëŸ¬í•œ í—¤ë“œë“¤ì€ ì–´ë–¤ ë‹¨ì–´ê°€ ì¤‘ìš”í•œì§€(what words matters) íŒŒì•…í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

<br>

Q. ì´ëŸ¬í•œ ëª¨ë“  ê³„ì‚°ì„ ìˆ˜í–‰í•  ë•Œ ê° ê³„ì‚°ì€ ì„œë¡œ ë‹¤ë¥¸ MLPë¥¼ ê±°ì¹˜ëŠ”ê°€? (Are they going through different MLPs when we're doing all these computations?)
A. í—¤ë“œë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ íˆ¬ì˜ í–‰ë ¬ì„ ì‚¬ìš©í•œë‹¤. (We're going to have different projection metrices for each of them.)

ìš°ë¦¬ëŠ” ì‹¤ì œë¡œ ê° í—¤ë“œê°€ ìì²´ì ì¸ íˆ¬ì˜ì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì— ëŒ€í•œ ìì„¸í•œ ì˜ˆë¥¼ ë‹¤ë¤˜ë‹¤.


ê°„ë‹¨íˆ ë§í•˜ë©´ ê³ ë„ë¡œ ë³‘ë ¬í™” ë˜ì–´ ìˆë‹¤. ì—¬ê¸°ì—ëŠ” í–‰ë ¬ ê³±ê³¼ ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax) ì—°ì‚°ì´ í¬í•¨ë˜ì–´ ìˆë‹¤.



And in parallel, you're going to have that computation that's going to happen. So each head is going to have one result here, that is then going to be concatenated and then projected once again when the output matrix. So yeah, long story short, it's highly parallelized. And it's basically just like projections. And here you have some matrix multiplication and softmax.

ì–´í…ì…˜ ë§µì„ ì‚´í´ë³´ë©´ ì–´í…ì…˜ í—¤ë“œì˜ ê¸°ëŠ¥(what they do)ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì´ë‹¤.

<br>

ì´ì™€ ê´€ë ¨í•´ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì¸ \<Attention Is All You Need\>ë¥¼ ì½ê¸°ë¥¼ ê°•ë ¥íˆ ì¶”ì²œí•œë‹¤. ëª‡ í˜ì´ì§€ ë¶„ëŸ‰ë°–ì— ë˜ì§€ ì•Šì§€ë§Œ ë‚´ìš©ì´ ë§¤ìš° ì•Œì°¨ë‹¤. 1ê°•ì—ì„œ ë‹¤ë£¬ ë‚´ìš©ì„ í†µí•´ ì¶©ë¶„íˆ ì´í•´í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

<br>

## Position Embeddings

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 4.12.25.png)

<br>

2017ë…„ì— ë„ì…ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” ìˆ˜ë…„ ë™ì•ˆ ì—¬ì „íˆ ìœ íš¨í•œ ì•„í‚¤í…ì²˜ì´ë‹¤. ê·¸ë¦¬ê³  ëª‡ ê°€ì§€ êµ¬ì„±ìš”ì†Œê°€ ì•½ê°„ ë³€ê²½ë˜ì—ˆë‹¤(slightly changed). ê·¸ë˜ì„œ ì•½ê°„ì˜ ì°¨ì´(slight variations)ê°€ ìˆë‹¤. í•˜ì§€ë§Œ ì „ë°˜ì ìœ¼ë¡œ ì˜¤ëŠ˜ë‚ ì˜ ëª¨ë¸ë“¤ì€ ê±°ì˜ ëŒ€ë¶€ë¶„ ì´ˆê¸°(initial) íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆë‹¤.

ê°•ì˜ëŠ” ë‘ íŒŒíŠ¸ë¡œ ë‚˜ëˆ„ì–´ ì§„í–‰ëœë‹¤. ì²« ë²ˆì§¸ íŒŒíŠ¸ì—ì„œëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì¤‘ìš”í•˜ê³  ëª‡ ê°€ì§€ ë³€í™”ê°€ ìˆì—ˆë˜ ë¶€ë¶„ì„ ë‹¤ë£¬ë‹¤. ë‘ ë²ˆì§¸ íŒŒíŠ¸ì—ì„œëŠ” ì˜¤ëŠ˜ë‚  ëª¨ë¸ì˜ ëª…ì¹­(nomenclature)ê³¼ ê·¸ê²ƒë“¤ê³¼ ì´ˆê¸° íŠ¸ëœìŠ¤í¬ë¨¸(original transformer)ì™€ì˜ ê´€ê³„ì— ëŒ€í•´ ì„¤ëª…í•œë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 4.12.35.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 4.12.46.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-21 ì˜¤í›„ 4.12.56.png)

<br>

íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì—ì„œ ì¤‘ìš”í•œ ì²« ë²ˆì§¸ ê°œë…ì€ ìœ„ì¹˜ ì„ë² ë”©(position embedding)ì´ë‹¤. í† í°ë“¤ì€ ë‹¤ë¥¸ ëª¨ë“  í† í°ë“¤ê³¼ ì„œë¡œ ì§ì ‘ì ìœ¼ë¡œ(in a direct fashion) ìƒí˜¸ì‘ìš©í•œë‹¤. ì¦‰ ì§ì ‘ì ì¸ ì—°ê²°(direct links)ì´ ìˆë‹¤. í•˜ì§€ë§Œ RNNì²˜ëŸ¼ ê° í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬

íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” 

ì¦‰ ìœ„ì¹˜ ì •ë³´(position information)ë¥¼ ìƒê²Œ ëœë‹¤.



ê·¸ë˜ì„œ ì´ˆê¸° íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì˜ ì €ìë“¤ì€ ì „ìš© ì„ë² ë”©(dedicated embedding)ì„ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆë‹¤. ì—¬ê¸°ì„œ ì „ìš©(dedicated)ì€ ê° ìœ„ì¹˜ì— í•˜ë‚˜ì˜ ì„ë² ë”©ì´ ìˆë‹¤(each position has one embedding)ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ì²« ë²ˆì§¸ ìœ„ì¹˜ì— í•˜ë‚˜ì˜ ì„ë² ë”©ì´ ìˆê³  ë‘ ë²ˆì§¸ ìœ„ì¹˜ì— í•˜ë‚˜ì˜ ì„ë² ë”©ì´ ìˆë‹¤. ê·¸ë˜ì„œ ì €ìë“¤ì€ ì…ë ¥ í† í° ì„ë² ë”©(input token embedding)ì— í•´ë‹¹ ì„ë² ë”©ì„ ì¶”ê°€í–ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "A cute teddy bear is reading."ì´ë¼ëŠ” ë¬¸ì¥ì˜ ì—¬ê¸°ì— ì²« ë²ˆì§¸ ìœ„ì¹˜(first position)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë‚˜íƒ€ë‚´ëŠ” ì„ë² ë”©ì„ ë”í•˜ëŠ” ê²ƒì´ë‹¤.

Ok, so let's start with the first important concept that's in this architecture. And this is the position embedding. So if you remember, here we're letting tokens interact with all other tokens in a direct fashion. So they have direct links. But contrary to things like RNNs, where you have a sequential dependency were you process each token one at a time, here you're basically losing this idea of a token being processed before another one. So you can lose this position information. So as a result of that, we need to somehow quantify tokens at each position and try to inject that information when the transformer is processing the inputs. So how are we going to do that? So the original transofrmer paper authors, they chose to have a dedicated embedding. And when I say dedicated, what that means is each position has one embedding. So position 1 has one embedding, position 2 has one embedding, et cetera, et cetera. And what they chose to do is to add that embedding to the input-token embedding. So for instance, if I say a cute teddy bear is reading, which is position number 1, representing the token a, plus the embedding representing the first position.

<br>

Q. ìœ„ì¹˜ ì„ë² ë”©ì€ í•™ìŠµë˜ë‚˜ìš” ì•„ë‹ˆë©´ ê³ ì •ì ì¸ê°€ìš”? (Are the position embeddings learned or static?)

A. ë‘˜ ë‹¤. (Both.)

ì €ìë“¤ì´ ë‘ ê°€ì§€ ë°©ë²•ì„ ëª¨ë‘ ì‹œë„í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ë‘ ë²ˆì§¸ ë°©ë²•ì´ ë¬´ì—‡ì¸ì§€ ì‚´í´ë³´ê² ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì„ë² ë”©ì´ í•™ìŠµë˜ì—ˆë‹¤ê³ (they are learned) ê°€ì •í•´ ë³´ì. ì´ê²Œ ë¬´ìŠ¨ ëœ»ì¼ê¹Œ? ê° ìœ„ì¹˜ì— ëŒ€í•œ ì„ë² ë”©ì„ í•™ìŠµí•´ì•¼ í•œë‹¤(need to learn embeddings for each position)ëŠ” ëœ»ì´ë‹¤. í•˜ì§€ë§Œ ì´ ì ‘ê·¼ë²•ì˜ ë¬¸ì œëŠ” í•™ìŠµ ë°ì´í„°ì…‹(training set)ì— ë§¤ìš° ì˜ì¡´ì (much dependent)ì´ë¼ëŠ” ê²ƒì´ë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ ì—¬ê¸°ì²˜ëŸ¼ í•­ìƒ ë‘ ë²ˆì§¸ ìœ„ì¹˜ì—ì„œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ëŠ” í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ í•™ìŠµëœ ì„ë² ë”©(learned embeddings)ì€ ê·¸ëŸ¬í•œ í¸í–¥ì´ í•™ìŠµë  ìˆ˜ ìˆë‹¤. ì´ê²ƒì´ ì²« ë²ˆì§¸ í•œê³„ì´ë‹¤.

ë‘ ë²ˆì§¸ í•œê³„ëŠ” í•™ìŠµí•  ìˆ˜ ìˆëŠ” ìœ„ì¹˜ì˜ ìˆ˜ëŠ” í•™ìŠµ ë°ì´í„°ì…‹ì— ìˆëŠ” ìµœëŒ€ ìœ„ì¹˜ ê°œìˆ˜ë§Œí¼ì´ë‹¤ (Can only learn positions up to the max number of position that is in your training set). ì˜ˆë¥¼ ë“¤ì–´ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ìµœëŒ€ 512ê°œì˜ ì‹œí€€ìŠ¤ë¡œ í•™ìŠµì‹œí‚¨ë‹¤ê³ (train your transformer on sequences that are up to 512) ê°€ì •í•´ë³´ì. ìš°ë¦¬ëŠ” í•´ë‹¹ ìœ„ì¹˜ê¹Œì§€ì˜ ìœ„ì¹˜ ì„ë² ë”©ë§Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤(can only learn position embeddings up to that position).

ì–´ë–»ê²Œ íŒŒë¼ë¯¸í„°í™”í•  ìˆ˜ ìˆì„ê¹Œ?(How do you parameterize that?) ìœ„ì¹˜ 1ë¶€í„° ìœ„ì¹˜ 512 ì‚¬ì´ì˜ ìœ„ì¹˜ì— ëŒ€í•œ í•™ìŠµ ê°€ëŠ¥í•œ ìœ„ì¹˜ ì„ë² ë”©ì˜ ìë¦¬ í‘œì‹œìë¥¼(have a kind of placeholder of a learnable position embedding between 1 and 512) ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  í•™ìŠµì„ ì§„í–‰í•  ë•ŒëŠ”(when you do your training) ì¼ë°˜ì ì¸ ê²½ì‚¬í•˜ê°•ë²•(gradient descent)ì„ í†µí•´ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. ì´ê²Œ ì²« ë²ˆì§¸ ë°©ë²•ì´ë‹¤. í•˜ì§€ë§Œ ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ì´ ë°©ë²•ì—ëŠ” í•œê³„ê°€ ìˆë‹¤. í›ˆë ¨ ë°ì´í„°ì…‹(training set)ì— ì¡´ì¬í•˜ëŠ” ìµœëŒ€ ìœ„ì¹˜ê¹Œì§€ì˜ ìœ„ì¹˜ ì„ë² ë”©ë§Œ í•™ìŠµí• (only learn embeddings of positions up to the max position that is present in the training set) ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì¶”ë¡ í•  ë•Œ í›ˆë ¨ ë°ì´í„°ì…‹ì— ìˆë˜ ìœ„ì¹˜ë¥¼ ë²—ì–´ë‚œ ìœ„ì¹˜(a position that is beyond the position that was in the training set)ê°€ ìˆë‹¤ë©´ ê·¸ ìœ„ì¹˜ëŠ” í•™ìŠµë˜ì§€ ì•Šì€ ìƒíƒœì´ë‹¤. ê·¸ë˜ì„œ ê°’(ë°¸ë¥˜?)ì„ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì„ ì°¾ì•„ì•¼ í•œë‹¤. ì´ê²ƒì´ ë‘ ë²ˆì§¸ í•œê³„ì ì´ë‹¤. í•˜ì§€ë§Œ ì¥ì ìœ¼ë¡œëŠ” ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ë„ë¡ ë‚´ë²„ë ¤ ë‘”ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” ê²½ì‚¬í•˜ê°•ë²•ì´ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ë° ìˆì–´ì„œ ë†€ë¼ìš´ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ ì €ìë“¤ì€ 

And for these reasons, these methods was someting that the authors said that was performing well, along with the second method, which is different,





<br>

1/22

which is around having an arbitrary formula for each dimension corresponding to a position embedding. And we're going to see that now.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.09.08.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.09.10.png)

<br>

ì²« ë²ˆì§¸ ë°©ë²•ì€ ìœ„ì¹˜ ë‹¹ í•˜ë‚˜ì˜ ì„ë² ë”©(one embedding per position)ì„ ì‚¬ìš©í•˜ê³  ê·¸ ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤. ë‘ ë²ˆì§¸ ë°©ë²•ì€ ìœ„ì¹˜ ë‹¹ í•˜ë‚˜ì˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì§€ë§Œ ê·¸ ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ë¯¸ë¦¬ ì •í•´ì§„ ê²ƒ(something that is predetermined)ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ì €ìë“¤ì€ ì‚¬ì¸ í•¨ìˆ˜ì™€ ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ê³µì‹ì„ ì„ íƒí–ˆë‹¤. ì—¬ê¸°ì„œ í•µì‹¬ì€ ì£¼ì–´ì§„ ìœ„ì¹˜($\text{m}$)ì— ëŒ€í•´ í¬ê¸°ê°€ $\text{d}$ì¸ ë²¡í„° ëª¨ë¸(a vector of size d model)ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤. í† í°ë“¤ì„ ë”í•´ì•¼(adding them ì¶”ê°€í•´ì•¼) í•˜ê¸° ë•Œë¬¸ì— $\text{d}$ëŠ” í† í° ì„ë² ë”©ì˜ ì°¨ì›ê³¼ ì¼ì¹˜í•´ì•¼ í•œë‹¤. ê·¸ë¦¬ê³  ëª¨ë“  ì¸ë±ìŠ¤ì— ëŒ€í•´ ì´ ê³µì‹ë“¤ì— ëŒ€í•œ ê°’(the corresponding value with respect to these formulas)ì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤. ì´ ê³µì‹ë“¤ì€ ë­˜ê¹Œ? ì²« ë²ˆì§¸ ê³µì‹ì€ ì–´ë–¤ ê²ƒê³¼ mì˜ ê³±ì— ì‚¬ì¸(sine of something times m)ì„ ì·¨í•œ ê²ƒì´ë‹¤. ë‘ ë²ˆì§¸ ê³µì‹ì€ ì–´ë–¤ ê²ƒê³¼ mì˜ ê³±ì— ì½”ì‚¬ì¸(cosine of something times m)ì„ ì·¨í•œ ê²ƒì´ë‹¤.


í‘œê¸°ë²•ì„ ë‹¨ìˆœí™”í•˜ì. $\omega_i=10000^{-\frac{2i}{d_{\text{model}}}}$ë¼ ê°€ì •í•˜ì. ì¦‰ $\omega_i$ê°€ $i$ ê³±í•˜ê¸° $m$ì˜ í•¨ìˆ˜(omega as a function of i times m)ë¼ê³  ê°€ì •í•´ë³´ì. $\omega_i$ê°€ $10000^{-\frac{2i}{d_{\text{model}}}}$ ì–‘(quantity)ì„ì„ ê¸°ì–µí•˜ë¼. ì´ëŸ° ì‹ìœ¼ë¡œ ì„ë² ë”©ì„ êµ¬ì„±í•œë‹¤ê³  ê°€ì •í•˜ì.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.09.24.png)

<br>

ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ì„œë¡œ ê°€ê¹Œì´ ìˆëŠ” ë‹¨ì–´ì¼ìˆ˜ë¡ ê´€ë ¨ì„±ì´ ë†’ì„ ê°€ëŠ¥ì„±ì´ í¬ê³  ë©€ë¦¬ ìˆëŠ” ë‹¨ì–´ì¼ìˆ˜ë¡ ê´€ë ¨ì„±ì´ ë‚®ì„ ê°€ëŠ¥ì„±ì´ í¬ë‹¤ëŠ”(words that are close together are liekly to be more relevant, as opposed to words that are further together) ì‚¬ì‹¤ì„ ë°˜ì˜í•˜ëŠ” ì‹ìœ¼ë¡œ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ë‹¤. ì¦‰ ë‘ ë‹¨ì–´ê°€ í•œ ìœ„ì¹˜ë§Œ ë–¨ì–´ì ¸ ìˆëŠ” ê²½ìš°ì™€ 10,000 ìœ„ì¹˜ ë–¨ì–´ì ¸ ìˆëŠ” ê²½ìš°(one position apart versus 10,000 position apart)ê°€ ìˆë‹¤ë©´ í•œ ìœ„ì¹˜ë§Œ ë–¨ì–´ì§„ ë‹¨ì–´ê°€ ë” ìœ ì‚¬í•´ì•¼ í•œë‹¤(one position apart is more similar than the other one). ì´ ê³µì‹ì´ íƒ€ë‹¹í•œì§€ ì‚´í´ë³´ì. ë‘ ê°œì˜ ìœ„ì¹˜ ì„ë² ë”©ì´ ìˆë‹¤. í•˜ë‚˜ëŠ” ìœ„ì¹˜ mì— í•˜ë‚˜ëŠ” ìœ„ì¹˜ nì— ìˆë‹¤.. ê·¸ë¦¬ê³  ë¯¸ë¦¬ ì •í•´ì§„ ê³µì‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ê°’ì„ ê³„ì‚°í–ˆë‹¤ê³  ê°€ì •í•´ë³´ì.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.09.33.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.11.png)|![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.22.png)

<br>

ì‚¼ê°í•¨ìˆ˜ ê³µì‹(trigonometry formulas)ì„ ê¸°ì–µí•œë‹¤ë©´ $\cos(a-b)=\cos(a)\cos(b)+\sin(a)\sin(b)$ì´ë‹¤.

$\cos(\omega_i(m-n))$ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤.

ì•Œê³  ë³´ë‹ˆ ì´ ì–‘(quantity)($\cos(\omega_i(m-n))$)ì€ ë‘ ìœ„ì¹˜ ì„ë² ë”©ì˜ ë‚´ì ì„ ê³„ì‚°í•  ë•Œ ë‚˜íƒ€ë‚˜ëŠ” ì—¬ëŸ¬ ìš”ì†Œ ì¤‘ í•˜ë‚˜(one component)ì¼ ë¿ì´ë‹¤.

ì™œëƒí•˜ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ìœ„ì¹˜ mê³¼ ìœ„ì¹˜ nì„ ë‚´ì í•  ë•Œ ë¨¼ì € ì²« ë²ˆì§¸ ìœ„ì¹˜ì˜ ê°’ì„ ê³±í•˜ê³ , ë‘ ë²ˆì§¸ ìœ„ì¹˜ì— ê°’ì„ ë”í•˜ê³ , ë‹¤ì‹œ ê³±í•˜ëŠ” ì‹ìœ¼ë¡œ ê³„ì†í•œë‹¤(take the first position, multiply them, then plus the second position, multiply them, et cetera, et cetera). ê·¸ë¦¬ê³  ì´ê²ƒì´ $\cos(\omega_i(m-n))$ ì–‘(quantiy)ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.

ê²°êµ­ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì„ë² ë”©ë“¤ì˜ ë‚´ì ì„ ìˆ˜í–‰í•˜ë©´ mê³¼ n ì‚¬ì´ì˜ ìƒëŒ€ ê±°ë¦¬ì— ëŒ€í•œ í•¨ìˆ˜ì¸ cosine ê°’ì˜ í•©(a sum of cosine that are a function of the relative distance between m and n)ì´ ë‚˜ì˜¨ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

<br>

Q. ë‘ ì„ë² ë”©ì´ ê°€ê¹Œìš¸ìˆ˜ë¡ ë” ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì¸ê°€? (The closer they are, the more similar they are?)

A. ì§ê´€ì ìœ¼ë¡œ ë§í•˜ìë©´ ì´ëŸ¬í•œ ì„ë² ë”© ê³µì‹í™” ë°©ë²•ì€(way of formulating the embeddings) ê·¼ì‚¬í™”í•˜ê±°ë‚˜(approximate) ëª¨ë°©í•˜ë ¤ëŠ”(mimic) ê²ƒì´ë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.29.png)

<br>

ê·¸ë˜ì„œ ë‚´ì ($\left\langle \text{PE}_m, \text{PE}_n \right\rangle$)ì„ ì–»ê²Œ ë˜ëŠ”ë° ì´ëŠ” mê³¼ n, ì¦‰ ë‘ ì„ë² ë”© ì‚¬ì´ì˜ ìƒëŒ€ ê±°ë¦¬ì— ëŒ€í•œ í•¨ìˆ˜ì´ë‹¤.

ë‹¤ì‹œ í•œë²ˆ ìƒê¸°í•˜ìë©´ ì™œ ë‚´ì ì— ê´€ì‹¬ì„ ê°€ì ¸ì•¼(why do I care) í• ê¹Œ? ì„ë² ë”© ì„¸ê³„ì—ì„œ(in the embedding world) ë‘ ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ì„±(similarity)ì„ ì •ëŸ‰í™”í•˜ë ¤ê³ (quantify) í•  ë•Œ ì¼ë°˜ì ìœ¼ë¡œ ë‘ ì„ë² ë”©ì˜ ë‚´ì ì´ ê°œì…ëœë‹¤(involving). ì¼ë°˜ì ìœ¼ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„(cosine similarity)ë¥¼ ì‚¬ìš©í•œë‹¤. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ê° ì„ë² ë”©ì˜ ë…¸ë¦„(norm)ì— ëŒ€í•œ ë‚´ì ì´ë‹¤. ê·¸ëŸ¬ë‹ˆê¹Œ ê¸°ë³¸ì ìœ¼ë¡œ ë‚´ì ê³¼ ê°™ë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ê°€ ë‚´ì ì— ê´€ì‹¬ì„ ê°–ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì—¬ê¸°ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ ì´ê²ƒì€ ë‘ ì§€ì  ì‚¬ì´ì˜ ìƒëŒ€ì ì¸ ê±°ë¦¬ì— ë”°ë¥¸ í•¨ìˆ˜(a function of the relative distance between the two)ì´ë‹¤.

íŠ¹íˆ $\cos(0)=1$ì´ë‹¤. ì´ ìˆ«ì(number)($\cos(\omega_i(m-n))$)ê°€ í´ìˆ˜ë¡ ì´ ìˆ«ìì˜ ì½”ì‚¬ì¸ ê°’ì€ ì‘ì•„ì§„ë‹¤(the higher this number, the lower the value of cosine of this number). ë¬¼ë¡  ì£¼ê¸°ì„±ì´ ìˆê¸°(periodic) ë•Œë¬¸ì— ì´ ë§ì´ ë°˜ë“œì‹œ ë§ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. $\pi$ë¥¼ ì§€ë‚˜ë©´ ë°©í–¥ì€ ì™„ì „íˆ ë°˜ëŒ€ê°€ ëœë‹¤. ë§í•˜ê³ ì í•˜ëŠ” ê²ƒì€ mê³¼ nì´ ê°™ì„ ë•Œ $\cos(0)$ì— ëŒ€í•œ í•©ì„ ì–»ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ê·¸ ê°’ì€ ë‚´ì ($\left\langle \text{PE}_m, \text{PE}_n \right\rangle$)ì´ ìµœëŒ€(maximum)ê°€ ë˜ëŠ” ê°’ì´ë‹¤. ì¦‰ mê³¼ nì´ ê°™ì„ ë•Œ ì´ ê°’ì´ ìµœëŒ€ê°€ ëœë‹¤. ì¦‰ ìœ„ì¹˜ ìì²´ë§Œ ë†“ê³  ë³´ë©´(looking at the position itself) ê°€ì¥ ìœ ì‚¬í•˜ë‹¤ëŠ”(the most similar) ëœ»ì´ë‹¤.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.38.png)

<br>

ì„ë² ë”© ê°’(values of embeddings)ì„ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ì™¼ìª½ ê·¸ë˜í”„ì—ì„œ yì¶•ì€ 50ê°œ ìœ„ì¹˜ ê°ê°ì— ëŒ€í•œ ëª¨ë“  ì„ë² ë”©ì„ ë‚˜íƒ€ë‚¸ë‹¤. xì¶•ì€ ì£¼ì–´ì§„ ë²¡í„°ì˜ ì—¬ëŸ¬ ì°¨ì›ì— ê±¸ì¹œ ê°’ë“¤ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì²« ë²ˆì§¸ í–‰ì„ ë³´ë©´ ë²¡í„°ë¥¼ ì–´ë–»ê²Œ ì¸ë±ì‹± í•˜ëŠ”ì§€ì— ë”°ë¼(depending on how you index your vector) ì²« ë²ˆì§¸ ìœ„ì¹˜ ë˜ëŠ” 0ë²ˆì§¸ ìœ„ì¹˜(fisrt position or number 0 position)ë¥¼ ë³´ê³  ìˆëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ìœ„ì¹˜ 0 ì„ë² ë”©(position 0 embedding)ì˜ ëª¨ë“  ê°’ì„ ë³´ê³  ìˆë‹¤(are looking). ë³´ì´ëŠ” ê²ƒê³¼ ê°™ì´ ì°¨ì›ì´ ë‚®ì„ìˆ˜ë¡ ì´ ê°’ì€ ë§¤ìš° ë¹ˆë²ˆí•˜ê²Œ ì˜¤ë¥´ë‚´ë¦°ë‹¤ (for low dimensions, this value goes up and down very frequently). ì¦‰ ë” ë†’ì€ ì£¼íŒŒìˆ˜(high frequency)ì´ë‹¤. ê·¸ë¦¬ê³  ì°¨ì›ì´ ë†’ì„ìˆ˜ë¡ ê°’ì´ ì˜¤ë¥´ë‚´ë¦¬ëŠ” ë° ë§ì€ ì‹œê°„ì´ ê±¸ë¦°ë‹¤ (when the dimension is high, it basically takes a lot of time for the value to go up and down). ì¦‰ ë” ë‚®ì€ ì£¼íŒŒìˆ˜(low frequency)ì´ë‹¤.

ì´ê²ƒì€ ì•ì„œ ì–¸ê¸‰í–ˆë˜ $\omega_i$ì™€ ê´€ë ¨ìˆë‹¤. $\omega_i$ëŠ” ì°¨ì› $i$ì˜ ê°’ì´ ì‘ì„ ë•Œ ë§¤ìš° ë†’ë‹¤ very high for low values of i, which is the dimension. ê·¸ë¦¬ê³  $i$ì˜ ê°’ì´ í´ ë•ŒëŠ” ë§¤ìš° ë‚®ë‹¤ very low for high values of i. ì¦‰ ì˜¤ë©”ê°€ëŠ” ì½”ì‚¬ì¸ê³¼ ì‚¬ì¸ ê°’ì´ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ë³€í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•œë‹¤ (basically just determines how quickly your cosine and shine basically vary).

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.47.png)

<br>

ì´ê²ƒì´ ë°”ë¡œ ì›ì €ìë“¤(original authors)ì´ ì‹œë„í–ˆë˜ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ë°©ë²•ë“¤ì„ ì‚¬ìš©í•˜ë©´ í•™ìŠµëœ ë°©ë²•(learned one)ê³¼ ë¹„ìŠ·í•œ(comparable) ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ì ì— ì£¼ëª©í–ˆë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ í° ì¥ì ì´ ìˆë‹¤. í›ˆë ¨ ì‹œì ì— ê´€ì°°í–ˆë˜ ì‹œí€€ìŠ¤ ê¸¸ì´ ë¿ë§Œ ì•„ë‹ˆë¼ ì–´ë–¤ ì‹œí€€ìŠ¤ ê¸¸ì´ì—ë„ ì ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤ can extend to any sequence length, not just a sequence length that you saw at training time. ì´ê²ƒì´ ë°”ë¡œ ì´ ë°©ì‹ì´ ë” ë°”ëŒì§í•œ(preferable) ì´ìœ  ì¤‘ í•˜ë‚˜ì´ë‹¤. ì´ê²ƒì´ ì €ìë“¤ì´ ì„ íƒí•œ ë°©ì‹(what the authors chose)ì´ë‹¤.

2025ë…„ìœ¼ë¡œ ì‹œê°„ì„ ë˜ëŒë ¤ ë³´ì(fast forward to 2025). ì•„ì§ë„ ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€("Are we still using that?") ë¬¼ì„ ìˆ˜ë„ ìˆë‹¤. ì–´ëŠ ì •ë„ëŠ” ê·¸ë ‡ë‹¤. ìš°ë¦¬ëŠ” ì—¬ì „íˆ ë©€ë¦¬ ë–¨ì–´ì§„(far) í† í°ì¼ìˆ˜ë¡ ê°€ê¹Œìš´(closer) í† í°ë³´ë‹¤ ìœ ì‚¬ì„±ì´ ë‚®ë„ë¡ í•˜ëŠ” ì•„ì´ë””ì–´ë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤ far tokens to be less similar than closer tokens. í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ì €ìë“¤ì´ í•œ ê²ƒì²˜ëŸ¼ ì„ë² ë”©ì„ ì‚½ì…í•˜ì§€ëŠ”(injecting) ì•ŠëŠ”ë‹¤. ê·¸ ì´ìœ ë¥¼ ì•Œì•„ë³´ì.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.10.53.png)

<br>

Because if you remember, what you care about is determining how similar tokens are in the self-attention computation. And where does the self-attention computation happen? In the attention layer. But here, what did I say? I said, let's compute these embeddings and let's add them here(Multi-head attention?). But actually what we want is to reflect this similarity in the attention layer.

<br>

Q. So the question is, is it added to the input feature?
A. Yes. So in this first method, yes. just add.

<br>

But the problem is, we mostly want these, I guess, intuition to hold true in the multi-head attention layer. So this is one of the reasons why people have tried different variations, and in particular have these position embeddings intervene directly in the attention layer, as opposed to the input. Because basically when you do it at the input -- just here, fair, OK -- it's going to be roughly something that is going to go into this attention layer, but it's kind of indirect. So what we want is to directly do something about the attention formula that would, I guess, reflect the fact that we want close tokens to be more similar, compared to further tokens.

<br>

And the way we do that is, if you remember, the self-attention layer is basically the softmax of qk transpose over square root of d times v. So what we want is to add a little something inside that softmax, which is basically where you quantify how similar a token is to another token. You want to add a little something to reflect the fact that some tokens, they're supposed to be more similar compared to that token, compared to others.

So there's a few methods that have tried to have some variation of that. So for those of use who know the T5 paper, what we're going to see a little bit later, they have tried these relative position bias by learning the bias term, which is in the formula above. So what they did was, let's suppose you have a given distance between the positions m and n. So their idea was, let's learn that(bias(m, n)). Let's basically bucketize all m minus n into some buckets. And let's just have the model learn these quantities that are then going to be infected into softmax.

<br>

Q. So the question is, does that pose a problem that the bias is here with respect to the probability at the end? Because it has to sum to 1.
A. Well, you can do whatever you want inside the softmax, because the softmax is going to normalize it anyways.

<br>

So you can think of the bias as being something that is maybe more negative for things that are far apart, compared to things that are closer together. So T5 says, let's learn them. We have another message from, I guess, this "Train Short, Test Long" paper, which introduced this method called ALiBi. ALiBi stands for Attention with linear bias, I believe. And what they did was, say instead of learning those biases, let's actually have a deterministic formula, which is as a function of the relative difference between those two positions. And they said that. So they had some results. So all these papers, they always compare based on one another to see which one is, I guess, more performant.

<br>

But the reality is that in today's models, most models actually use another kind of position embedding method. And we're going to see it now. So this method relies on rotating the query and the key vector by some angle. And I guess you can think of it as, you have your query, you have your key, let's suppose in a 2D space. So what you're going to do is rotate your query by some angle that is a function of its position. And you're going to rotate the key vector by some angle that is a function of its position n. And I guess how do you do that, by the way? So let's suppose you have a vector, by the way. And you want to rotate that vector, because I had the answer on the slide. But how would you go about this, I guess, for people who just want to talk about this with intuition? So who here has done, I don't know, rotations in space? Matrix multiplication. And you're going to use a quantity that's called rotation matrix. And the rotation matrix is expressed as follows. So it's basically a 2 by 2 matrix in the 2D plane that has cosine of this angle, minus sine of this angle. sine of this angle, cosine of this angle. I'm looking at the time. It's quite simple to just show that it works, but we may run out of time. Do you want me to quickly show you that it's indeed a way to rotate the vector? Okay so here as a reminder whay we're trying to show is that we can use a matrix multiplication to rotate a vector in 2D space. (íŒì„œ ë”°ë¼ ê·¸ë ¤ë³¼ ê²ƒ) So let's suppose we have the following vector. That is something that you can quantify with two dimensions, let's say x and y. You can express your vector in 2D space with this. But I guess this, if you note, are the norm of the vector, and phi, the angle with respect to the x-axis. You can also write v as R with vector cosine of phi and sine of phi. So if you multiply the rotation matrix with this phi, what you're going to obtain is some multiplication of cosine minus sine, sine and cosine of this and that. And I will leave this exercise for you. But you can show that rotation times this v can be expressed as R of cosine of theta plus phi, and sine of theta plus phi. So this is a quick proof. So I'll just leave you the multiplication of the rotation matrix and v. But you will obtain these trigonometric identities that will, I guess, lead you to this formula. This basically shows that if you multiply this matrix and this vector, you're basically rotating the vector by this angle.

<br>

So I guess here, just going back to this method, what we want to do is to quantify the similarity between tokens, and have close tokens be more similar compared to tokens that are more a far. So the problem that we had with the previous methods was -- so in the first method, this learned embedding, you always had this overfitting issue. Because basically when you learn these biases, it always depends on which training set you have. So maybe your dataset is in a way that, let's say, tokens that are close are similar, but in a different way compared to what you see at inference time.

And this ALiBi method, it didn't have that learnable component, but it was quite restrictive, because it's a very simple formula after all, just the relative difference between n and m. So I guess people have tried different ways of coming up with something that tells you that, I guess, an embedding that reflects the fact that you want further positions to be less similar than closer ones. So in this method, we're going back to the sine and cosine world from what the author had proposed. And think about similarity from the lens of cosine and sine functions. So this is a little intro.

<br>

And their method is called RoFE. So I'm not sure if you've heard of it. So it stands for Rotary Position Embeddings. And we're going to see that this method -- so why do we care about this method? So this method has two great things. So the first one is that if you rotate the query and the key, you will end up with a quantity that will be a function of the relative distance between the two. That's going to be very nice, and that's why I wrote this thing on the blackboard. Not sure if you can see, by the way, but we won't have time to go into the mathematical detail. But if you want to just express these things at home, this is just the foundation.

And so in particular, if you remember your attention formula, so you have query times key transpose. So if you rotate the query by an angle m and the key by an angle n, what you're going to end up is a formula that has the rotation matrix of angle theta and, I guess, n minus m. And this is greate because this is a function of the relative distance between these two positions. OK, so why do I talk about this in detail? Well, it turns out that most models these days, they use RoFE, which is why it's important. And I would say another thing, it's maybe a little bit hard to get the intuition as to why that works. But hopefully, the explanation that I gave regarding the sine and cosine at the very beginning can help you just build that intuition.

<br>

And speaking of that, it turns out that the upper bound of the attention weight given by the query and the key is such that we observe a lone-term decay. Meaning that as n minus m is large, we do see the upper bound that gets smaller and smaller. Well, you see these little oscillations, it's not perfect either. But we do have some mathematical, I guess, results as to just the upper bounds decaying over the long-term.

<br>

Cool. Any question on this? So the question is the relative distance is captured in the rotation matrix -- and yes. yes.

Q. What is theta?
A. So theta is actually fixed. So do you remember this omega that I talked about here? So it's basically some function of i and d. I actually pass it quite quickly. But what I showed you is in the 2D space. But here we're in a d-dimensional space, which is greater than 2. So I glossed it very quickly. But the way you extend this method is by having these 2D space by block. But then the theta is a function of typically something that you fix, but a function of i, which is the dimension. It's basically between 1 and d, d over 2. It's a function of that, and it's a function of d as well. You will see this theta as being roughly euqual to omega_i, just this one, more or less, more or less.

Q. So the question is, so that it has the same dimension as the latent dimension? So, well, here you have a product of matrices. So you need to have the dimensions match. So I guess your answer -- yeah.


So this was position embeddings.

Q. So the question is about, how do you obtain this curve? So this is actually a curve that I believe is shown mathematically. So it's kind of complicated. We're not going to write down the formula. But if you're interested, so in this paper, in the RoFormer paper, there's an appendix where they show mathematically that it is upper bounded by some quantity. And this is what this is showing.

<br>

## Layer Normalization

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 1.13.45.png)

<br>

ìœ„ì¹˜ ì„ë² ë”©ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì•½ê°„ ë³€í˜•ëœ ë¶€ë¶„ ì¤‘ í•˜ë‚˜ì´ë‹¤. ì§€ê¸ˆê¹Œì§€ ì–´ë–»ê²Œ ë°”ë€Œì—ˆëŠ”ì§€, ê·¸ë¦¬ê³  ì™œ ë°”ë€Œì—ˆëŠ”ì§€ ì‚´í´ë³´ì•˜ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì•½ê°„ ë³€ê²½ëœ ë˜ ë‹¤ë¥¸ êµ¬ì„±ìš”ì†Œë„ ì‚´í´ë³´ê² ë‹¤. ë°”ë¡œ ë ˆì´ì–´ ì •ê·œí™”(layer normalization)ì´ë‹¤.

<br>

## Transformer-Based Models

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.57.58.png)

<br>

So we're going to continue this lecture with some deeper dive into the kinds of models that we have in the transformer landscape. And then we're going to do a deep dive into one specific architecture that is very useful for classification settings.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.58.08.png)

<br>

So first, we're going to come back to the architecture that we saw together last time, so this traditional encoder/decoder architecture where you have both components. And so you have the original transformer paper from 2017 that had this architecture. But also later on, you see more architectures that built on top of it.

So here we talk about the T5 family of models. So T5 is a paper that's an abbreviation of multiple Ts. So the first one is transfer, and then it's text-to-text, transformers. So this is where the T5 naming comes from. And then it's derived into multiple versions. So the T5 is like the vanilla paper. And then it had mT5, m stands for multilingual where there was some more work on the data it was trained on, as well as the vocabulary that it was computed over. And then you have ByT5, which is some sort of tokenizer-free method of all of this where you forego of the fact of tokenizing. And instead of that, you basically operate at the byte level. So By is like byte. And basically, you have a vocabulary size that is much smaller. So instead of having an o of 30k, you have 2 to the power of 8. So a like a byte is 8 bits. And then you can represent every character into bytes. So that's what they do.

And then one thing I want to mention regarding the T5 family is that the objective function changes a bit with what the original transformer did. So the original transformer did next token prediction for the training task. But the T5 family, what they did is that they operated on the so-called span corruption task. So basically you would have your sentence as an input to the encoder. And instead of putting everything to the encoder you would leave blanks. And this is what we call span corruption. And then the sapn corruption could be one or multiple tokens missing. So if I want to give an example, so for example, my teddy bear is cute and reading. so you could have my teddy bear span corrupted, and is reading. So that could be one potential encoder input. And then you could have up to n -- I mean, n is the parameterization of the number of spans that are corrupted. So you would have n such tokens here. And then the T5 families calls them sentinel tokens. So if you see sentinel tokens, they represent a span of corrupted tokens. So you have them here in the encoder. And then the decoder's work is to find each of these spans in series. So you start with a token that denotes the first corrupted span. And then you start the decoding process until it hits a prediction that predicts the next sentinel token up to the n plus 1, 1, where the tokens that are decoded between two consecutive sentinel tokens corresponds to the corrupted spans that are recovered. So these parentheses is basically a shift from the next token prediction objective function.

<br>

Q. So the question is, can you elaborate with the decoding process? So what you said regarding the reconstruction is exactly right. So you have sentinel tokens that denote some missing text. So what you want to decode is that missing text. So the decoder output will be exactly like each span reconstructed. And then if you want to know how training works, you do a teacher-forcing mechanism where you input everything in the decoder, and then try to reconstruct everything at once.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.58.18.png)

<br>

And then now we're going to talk about another class of transformers where basically you have this encoder/decoder structure, where you just forget about the decoder and then just deal with the encoder. So you might tell me, OK, hey, you cannot do generation with this. And then I would respond to you, yes, that's exactly the point. So it has encoder representations that can be used for tasks that might be more geared towards classification. So sentiment extraction, token classification, like all of these things that used to be done with specific language models, they can be done with the encoder part of the transformer. And we're going to dive deeper a bit later into a three-key encoder only models. So BERT, which is like the central one, I would say in this landscape, and then two other architectures, DistilBERT and RoBERTa, that are investicating on axis of improvements. So it's going to be good to see.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.58.29.png)

<br>

And then there is one last class. So as Afshine just mentioned, today's LLMs, they remove the encoder part altogether. And then when you have no encoder, you don't have your encoder embeddings at the end of your stacked encoders that could be fed to the cross-attention. So this module disappears altogether. So each decoder that is stacked just has masked self-attention and an FFN as part of it. And yeah, that's basically something that has caught up since then.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.58.44.png)

<br>

Because when you look at the popularity of each of these models, so you used to have these transformer-like architecture that was popular towards the beginning, where the main hypothesis was that the encoder part is very useful for getting to the decoder's representation. But as time went, people realized that your compute budget could be best invested in the decoder only. And then there has been more investment into the kind of tasks that is, I would say, easiest to scale up and generalize -- next-world prediction -- as opposed to the task I just methioned for T5, which might be more bespoke. So you need to corrupt things. So it's more complex. Whereas next-word prediction is, I would say, the simplest thing you can do. And it proved to be wonders and well-aligned with the task of being a helpful kind of chatbot, which is like twday's applications mainly.

<br>

![Screenshot](/assets/img/2026-01-21/ìŠ¤í¬ë¦°ìƒ· 2026-01-22 ì˜¤í›„ 3.58.53.png)

<br>

And then the decoder-only architectures -- so I'm not going to talk about them today. But it's going to be the central part of the next lectures when we're going to talk about LLMs more and more.

<br>

## BERT Deep Dive

<br>

So now we can dive deep, as promised, into encoder-only architectures and with BERT.

<br>

So first we're going to start by seeing what does BERT mean. So BERT, it's an acronym that denotes Bidirectional Encoder Representations from Transformers. And we're going to see together how does each section of this acronym, what does it correspond to?

<br>

So let's just start with the encoder part, which is the easiest to grasp.

<br>

So as we said, we just dropped the decoder. So this encoder from transformer is basically exactly what it means.

<br>

Now on the other part, so why do we talk about bidirectionality? So it's a way, so the paper's result is remarkable, because we are able from a given input to get output representations that have attended to everything for each token.

<br>

And this is the case. Because since we only have the encoder, we have this self-attention layer that truly attends to every other token.

<br>

And this is in contrast with the masked self-attention that you have where we said that the mask is making the attention mechanism causal. So every token can attend to itself and to the tokens before it. And this is, by the way, something that the authors discuss a lot in the paper, saying that GPT came out, GPT they're not truly bidirectional. And then these encodings that can be used for classification tasks, they truly are.

<br>

Q.

<br>

## References

ğŸ¥ (Lecture) <https://www.youtube.com/watch?v=yT84Y5zCnaA>

ğŸ“„ (Slide) <https://cme295.stanford.edu/slides/fall25-cme295-lecture2.pdf>