---
title: "Section 1: From Generation to Sampling"
date: 2025-11-25 10:00:00 +0900
categories: [Lecture]
---

## Section 1: From Generation to Sampling

<br>


(1 slide)

So let's start off with section one and think about like what it means to generate something. So we will see we going go from gong to go to from generation to sampling.

(2 slide)

So the first thing is how do we represent the objects that we want to generate in a computer. So that should not be totally new to you but let's just recap. So for example like an image has a certain number of pixels as a certain height and a certain width and usually have three color channels. so numerically we represent this object as a vector set that's in the vector space or it's a vector of length H*W*3 so usually we can represent that like that numerically in a computer. Videos are basically a stream of images if you have t time frames each of these this image represents an image in itself so you usually can represent this in the computer as a vector of that shape where like SE T times whatever the size of the image was. Let's think about for example a molecular structure. molecular structure let's say has n atoms and each atom has 3 coordinates then you would represent that as a vector of that shape where it's basically each row corresponds to the coordinates of a specific atom. So all of this is to say that in this class we want to represent the object we want to generate as vectors. There's other ways of representing but in the end of the day what we want to generate mathematically is just vectors.

(3 slide)

Let's think about what does it mean to successfully generate something. So let's think about it we have a prompt a picture of a dog and let's say you have these four images that you see here on the right. So the first one is just some noise maybe something you've seen on an old TV. How would you prescribe this that could would be completely useless right. That's has nothing to do with what we actually want. Let's say this image it's not a picture of a dog but at least it looks like something it looks like an image that we would have done in real life. Okay so it's bad but better thatn the previous one. This one is at least an animal that's the wrong one it's a cat. And finally we get a dog. So you can see you rank these images based on how good they are and how good they align with the prompt but these are subjective statements like when I say Oh this fits better, fits less better, that's not a statement I can mathematically formalize So the question is how do we formalize this?

(4 slide)

And the language we use and everybody in general of modeling use this language of probability Theory where we basically introduce this object what's called the data distribution. Ant it basically converts the statement of how good something is to How likely something is on a specific dataset. For example in this example we could say how likely are we to find this picture on the internet. So let's say we have prompt of the picture of the dog. It would be almost impossible to find a picture like this with this caption right? This would be pretty rare, this would be unlikely but maybe there was a swap between animals and this would be very likely so we basically translating how good an image is to how likely it is under the data distribution. It's basically we translated this subjective statement into a statement of probability theory.

(5 slide)

So let's think about what this means formally. So I just said we have a data distribution that essentially is the distribution of objects that we want to generate. And throughout this class you should keep this notation in your mind. It's called P data. That's a data distribution. Now let's think back about how we represent probability distributions. The usual represent representation the one we are going to use here is one of a probability density basically says that P data goes from the vector space r to the D which our space of objects and gives you a non-negative number. So given an object set it gives you a likelihood or probability of how likely that object is. I should stress we don't know this probability density but we just postulate it as a distribution they want to sample from. So what does it mean to generate something it means to sample from this distribution to roll the dice and get samples from distribution out and hopefully if we done this right what we will get is an actual good image so an image of a dog or at least most of the time we should get that.


(6 slide)

Good. but we denote just want to do that somehow because we're doing machine learning we need some sort of like data. So the question is like how does the data set fit into this picture. Let's think about our a few examples for images we could just use public available images from the internet we could just pull them and like collect the dataset. For video we could just use something like YouTube. For protein structures we could use something like the protein Data Bank just generally scientific data that's been gathered over last few decades. And what's a dataset? A dataset consists of a finite numbers of samples from that data distribution. So we just denote this as set one to set n which are just samples from the data distribution. And we're going to use that to train our model.

(7 slide)

Alright so let's think about something likek the case I just described to you was what's commonly called unconditional generation. So usually we have one model for let's say a fixed prompt for example that prompt could be dog and it's basically means that you always get the same image from the same model. We would always get images of dogs they would likey look a little different but at the end of the day they would all be images of dogs. What we more want is you want to condition whatever we want to generate on a certain variable that throughout this class is going to be called y. So for example it could be y could be dog, could be cat, could be landscape, could be much more complicated object. It could be a room full of MIT students listening to a diffusion class. So something like it could be a much longer tex(t). So we want to have this variable y basically condition our generation. And for this we introduce this obejct what's called the conditional data distribution which we denote as P dot. uh dot stands for a placeholder given y. So it basically means given this prompt y what's the distribution of the data given this prompt. And conditional generation essentially means that sampling this conditional distribution so we sample an object set from P data given y. And it's important to note that this is the case we're going to focus on first off. Because like most general of modeling frameworks start with that and then they translate it to this case. So this is the case we're ultimately interested in but we want to first focus on this case so just keep that in mind that where we we're starting and but where we're going.

(8 slide)

So let's think about a generative model. So given it's name a generative model tries to generate something which we've just learned means sampling from the distribution. So in other words a generative model tries to generate samples from the distribution and usually what we have is what's called an initial distribution. We call this here as P init it's an important notation as well that we use throughout this class which is essentially the initial distribution and in the default case you could always think about Pinit as a gorion(골션?) so as a standard gion(가우시안?) of zero mean and diagonal identity matrix as covariance matrix so that's what's here on the top right. And the idea of a generative model essentially start with an initial distribution so your sample from a Gaussian which visually would just look like white noise for an image. And what you get out is you get an image of a dog so essentially in the language of probaibilty Theory a generative model converts an initial distribution into a data distribution. And the goal of this class is to teach you how to do that and not so somehow but with flowing in the fusion models. And that's what we're going to talk about in a second.

(16 slide)

We want to build generative models with flows and diffusion. And the goal of this section is going to be to define what that is and how we sample from it. At this point a quick note so I'm going to write now on the blackboard there's lecture notes online which is a selfcontained summary of what I'm going to tell you here. So you're very invited to like take notes along but I also want to just as a disclaimer use selection notes. That's what I would do.

(칠판 판서)

Let's get started. So we want to talk about flow and diffusion models and throughout this class we will always have an a flow component and a diffusion component and it's much like the most important thing is you understand the flow component because it's really the basis for the other part. So I would almost say like the flow component is the one that's really core to understand so if you struggle make sure you understand the flow component and then maybe revisit the diffusion component.

Let's start off with thinking about what's the fundamental object of a flow and that's what's a trajectory. So I'll first define what a trajectory is. The trajectory is what intuitively think a trajectory is basically is a functino of time and in this class we mainly going to focus on time from 0 to 1. And it essentially given a time point t gives you a vector Xt. So you basically saying for every time point t I'm getting a vecotr Xt out. And a way to think about this um is essentially in two dimensions for example you would have a point and you're basically moving through space and that's basically your trajectory. The other fundamental object is a vector field. and Vector field we commonly denote here as U and it has a spatial component and a time component. So we have a vector x a time t and it returns a vector UT of x (u_t(x)). Visually you should think about a vector field as something that at every point in space gives you direction so like let's say you start like at this point it gives you direction here at this point it gives you direction here. It basically it gives you directions at every point and you basically ideas layer to follow along this. So let's define now and that's very foundamental what an ordinary differential equation is. An ODE which stands for ordinary differential equation which you might have also heard before is basically describes conditions on a trajectory so the first thing we have is that a trajectory should start at a specific point which is called the inital condition. So x0 should like capital x0 should start at lowercase x0 which is what's called the initial condition. And then we have the condition that we're basically saying that we want to follow along the vector field that's described describes the direction. So let's say we started here at x_0 and we basically follow along the time vectors across time. You should keep in mind that these direction change over time because the vector field here is time dependent but basically you should think about it's basically following along the direction specified by a vector field. So how do we describe this you're basically saying that the derivative or the velocity of the trajectory is given by u_t at the location where X_t currently is. So says the direction of X_t is given by the location of X and then specified by u_t. And that's what's called an ODE or combined with the initial condition. It's called an ODE. Maybe some of us you have heard this I mean ODEs are fundamental in mechanics in Engineering in Physics so... I'm sure you have heard this in some form or another before. But this term is less common and that's called the flow. So a flow is essentially a collection of trajectories that follow the ODE. Essentially we gather a lot of solutions for different initial conditions and then we gather them in all in one function and call that a flow. So let's define what a flow is. We commonly call this psi. And like before it has a spatial and a time component. And then given an initial condition x_0 and t It maps it to psi_t x_0. And we basically saying is that for every initial condition x_0. I want this to be a solution to my ODE which essentially as the following condition. It means the initial condition says psi 0 of x0 is just x0 which is essentially this condition so we're starting at x0. And the other component is that the time derivative os psi_t of x_0 is specified by this equation. So it's specified by psi_t of x0 and u_t. Just to recap so a flow is essentially collection of solutions to an ODE for a lot of initial conditions. So maybe llike as a diagram so an ODE is defined by a vector field. So a vector field defines an ODE. A trajectory is a solution to this ODE. And then a flow is a collection of trajectories for various initial conditions. So a vector field defines an ODE in this format. A trajectory X_t is a solution to this ODE. And a flow defines a solution a collection of solutions to that for various initial conditions

Look at an examples. So what you see here is an example of a flow and that's a visualization that takes a bit of time to get used to so in blue you see the vector field and then in red you see for every grid point that's an initial condition and you see how this initial condition moves through space. So the grids how they get deformed it's basically a visualization of the flow. So basically it says now that's a flow map and every time it there's a new psi_t for different t and it kind of moves through time.



## Section 2: Flow and Diffusion Models



---