---
title: "[GR00T N1.6] GR00T Inference"
date: 2026-02-09 14:45:00 +0900
categories: [Tutorial]

published: true
use_math: true
---

---

&nbsp;

이번 튜토리얼에서는 GR00T 추론 모델을 사용해 테스트 데이터셋이 주어졌을 때 관측치(observations)로부터 행동을 예측하는 방법을 다룬다.

<br>

```python
import os
import torch
import gr00t

from gr00t.data.dataset.lerobot_episode_loader import LeRobotEpisodeLoader
from gr00t.data.dataset.sharded_single_step_dataset import extract_step_data
from gr00t.data.embodiment_tags import EmbodimentTag
from gr00t.policy.gr00t_policy import Gr00tPolicy
```

<br>

```python
# Change the following paths
MODEL_PATH = "nvidia/GR00T-N1.6-3B"

# REPO_PATH is the path of the pip install gr00t repo and one level up
REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))
DATASET_PATH = os.path.join(REPO_PATH, "demo_data/gr1.PickNPlace")
EMBODIMENT_TAG = "gr1"

device = "cuda" if torch.cuda.is_available() else "cpu"
```

<br>

## Load Pretrained Policy

<br>

정책 모델(Policy Model)은 다른 Hugging Face 모델들과 마찬가지로 로드된다.

GR00T 모델에는 두 가지 새로운 개념이 있다.

* `modality_config` (모달리티 설정): 모델에서 사용하는 딕셔너리(dictionary)의 키(keys)를 정의한다. (e.g., `action`, `state`, `annotation`, `video`)
* `modality_transform` (모달리티 변환): 데이터 로딩 중에 적용되는 일련의 변환(transform)이다.

<br

```python
policy = Gr00tPolicy(
    model_path=MODEL_PATH,
    embodiment_tag=EmbodimentTag(EMBODIMENT_TAG),
    device=device,
    strict=True,
)

# Print out the policy model architecture
print(policy.model)
```

<br>

## Load Dataset

<br

먼저 사용자는 `Gr00TPolicy` 사전 학습 모델을 사전 학습할 때 어떤 embodiment tags를 사용했는지 확인해야 한다.

<br>

```python
import numpy as np

modality_config = policy.get_modality_config()

print(modality_config.keys())

for key, value in modality_config.items():
    if isinstance(value, np.ndarray):
        print(key, value.shape)
    else:
        print(key, value)
```

<br>

```python
# Create the dataset
dataset = LeRobotEpisodeLoader(
    dataset_path=DATASET_PATH,
    modality_configs=modality_config,
    # video_backend="torchcodec",
    video_backend="decord",
    video_backend_kwargs=None,
)
```

<br>

단일 데이터(a single data)를 출력하고 시각화하면 다음과 같다.

<br>

```python
import numpy as np

episode_data = dataset[0]
step_data = extract_step_data(
    episode_data, step_index=0, modality_configs=modality_config, embodiment_tag=EmbodimentTag(EMBODIMENT_TAG), allow_padding=False
)

# print(step_data)

print("\n\n====================================")
print("Images:")
for img_key in step_data.images:
    print(" " * 4, img_key, f"{len(step_data.images[img_key])} x {step_data.images[img_key][0].shape}")

print("\nStates:")
for state_key in step_data.states:
    print(" " * 4, state_key, step_data.states[state_key].shape)

print("\nActions:")
for action_key in step_data.actions:
    print(" " * 4, action_key, step_data.actions[action_key].shape)

print("\nTask: ", step_data.text)
```

<details>
<summary>Output</summary>
<div markdown="1">

```
Images:
     ego_view_bg_crop_pad_res256_freq20 1 x (256, 256, 3)

States:
     left_arm (1, 7)
     right_arm (1, 7)
     left_hand (1, 6)
     right_hand (1, 6)
     waist (1, 3)

Actions:
     left_arm (16, 7)
     right_arm (16, 7)
     left_hand (16, 6)
     right_hand (16, 6)
     waist (16, 3)

Task:  pick the pear from the counter and place it in the plate
```

</div>
</details>

<br>

"right arm"의 상태와 행동 데이터를 그래프로 그려서 어떻게 생겼는지 확인해 보자. 오른손(right hand)의 상태를 나타내는 이미지도 함께 보인다.

<br>

```python
import matplotlib.pyplot as plt

episode_index = 0
max_steps = 400
joint_name = "right_arm"
image_key = "ego_view_bg_crop_pad_res256_freq20"

state_joints_across_time = []
gt_action_joints_across_time = []
images = []

sample_images = 6
episode_data = dataset[episode_index]
print(len(episode_data))

for step_count in range(max_steps):
    data_point = extract_step_data(
        episode_data, step_index=step_count, modality_configs=modality_config, embodiment_tag=EmbodimentTag(EMBODIMENT_TAG), allow_padding=False
    )
    state_joints = data_point.states[joint_name][0]
    gt_action_joints = data_point.actions[joint_name][0]

    state_joints_across_time.append(state_joints)
    gt_action_joints_across_time.append(gt_action_joints)

    # We can also get the image data
    if step_count % (max_steps // sample_images) == 0:
        image = data_point.images[image_key][0]
        images.append(image)

# Size is (max_steps, num_joints)
state_joints_across_time = np.array(state_joints_across_time)
gt_action_joints_across_time = np.array(gt_action_joints_across_time)


# Plot the joint angles across time
num_joints = state_joints_across_time.shape[1]
fig, axes = plt.subplots(nrows=num_joints, ncols=1, figsize=(8, 2*num_joints))

for i, ax in enumerate(axes):
    ax.plot(state_joints_across_time[:, i], label="state joints")
    ax.plot(gt_action_joints_across_time[:, i], label="gt action joints")
    ax.set_title(f"Joint {i}")
    ax.legend()

plt.tight_layout()
# plt.savefig("output-joint-angles.png")
plt.show()


# Plot the images in a row
fig, axes = plt.subplots(nrows=1, ncols=sample_images, figsize=(16, 4))

for i, ax in enumerate(axes):
    ax.imshow(images[i])
    ax.axis("off")

# plt.savefig("output-images.png")
```

<br>

![Joint Angles](/assets/img/2026-02-09/output-joint-angles.png)|![Images](/assets/img/2026-02-09/output-images_2x3.png)

<br>

사전 학습된 체크포인트(checkpoint)로부터 정책을 실행할(run) 수 있다.

<br>

```python
observation = {
    "video": {k: np.stack(step_data.images[k])[None] for k in step_data.images},  # stach images and add batch dimension
    "state": {k: step_data.states[k][None] for k in step_data.states},  # add batch dimension
    "action": {k: step_data.actions[k][None] for k in step_data.actions},  # add batch dimension
    "language": {
        modality_config["language"].modality_keys[0]: [[step_data.text]],  # add time and batch dimension
    }
}
predicted_action, info = policy.get_action(observation)
for key, value in predicted_action.items():
    print(key, value.shape)
```

<br>

## References
