---
title: "[GR00T N1.6] GR00T Inference"
date: 2026-02-09 14:45:00 +0900
categories: [Tutorial]

published: true
use_math: true
---

---

&nbsp;

이번 포스팅에서는 테스트 데이터셋이 주어졌을 때 GR00T 추론 모델을 사용해 관측치(observations)로부터 행동을 예측하는 방법을 다룬다.

<br>

```python
import os
import torch
import gr00t

from gr00t.data.dataset.lerobot_episode_loader import LeRobotEpisodeLoader
from gr00t.data.dataset.sharded_single_step_dataset import extract_step_data
from gr00t.data.embodiment_tags import EmbodimentTag
from gr00t.policy.gr00t_policy import Gr00tPolicy
```

<br>

```python
# Change the following paths
MODEL_PATH = "nvidia/GR00T-N1.6-3B"

# REPO_PATH is the path of the pip install gr00t repo and one level up
REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))
DATASET_PATH = os.path.join(REPO_PATH, "demo_data/gr1.PickNPlace")
EMBODIMENT_TAG = "gr1"

device = "cuda" if torch.cuda.is_available() else "cpu"
```

<br>

## Load Pretrained Policy

<br>

정책 모델(Policy Model)은 다른 Hugging Face 모델들과 마찬가지로 로드된다.

GR00T 모델에는 두 가지 새로운 개념이 있다.

* `modality_config` (모달리티 설정): 모델에서 사용하는 딕셔너리(dictionary)의 키(keys)를 정의한다. (e.g., `action`, `state`, `annotation`, `video`)
* `modality_transform` (모달리티 변환): 데이터 로딩 중에 적용되는 일련의 변환(transform)이다.

<br>

```python
policy = Gr00tPolicy(
    model_path=MODEL_PATH,
    embodiment_tag=EmbodimentTag(EMBODIMENT_TAG),
    device=device,
    strict=True
    )

# Print out the policy model architecture
print(policy.model)
```

<details>
<summary>Output</summary>
<div markdown="1">

```
Gr00tN1d6(
  (backbone): EagleBackbone(
    (model): Eagle3_VLForConditionalGeneration(
      (vision_model): Siglip2VisionModel(
        (vision_model): Siglip2VisionTransformer(
          (embeddings): Siglip2VisionEmbeddings(
            (patch_embedding): Linear(in_features=588, out_features=1152, bias=True)
            (position_embedding): Embedding(256, 1152)
          )
          (encoder): Siglip2Encoder(
            (rope_2d): Rope2DPosEmb(dim=72, max_height=512, max_width=512, theta_base=14)
            (layers): ModuleList(
              (0-26): 27 x Siglip2EncoderLayer(
                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
                (self_attn): Siglip2Attention(
                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)
                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)
                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)
                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)
                )
                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
                (mlp): Siglip2MLP(
                  (activation_fn): PytorchGELUTanh()
                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)
                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)
                )
              )
            )
          )
          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
          (head): Siglip2MultiheadAttentionPoolingHead(
            (attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)
            )
            (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            (mlp): Siglip2MLP(
              (activation_fn): PytorchGELUTanh()
              (fc1): Linear(in_features=1152, out_features=4304, bias=True)
              (fc2): Linear(in_features=4304, out_features=1152, bias=True)
            )
          )
        )
      )
      (language_model): Qwen3ForCausalLM(
        (model): Qwen3Model(
          (embed_tokens): Embedding(151680, 2048)
          (layers): ModuleList(
            (0-15): 16 x Qwen3DecoderLayer(
              (self_attn): Qwen3Attention(
                (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
                (k_proj): Linear(in_features=2048, out_features=1024, bias=False)
                (v_proj): Linear(in_features=2048, out_features=1024, bias=False)
                (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
                (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
                (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
              )
              (mlp): Qwen3MLP(
                (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)
                (up_proj): Linear(in_features=2048, out_features=6144, bias=False)
                (down_proj): Linear(in_features=6144, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)
              (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)
            )
          )
          (norm): Qwen3RMSNorm((2048,), eps=1e-06)
          (rotary_emb): Qwen3RotaryEmbedding()
        )
        (lm_head): Linear(in_features=2048, out_features=151680, bias=False)
      )
      (mlp1): Sequential(
        (0): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=4608, out_features=2048, bias=True)
        (2): GELU(approximate='none')
        (3): Linear(in_features=2048, out_features=2048, bias=True)
      )
    )
  )
  (action_head): Gr00tN1d6ActionHead(
    (model): AlternateVLDiT(
      (timestep_encoder): TimestepEncoder(
        (time_proj): Timesteps()
        (timestep_embedder): TimestepEmbedding(
          (linear_1): Linear(in_features=256, out_features=1536, bias=True)
          (act): SiLU()
          (linear_2): Linear(in_features=1536, out_features=1536, bias=True)
        )
      )
      (transformer_blocks): ModuleList(
        (0): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (3): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (4): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (5): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (6): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (7): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (8): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (9): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (10): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (11): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (12): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (13): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (14): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (15): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (16): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (17): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (18): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (19): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (20): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (21): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (22): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (23): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (24): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (25): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (26): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (27): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (28): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (29): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (30): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=2048, out_features=1536, bias=True)
            (to_v): Linear(in_features=2048, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
        (31): BasicTransformerBlock(
          (norm1): AdaLayerNorm(
            (silu): SiLU()
            (linear): Linear(in_features=1536, out_features=3072, bias=True)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          )
          (attn1): Attention(
            (to_q): Linear(in_features=1536, out_features=1536, bias=True)
            (to_k): Linear(in_features=1536, out_features=1536, bias=True)
            (to_v): Linear(in_features=1536, out_features=1536, bias=True)
            (to_out): ModuleList(
              (0): Linear(in_features=1536, out_features=1536, bias=True)
              (1): Dropout(p=0.2, inplace=False)
            )
          )
          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GELU(
                (proj): Linear(in_features=1536, out_features=6144, bias=True)
              )
              (1): Dropout(p=0.2, inplace=False)
              (2): Linear(in_features=6144, out_features=1536, bias=True)
              (3): Dropout(p=0.2, inplace=False)
            )
          )
          (final_dropout): Dropout(p=0.2, inplace=False)
        )
      )
      (norm_out): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      (proj_out_1): Linear(in_features=1536, out_features=3072, bias=True)
      (proj_out_2): Linear(in_features=1536, out_features=1024, bias=True)
    )
    (state_encoder): CategorySpecificMLP(
      (layer1): CategorySpecificLinear()
      (layer2): CategorySpecificLinear()
    )
    (action_encoder): MultiEmbodimentActionEncoder(
      (W1): CategorySpecificLinear()
      (W2): CategorySpecificLinear()
      (W3): CategorySpecificLinear()
      (pos_encoding): SinusoidalPositionalEncoding()
    )
    (action_decoder): CategorySpecificMLP(
      (layer1): CategorySpecificLinear()
      (layer2): CategorySpecificLinear()
    )
    (vlln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (position_embedding): Embedding(1024, 1536)
  )
)
```

</div>
</details>

<br>

## Load Dataset

<br>

`Gr00TPolicy` 사전 학습 모델을 사전 학습할 때 어떤 embodiment tags를 사용했는지 확인해야 한다.

<br>

```python
import numpy as np

modality_config = policy.get_modality_config()

print(modality_config.keys())

for key, value in modality_config.items():
    if isinstance(value, np.ndarray):
        print(key, value.shape)
    else:
        print(key, value)
```

<details>
<summary>Output</summary>
<div markdown="1">

```
dict_keys(['video', 'state', 'action', 'language'])
video ModalityConfig(delta_indices=[0], modality_keys=['ego_view_bg_crop_pad_res256_freq20'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)
state ModalityConfig(delta_indices=[0], modality_keys=['left_arm', 'right_arm', 'left_hand', 'right_hand', 'waist'], sin_cos_embedding_keys=['left_arm', 'right_arm', 'left_hand', 'right_hand', 'waist'], mean_std_embedding_keys=None, action_configs=None)
action ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['left_arm', 'right_arm', 'left_hand', 'right_hand', 'waist'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=[ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None)])
language ModalityConfig(delta_indices=[0], modality_keys=['task'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)
```

</div>
</details>

<br>

```python
# Create the dataset
dataset = LeRobotEpisodeLoader(
    dataset_path=DATASET_PATH,
    modality_configs=modality_config,
    # video_backend="torchcodec",
    video_backend="decord",
    video_backend_kwargs=None
    )
```

<br>

단일 데이터를 출력하고 시각화하면 다음과 같다.

<br>

```python
import numpy as np

episode_data = dataset[0]
step_data = extract_step_data(
    episode_data, step_index=0, modality_configs=modality_config, embodiment_tag=EmbodimentTag(EMBODIMENT_TAG), allow_padding=False
    )

# print(step_data)

print("\n\n====================================")
print("Images:")
for img_key in step_data.images:
    print(" " * 4, img_key, f"{len(step_data.images[img_key])} x {step_data.images[img_key][0].shape}")

print("\nStates:")
for state_key in step_data.states:
    print(" " * 4, state_key, step_data.states[state_key].shape)

print("\nActions:")
for action_key in step_data.actions:
    print(" " * 4, action_key, step_data.actions[action_key].shape)

print("\nTask: ", step_data.text)
print("====================================")
```

<details>
<summary>Output</summary>
<div markdown="1">

```
====================================
Images:
     ego_view_bg_crop_pad_res256_freq20 1 x (256, 256, 3)

States:
     left_arm (1, 7)
     right_arm (1, 7)
     left_hand (1, 6)
     right_hand (1, 6)
     waist (1, 3)

Actions:
     left_arm (16, 7)
     right_arm (16, 7)
     left_hand (16, 6)
     right_hand (16, 6)
     waist (16, 3)

Task:  pick the pear from the counter and place it in the plate
====================================
```

</div>
</details>

<br>

"right arm"의 상태와 행동 데이터를 나타낸 그래프와 오른손 상태를 나타낸 이미지를 확인한다.

<br>

```python
import matplotlib.pyplot as plt

episode_index = 0
max_steps = 400
joint_name = "right_arm"
image_key = "ego_view_bg_crop_pad_res256_freq20"

state_joints_across_time = []
gt_action_joints_across_time = []
images = []

sample_images = 6
episode_data = dataset[episode_index]
print(len(episode_data))

for step_count in range(max_steps):
    data_point = extract_step_data(
        episode_data, step_index=step_count, modality_configs=modality_config, embodiment_tag=EmbodimentTag(EMBODIMENT_TAG), allow_padding=False
    )
    state_joints = data_point.states[joint_name][0]
    gt_action_joints = data_point.actions[joint_name][0]

    state_joints_across_time.append(state_joints)
    gt_action_joints_across_time.append(gt_action_joints)

    # We can also get the image data
    if step_count % (max_steps // sample_images) == 0:
        image = data_point.images[image_key][0]
        images.append(image)

# Size is (max_steps, num_joints)
state_joints_across_time = np.array(state_joints_across_time)
gt_action_joints_across_time = np.array(gt_action_joints_across_time)


# Plot the joint angles across time
num_joints = state_joints_across_time.shape[1]
fig, axes = plt.subplots(nrows=num_joints, ncols=1, figsize=(8, 2*num_joints))

for i, ax in enumerate(axes):
    ax.plot(state_joints_across_time[:, i], label="state joints")
    ax.plot(gt_action_joints_across_time[:, i], label="gt action joints")
    ax.set_title(f"Joint {i}")
    ax.legend()

plt.tight_layout()
plt.savefig("output-joint-angles.png")
plt.show()


# # Plot the images in a row
# fig, axes = plt.subplots(nrows=1, ncols=sample_images, figsize=(16, 4))

# for i, ax in enumerate(axes):
#     ax.imshow(images[i])
#     ax.set_title(f"Image {i}")
#     ax.axis("off")

# # plt.savefig("output-images.png")


rows = 2
cols = 3

fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 8))
axes_flat = axes.flatten()

for i, ax in enumerate(axes_flat):
    if i < len(images):
        ax.imshow(images[i])
        ax.set_title(f"Image {i}", fontsize=10, pad=5)
        ax.axis("off")
    else:
        ax.axis("off")

plt.savefig("output_images_2x3.png", bbox_inches='tight')
plt.show()
```

<details>
<summary>Output</summary>
<div markdown="1">

![Joint Angles](/assets/img/2026-02-09/output-joint-angles.png)

<br>

![Images](/assets/img/2026-02-09/output_images_2x3.png)

</div>
</details>

<br>

사전 학습된 체크포인트(checkpoint)로부터 정책을 실행할 수 있다.

<br>

```python
observation = {
    "video": {k: np.stack(step_data.images[k])[None] for k in step_data.images},        # Stach images and add batch dimension
    "state": {k: step_data.states[k][None] for k in step_data.states},                  # Add batch dimension
    "action": {k: step_data.actions[k][None] for k in step_data.actions},               # Add batch dimension
    "language": {modality_config["language"].modality_keys[0]: [[step_data.text]]}      # Add time and batch dimension
    }

predicted_action, info = policy.get_action(observation)

for key, value in predicted_action.items():
    print(key, value.shape)
```

<details>
<summary>Output</summary>
<div markdown="1">

```
left_arm (1, 16, 7)
right_arm (1, 16, 7)
left_hand (1, 16, 6)
right_hand (1, 16, 6)
waist (1, 16, 3)
```

</div>
</details>

<br>