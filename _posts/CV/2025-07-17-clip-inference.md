---
title: "CLIP Model Inference"
date: 2025-07-17 09:57:00 +0900
categories: CV
---

&nbsp;

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” CLIP ëª¨ë¸ì„ ì§ì ‘ ì‚¬ìš©í•´ ë³´ê³  ê·¸ ê²°ê³¼ë¥¼ ì‹œê°í™”í•œë‹¤.

<br>

## Model & Dataset

<br>

ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì€ Hugging Face(ğŸ¤—)ì˜ `openai/clip-vit-base-patch32` ëª¨ë¸[[1]](https://huggingface.co/openai/clip-vit-base-patch32)ê³¼ `clip-benchmark/wds_imagenetv2` ë°ì´í„°ì…‹[[2]](https://huggingface.co/datasets/clip-benchmark/wds_imagenetv2)ì„ ì‚¬ìš©í•œë‹¤. ê°ê°ì˜ ì‚¬ìš©ë²•ì€ Hugging Faceì˜ í•´ë‹¹ í˜ì´ì§€ì—ì„œ Use this model/datasetì„ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
* `openai/clip-vit-base-patch32`: CLIP ë…¼ë¬¸ì´ ì²˜ìŒ ê³µê°œë˜ì—ˆì„ ë‹¹ì‹œ OpenAIì—ì„œ ì œê³µí•œ ëª¨ë¸ì´ë‹¤. ë² ì´ìŠ¤ ëª¨ë¸ì´ê³  íŒ¨ì¹˜ ì‚¬ì´ì¦ˆëŠ” 32ì´ë‹¤.
* `clip-benchmark/wds_imagenetv2`: ì´ë¯¸ì§€(`webp`)ì™€ í´ë˜ìŠ¤(`cls`)ë¡œ êµ¬ì„±ëœ ë°ì´í„° ì…‹ì´ë‹¤. ì •ìˆ˜ë¡œ ë‚˜íƒ€ë‚´ì–´ì§€ëŠ” í´ë˜ìŠ¤ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ëŠ” `classnames.txt` íŒŒì¼ì„ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ëª¨ë¸ì˜ ì¼ë°˜í™”(generalization) ì„±ëŠ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ê²ƒìœ¼ë¡œ `test` ë°ì´í„°ì…‹ë§Œ ì¡´ì¬í•œë‹¤.

<br>

![dataset viewer](/assets/img/2025-07-17/dataset_viewer.png)

<br>

![wds_imagenetv2](/assets/img/2025-07-17/wds_imagenetv2.png)

<br>

## Zero-Shot Classification

<br>

CLIP ë…¼ë¬¸[[3]](https://arxiv.org/abs/2103.00020)ì—ì„œëŠ” ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì‚¬ì´ì˜ ì—°ê´€ì„±ì„ cosine similarityë¥¼ í†µí•´ ë‚˜íƒ€ë‚¸ë‹¤. ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ë™ì¼í•œ ì„ë² ë”© ê³µê°„(embedding space)ì— ë†“ê³  ì˜¬ë°”ë¥¸ ìŒì˜ cosine similarityëŠ” ìµœëŒ€í™”í•˜ê³  ì˜ëª»ëœ ìŒì˜ cosine similarityëŠ” ìµœì†Œí™”í•˜ëŠ” ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ ì¸ì½”ë”ì™€ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤.

<br>

![cosine similarity from paper](/assets/img/2025-07-17/cosine_similarity_from_paper.png)

<br>

ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê¸°ì— ì•ì„œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¤€ë¹„í•œë‹¤.

<br>

```python
# pip install transformers
# pip install datasets

from transformers import CLIPModel, CLIPProcessor
from datasets import load_dataset
```

<br>

`CLIPModel`ì€ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” CLIP ëª¨ë¸ì´ë‹¤. `CLIPProcessor`ëŠ” ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ CLIP ëª¨ë¸ì˜ ì…ë ¥ í˜•íƒœë¡œ ê°€ê³µí•œë‹¤.

<br>

ì•„ë˜ì˜ ì½”ë“œë¥¼ í†µí•´ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.

<br>

```python
# Load Model
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Load Dataset
dataset = load_dataset("clip-benchmark/wds_imagenetv2")
test_dataset = dataset["test"]
```

<br>

`from_pretrained()` í•¨ìˆ˜ë¥¼ í†µí•´ Hugging Faceì— ìˆëŠ” ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ë‹¤. ëª¨ë¸ì˜ ì¸í¼ëŸ°ìŠ¤(inference)ë¥¼ ìœ„í•œ `test` ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜¨ë‹¤.

<br>

ë‹¤ìŒì€ ì´ë¯¸ì§€ ë°ì´í„°ì™€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°ê° ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”©ìœ¼ë¡œ ë§Œë“ ë‹¤.

<br>

```python
sample_size = 10
subset = test_dataset.shuffle().select(range(sample_size))
images = list(subset["webp"])

cls2label = open("classnames.txt", "r").readlines()
label_texts = [cls2label[sub].rstrip() for sub in subset["cls"]]

with torch.no_grad():
    image_embeddings = model.get_image_features(pixel_values=inputs_image["pixel_values"])
    text_embeddings = model.get_text_features(input_ids=inputs_text["input_ids"], attention_mask=inputs_text["attention_mask"])
```

<br>

ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ê°ê° í¬ê¸°ê°€ 1ì¸ ë²¡í„°ë¡œ ë³€í™˜í•œ ë‹¤ìŒ ë‘˜ ì‚¬ì´ì˜ cosine similarityë¥¼ ê³„ì‚°í•œë‹¤.

<br>

```python
image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)
text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)

similarity_matrix = cosine_similarity(image_embeddings.cpu().numpy(), text_embeddings.cpu().numpy())
```

<br>

ì•„ë˜ì˜ ê·¸ë¦¼ì€ ì•ì„œ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•œ 10ì¥ì˜ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì‚¬ì´ì˜ cosine similarityë¥¼ íˆíŠ¸ë§µ(heatmap)ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.

<br>

![cosine similarity](/assets/img/2025-07-17/cosine_similarity.png)

<br>

Cosine similarityê°€ í´ìˆ˜ë¡ ë‘ ë²¡í„°ëŠ” ìœ ì‚¬í•˜ë©° íˆíŠ¸ë§µ ìƒì—ì„œ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œí˜„ëœë‹¤. ì¦‰, ëŒ€ê°ì„ ì— ë†“ì¸ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ëŠ” ì—°ê´€ì„±ì´ ë†’ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.

<br>

## Embedding Space Visualization

<br>

See you soon...

<br>

---

## References
 
[1] <https://huggingface.co/openai/clip-vit-base-patch32>  
[2] <https://huggingface.co/datasets/clip-benchmark/wds_imagenetv2>  
[3] <https://arxiv.org/abs/2103.00020>

&nbsp;