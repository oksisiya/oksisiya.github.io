---
title: "CLIP Model Inference - Cosine similarity"
date: 2025-07-16 10:35:00 +0900
categories: CV
---

&nbsp;

![cosine similarity from paper](assets\img\2025-07-16-cv-clip-inference\cosine_similarity_from_paper.png)

<br>

ë…¼ë¬¸ì„ ë³´ë©´ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì‚¬ì´ì˜ ì—°ê´€ì„±ì„ cosine similarityë¥¼ í†µí•´ ë³´ì—¬ì¤€ë‹¤.

<br>

Hugging Face(ğŸ¤—)ì— ìˆëŠ” ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œë‹¤.

<br>

ë°ì´í„°ì…‹ ê°™ì€ ê²½ìš° ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸ í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ì´ë¯€ë¡œ Test ë°ì´í„° ì…‹ë§Œ ì¡´ì¬í•œë‹¤.


ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì˜ ì‚¬ìš©ë²•ì€ Hugging Faceì˜ í•´ë‹¹ í˜ì´ì§€ì—ì„œ Use this model(or dataset)ì„ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

<br>

![wds_imagenetv2](assets\img\2025-07-16-cv-clip-inference\wds_imagentv2.png)

<br>

ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê¸°ì— ì•ì„œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ importí•œë‹¤.

<br>

```python
# pip install transformers
# pip install datasets

from transformers import CLIPModel, CLIPProcessor
from datasets import load_dataset
```


<br>

`CLIPModel`ì€ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” CLIP ëª¨ë¸ì´ë‹¤. `CLIPProcessor`ëŠ” ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ CLIP ëª¨ë¸ì˜ ì…ë ¥ í˜•íƒœë¡œ ê°€ê³µí•œë‹¤. ê°€ê³µëœ ë°ì´í„°ë¥¼ CLIP ëª¨ë¸ì— ì…ë ¥í•˜ë©´ ì„ë² ë”© ìŠ¤í˜ì´ìŠ¤ ìƒì—ì„œ ì–´ë–¤ ë²¡í„° í˜•íƒœë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.

<br>

ì•„ë˜ì˜ ì½”ë“œë¥¼ í†µí•´ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.

<br>

```python
# Load Model
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Load Dataset
dataset = load_dataset("clip-benchmark/wds_imagenetv2")
test_dataset = dataset["test"]
```

<br>

`from_pretrained` í•¨ìˆ˜ë¥¼ í†µí•´ Hugging Faceì— ìˆëŠ” ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ë‹¤. ëª¨ë¸ì˜ ì¸í¼ëŸ°ìŠ¤ë¥¼ ìœ„í•œ `test` ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜¨ë‹¤. ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ì´ë¦„ì€ Huggine Face í˜ì´ì§€ì—ì„œ ë³µì‚¬/ë¶™ì—¬ë„£ëŠ”ë‹¤.

<br>

ë‹¤ìŒì€ ì´ë¯¸ì§€ ë°ì´í„°ì™€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°ê° ì„ë² ë”© ë²¡í„°ë¡œ ë§Œë“ ë‹¤.

<br>

```python
sample_size = 10
subset = test_dataset.shuffle().select(range(sample_size))
images = list(subset["webp"])

cls2label = open("classnames.txt", "r").readlines()
label_texts = [cls2label[sub].rstrip() for sub in subset["cls"]]

with torch.no_grad():
    image_embeddings = model.get_image_features(pixel_values=inputs_image["pixel_values"])
    text_embeddings = model.get_text_features(input_ids=inputs_text["input_ids"], attention_mask=inputs_text["attention_mask"])
```

<br>

ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ê°ê° í¬ê¸°ê°€ 1ì¸ ë²¡í„°ë¡œ ë³€í™˜í•œ ë‹¤ìŒ cosine similarityë¥¼ ê³„ì‚°í•œë‹¤.

<br>

```python
image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)
text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)

similarity_matrix = cosine_similarity(image_embeddings.cpu().numpy(), text_embeddings.cpu().numpy())
```

<br>

![cosine similarity](assets\img\2025-07-16-cv-clip-inference\cosine_similarity.png)

<br>

ì•ì„œ ëœë¤í•˜ê²Œ ì„ íƒí•œ 10ì¥ì˜ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì‚¬ì´ì˜ ì—°ê´€ì„±, cosine similarityë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ë‚˜íƒ€ëƒˆë‹¤. ì—°ê´€ì„±ì´ ë†’ì„ìˆ˜ë¡ ë¹¨ê°›ê²Œ í‘œí˜„ëœë‹¤.ëŒ€ê°ì„  ë¶€ë¶„ì´ ì •ë‹µì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ similarityê°€ ë†’ì€ ê²ƒë“¤ì´ ëŒ€ê°ì„ ì— ìœ„ì¹˜í•œë‹¤.

<br>

ì´ëŸ¬í•œ ë°©ë²•ì„ í†µí•´ CLIP ëª¨ë¸ì´ classificationì„ ì˜ ìˆ˜í–‰í–ˆëŠ”ì§€ íŒë‹¨í•  ìˆ˜ ìˆë‹¤.


<br>

---

## References

[1] <https://arxiv.org/abs/2103.00020>  
[2] <https://huggingface.co/openai/clip-vit-base-patch32>  
[3] <https://huggingface.co/datasets/clip-benchmark/wds_imagenetv2>  

&nbsp;