---
title: "CLIP Model Inference"
date: 2025-07-16 10:35:00 +0900
categories: CV
---

&nbsp;

Hugging Face(ğŸ¤—)ì— ìˆëŠ” ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œë‹¤.

<br>

![wds_imagenetv2](assets\img\2025-07-16-cv-clip-inference\2025-07-16-clip-inference-dataset.png)

<br>

ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê¸°ì— ì•ì„œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ importí•œë‹¤.

<br>

```python
# pip install transformers
# pip install datasets

from transformers import CLIPModel, CLIPProcessor
from datasets import load_dataset
```

<br>

`CLIPModel`ì€ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” CLIP ëª¨ë¸ì´ë‹¤. `CLIPProcessor`ëŠ” ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ CLIP ëª¨ë¸ì˜ ì…ë ¥ í˜•íƒœë¡œ ê°€ê³µí•œë‹¤. ê°€ê³µëœ ë°ì´í„°ë¥¼ CLIP ëª¨ë¸ì— ì…ë ¥í•˜ë©´ ì„ë² ë”© ìŠ¤í˜ì´ìŠ¤ ìƒì—ì„œ ì–´ë–¤ ë²¡í„° í˜•íƒœë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.

<br>

ì•„ë˜ì˜ ì½”ë“œë¥¼ í†µí•´ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.

<br>

```python
# Load Model
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Load Dataset
dataset = load_dataset("clip-benchmark/wds_imagenetv2")
test_dataset = dataset["test"]
```

<br>

`from_pretrained` í•¨ìˆ˜ë¥¼ í†µí•´ Hugging Faceì— ìˆëŠ” ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ë‹¤. ë°ì´í„°ì…‹ì€ ëª¨ë¸ì˜ ì¸í¼ëŸ°ìŠ¤ë¥¼ ìœ„í•œ `test` íŒŒì¼ì„ ê°€ì ¸ì˜¨ë‹¤. (ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì˜ ì‚¬ìš©ë²•ì€ Hugging Faceì˜ í•´ë‹¹ í˜ì´ì§€ì—ì„œ Use this modelì„ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆë‹¤.)

<br>

---

## References

[1] <https://huggingface.co/openai/clip-vit-base-patch32>  
[2] <https://huggingface.co/datasets/clip-benchmark/wds_imagenetv2>  

&nbsp;