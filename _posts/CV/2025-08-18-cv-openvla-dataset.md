---
title: "[OpenVLA] OpenVLA ëª¨ë¸ ì¶”ë¡  1) Open X-Embodiment Dataset"
date: 2025-08-18 10:50:00 +0900
categories: [CV, Multi Modal]
---

&nbsp;

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” OpenVLA ëª¨ë¸ì˜ í›ˆë ¨ ë°ì´í„°ì…‹ì¸ Open X-Embodiment ë°ì´í„°ì…‹ì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤.

<br>

## Open X-Embodiment Dataset

<br>

Open X-Embodiment (OpenX) ë°ì´í„°ì…‹ì€ 97ë§Œ ê°œì˜ ë¡œë´‡ ì—í”¼ì†Œë“œë¡œ êµ¬ì„±ëœ ëŒ€ê·œëª¨ ë¡œë´‡ í›ˆë ¨ ë°ì´í„°ì…‹ [[1]](<https://arxiv.org/abs/2406.09246>)ì´ë‹¤. ë¡œë´‡ì˜ ë™ì‘ì€ ìˆœì°¨ì ì¸(sequential) íŠ¹ì„±ì„ ê°–ëŠ”ë‹¤. ìƒí™©(í”„ë ˆì„)ì´ ë°”ë€” ë•Œë§ˆë‹¤ ë¡œë´‡ì´ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” í–‰ë™ë„ ë°”ë€Œê²Œ ëœë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë¡œë´‡ì˜ í›ˆë ¨ ë°ì´í„°ì…‹ì€ ì§€ì‹œë¬¸(instruction)ê³¼ ì‚¬ëŒì´ ìˆ˜í–‰í•œ ì˜ìƒì„ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ì´ë¯¸ì§€(image) ë°ì´í„°ë¡œ êµ¬ì„±ëœë‹¤.

<br>

![970k robot episodes](/assets/img/2025-08-18/970k-robot-episodes.png)

<br>

## Load from Hugging Face

<br>

ë°ì´í„°ì…‹ì„ ì§ì ‘ ë¡œë“œí•´ ì–´ë–¤ ì‹ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤. OpenX ë°ì´í„°ì…‹ì€ ê³µê°œ ë°ì´í„°ì…‹ìœ¼ë¡œ, Colab ë§í¬ [[2]](<https://colab.research.google.com/github/google-deepmind/open_x_embodiment/blob/main/colabs/Open_X_Embodiment_Datasets.ipynb>)ë¥¼ í†µí•´ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•´ì„œ ì‚¬ìš©í•˜ê±°ë‚˜ Hugging Face(ğŸ¤—)ë¡œë¶€í„° ë°ì´í„°ì…‹ì„ ë¡œë“œí•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œ OpenVLA ëª¨ë¸ì„ Huggle Faceì—ì„œ ë¶ˆëŸ¬ì˜¬ ê²ƒì´ê¸° ë•Œë¬¸ì— êµ¬í˜„í•˜ê¸° í¸í•˜ë„ë¡ í›„ìì˜ ë°©ë²•ì„ íƒí•œë‹¤.

<br>

![Hugging Face](/assets/img/2025-08-18/Hugging-Face.png)

<br>

ì•„ë˜ì˜ ê·¸ë¦¼ì€ OpenX ë°ì´í„°ì…‹ì˜ ì„œë¸Œì…‹(subset) ëª©ë¡ì´ë‹¤. 97ë§Œ ê°œì˜ ì—í”¼ì†Œë“œë¥¼ êµ¬ì„±í•˜ëŠ” ì„œë¸Œ ì—í”¼ì†Œë“œë“¤ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ íƒœìŠ¤í¬ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. íƒœìŠ¤í¬ì— ë”°ë¼ ë¡œë´‡ê³¼ ë°°ê²½, í–‰ë™ ë“±ì´ ë°”ë€Œê²Œ ë˜ëŠ”ë° OpenVLA ëª¨ë¸ì´ ì´ëŸ¬í•œ ë‹¤ì–‘ì„±ì„ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì— ê¸°ì¡´ VLA ëª¨ë¸ì— ë¹„í•´ì„œ ì¼ë°˜í™” ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆë‹¤.

<br>

![Optional subdatasets](/assets/img/2025-08-18/optional-subdatasets.png)

<br>

Hugging Faceì˜ `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ìˆ˜ë§ì€ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ì„œë¸Œì…‹ìœ¼ë¡œ ì§€ì •í•´ íŠ¹ì • íƒœìŠ¤í¬ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ì½”ë“œëŠ” Hugging Faceì˜ ì‚¬ìš© ì˜ˆì‹œì´ë‹¤.

<br>

```python
# Hugging Faceì˜ Usage Example

import datasets
ds = datasets.load_dataset("jxu124/OpenX-Embodiment", "fractal20220817_data", streaming=True, split='train')
```

<br>

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” ë¹„êµì  ì‘ì€ ìš©ëŸ‰ì¸ `nyu_door_opening_surprising_effectiveness` ì„œë¸Œì…‹ì„ ë‹¤ë£¬ë‹¤. (ì„œë¸Œì…‹ë³„ë¡œ ìš©ëŸ‰ì€ ì²œì°¨ë§Œë³„ì´ê³  100GBë¥¼ ë„˜ê¸°ëŠ” ê²ƒë„ ìˆë‹¤.)

<br>

```python
import datasets
import io

from PIL import Image
from matplotlib import pyplot as plt

ds = datasets.load_dataset("jxu124/OpenX-Embodiment", "nyu_door_opening_surprising_effectiveness", trust_remote_code=True)

samples = ds['train'][0]['data.pickle']['steps']

images = []
for sample in samples:
    encoded_image = sample['observation']['image']['bytes']
    instruct = sample['observation']['natural_language_instruction']

    image = Image.open(io.BytesIO(encoded_image))
    images.append(image)
```

<br>

```
# print(ds)
DatasetDict({
    train: Dataset({
        features: ['__key__', '__url__', 'data.pickle', '__local_path__'],
        num_rows: 435
    })
})

# print(instruction)
b'open door'

# ds['train'][0]
{'__key__': 'sample_000000000000', '__url__': '/Users/osuyeon/.cache/huggingface/hub/datasets--jxu124--OpenX-Embodiment/snapshots/3c...opening_surprising_effectiveness_00000.tar', 'data.pickle': {'image_list': [...], 'steps': [...]}, '__local_path__': '/Users/osuyeon/.cache/huggingface/hub/datasets--jxu124--OpenX-Embodiment/snapshots/3c...opening_surprising_effectiveness_00000.tar'}

# ds['train'][0].keys()
dict_keys(['__key__', '__url__', 'data.pickle', '__local_path__'])

# ds['train'][0]['data.pickle']
{'image_list': ['image'], 'steps': [{...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, ...]}

# ds['train'][0]['data.pickle'].keys()
dict_keys(['image_list', 'steps'])

# ds['train'][0]['data.pickle']['image_list']
['image']

# ds['train'][0]['data.pickle']['image_list']['image']
Traceback (most recent call last):
  File "<string>", line 1, in <module>
TypeError: list indices must be integers or slices, not str

# ds['train'][0]['data.pickle']['steps']
[{'action': {...}, 'is_first': True, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': False, 'is_terminal': False, 'observation': {...}, 'reward': 0.0}, {'action': {...}, 'is_first': False, 'is_last': True, 'is_terminal': True, 'observation': {...}, 'reward': 1.0}, {'action': {...}, 'is_first': False, 'is_last': True, 'is_terminal': True, 'observation': {...}, 'reward': 0.0}]

# len(samples) (í”„ë ˆì„ ê°œìˆ˜)
46

# sample
{'action': {'gripper_closedness_action': [...], 'rotation_delta': [...], 'terminate_episode': 0.0, 'world_vector': [...]}, 'is_first': True, 'is_last': False, 'is_terminal': False, 'observation': {'image': {...}, 'natural_language_embedding': [...], 'natural_language_instruction': b'open door'}, 'reward': 0.0}

# sample.keys() (is_: ì²« ë²ˆì§¸ í”„ë ˆì„, ë§ˆì§€ë§‰ í”„ë ˆì„, ì¢…ë£Œë˜ì—ˆëŠ”ê°€)
dict_keys(['action', 'is_first', 'is_last', 'is_terminal', 'observation', 'reward'])

# sample['action'] (ì‚¬ëŒì´ ìˆ˜í–‰í•œ ë¡œë´‡ì˜ ì›€ì§ì„ ì •ë³´ = ì •ë‹µ)
{'gripper_closedness_action': [-0.00024771690368652344], 'rotation_delta': [-0.004529649391770363, 0.005992727819830179, 0.004243004601448774], 'terminate_episode': 0.0, 'world_vector': [-9.537393634673208e-05, 0.0012610105331987143, -0.003427658462896943]}

# sample['action'].keys()
dict_keys(['gripper_closedness_action', 'rotation_delta', 'terminate_episode', 'world_vector'])

# sample['observation'] (imageì™€ insturction)
{'image': {'bytes': b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00\x00\x01\x00\x01\x00\x00\xff\xdb\x00C\x00\x08\x06\x06\x07\x06\x05\x08\x07\x07\x07\t\t\x08\n\x0c\x14\r\x0c\x0b\x0b\x0c\x19\x12\x13\x0f\x14\x1d\x1a\x1f\x1e\x1d\x1a\x1c\x1c $.\' ",#\x1c\x1c(7),01444\x1f\'9=82<...\xbe\xf3\xd4{w=Ic\x97\xe6\x7f\x96\x8f\xf6\x9a\x9c\xb8\xf9\xbf\xef\x9aj\xfd\xca\x00j\xe3\xe6j)\xdb~E\xa7|\xbf\xf8\xee\xda\x00\xff\xd9', 'path': None}, 'natural_language_embedding': [0.004324073437601328, -0.012277779169380665, -0.03588211536407471, 0.06965800374746323, -0.02430354617536068, -0.022569917142391205, -0.006370545830577612, -0.044684361666440964, 0.05525897070765495, -0.042060744017362595, 0.027653323486447334, -0.00270639406517148, 0.009368452243506908, -0.07284097373485565, -0.0626731812953949, 0.048572614789009094, 0.06526853889226913, -0.06726699322462082, -0.048759575933218, ...], 'natural_language_instruction': b'open door'}

# sample['observation'].keys()
dict_keys(['image', 'natural_language_embedding', 'natural_language_instruction'])
```

<br>

## Convert Subdataset to GIF

<br>

ì„œë¸Œì…‹ì„ í•˜ë‚˜ì˜ GIF íŒŒì¼ë¡œ ë³€í™˜í•´ ì–´ë–¤ íƒœìŠ¤í¬ì¸ì§€ í™•ì¸í•œë‹¤. `nyu_door_opening_surprising_effectiveness` ì„œë¸Œì…‹ì€ ì´ 46ê°œì˜ í”„ë ˆì„ìœ¼ë¡œ êµ¬ì„±ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

<br>

![nyu_door_opening_surprising_effectiveness (PNG)](/assets/img/2025-08-18/nyu_door_opening_surprising_effectiveness.png)

<br>

ë¡œë´‡ íŒ”ì´ ì„œëì˜ ë¬¸ì„ ì—¬ëŠ” íƒœìŠ¤í¬ë¡œ, ì„œëì˜ ì†ì¡ì´ë¥¼ ì¡ì„ ìˆ˜ ìˆë„ë¡ ë¡œë´‡ íŒ”ì˜ ìœ„ì¹˜ì™€ ê°ë„ë¥¼ ì¡°ì •í•˜ê³  ë¬¸ì„ ì§‘ì–´ì„œ ë‹¹ê¸°ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.

<br>

![nyu_door_opening_surprising_effectiveness (GIF)](/assets/img/2025-08-18/nyu_door_opening_surprising_effectiveness.gif)

<br>

ì´ëŸ¬í•œ ì´ë¯¸ì§€ì™€ ì§€ì‹œë¬¸ì„ VLA ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì£¼ë©´ ë¡œë´‡ì˜ í–‰ë™ì„ ëª¨ë¸ì˜ ì¶œë ¥ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆë‹¤. ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œëŠ” OpenVLA ëª¨ë¸ì„ ë¡œë´‡ íƒœìŠ¤í¬ì— ì§ì ‘ ì ìš©í•´ ë³´ë„ë¡ í•œë‹¤.

<br>

## Another Subdatasets

<br>

* **berkeley_cable_routing** (instruction: b'route cable')

![berkeley_cable_routing (PNG)](/assets/img/2025-08-18/berkeley_cable_routing.png)|![berkeley_cable_routing (GIF)](/assets/img/2025-08-18/berkeley_cable_routing.gif)

<br>

* **toto** (instruction: b'pour')(Time Elapsed: 26:03)

![toto (PNG)](/assets/img/2025-08-18/toto.png)|![toto (GIF)](/assets/img/2025-08-18/toto.gif)

<br>

## Error

<br>

<div class="box-danger">
<div class="title"> (Code 7) RuntimeError: Dataset scripts are no longer supported, but found OpenX-Embodiment. py </div>
Hugging Faceì˜ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—…ë°ì´íŠ¸ë˜ë©´ì„œ ë” ì´ìƒ ë¡œì»¬ Python ìŠ¤í¬ë¦½íŠ¸(.py) ê¸°ë°˜ì˜ ë°ì´í„°ì…‹ ë¡œë”© ë°©ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì—ëŸ¬ì´ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë²„ì „ì„ (datasets==4.0.0ì—ì„œ 3.6.0ìœ¼ë¡œ) ë‹¤ìš´ê·¸ë ˆì´ë“œ í–ˆë‹¤.
<br>
<br>
>> <code>pip install "datasets<4.0.0"</code>
</div>

<br>

---

## References
[1] <https://arxiv.org/abs/2406.09246>  
[2] <https://colab.research.google.com/github/google-deepmind/open_x_embodiment/blob/main/colabs/Open_X_Embodiment_Datasets.ipynb>