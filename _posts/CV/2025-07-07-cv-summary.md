---
title: "Summary"
date: 2025-07-07 16:57:00 +0900
categories: [CV, Multi Modal]
---

&nbsp;

|Year|Model|Modality|Structure|Task
||---|---||
||BERT|Text||
||GPT|Text||
|3 Jun 2021|ViT|Image|
|26 Feb 2021|**CLIP**|Text + Image|Text Encoder: GPT-like Transformer (Tokenizer) <br> Image Encoder: ViT|Zero-shot classification|
|17 Aug 2021|SwinT|||
|19 Jul 2024|**Grounding DINO**|Text + Image|Text Encoder: BERT <br> Image Encoder: SwinT|
||BLIP|||
||BLIP-2|||
||Flamingo|||
||LLaVA|||
|16 Aug 2022|SayCan|||
|11 Aug 2023|RT-1|||
|6 Mar 2023|PaLM-E|||
|28 Jul 2023|RT-2|||
|5 Sep 2024|OpenVLA|||

<br>

### Transformer Model for Language

<br>

* [ ] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
<br> (<https://arxiv.org/abs/1810.04805?source=post_page>)

<br>

* [ ] Improving Language Understanding by Generative Pre-Training
<br> (<https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf>)

<br>

### Transformer Model for Vision

<br>

* [ ] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
<br> (<https://arxiv.org/abs/2010.11929>)

<br>

### Multi Modal Model

<br>

* [ ] Learning Transferable Visual Models From Natural Language Supervision
<br> (<https://arxiv.org/abs/2103.00020>)

<br>

* [ ] LAION-5B: An open large-scale dataset for training next generation image-text models
<br> (<https://arxiv.org/abs/2210.08402>)

<br>

* [ ] End-to-End Object Detection with Transformers
<br> (<https://arxiv.org/abs/2005.12872>)

<br>

* [ ] DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection
<br> (<https://arxiv.org/abs/2203.03605>)

<br>

* [ ] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
<br> (<https://arxiv.org/abs/2103.14030>)

<br>

* [ ] Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection
<br> (<https://arxiv.org/abs/2303.05499>)

<br>

### Vision Language Model

<br>

* [ ] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
<br> (<https://arxiv.org/abs/2201.12086>)

<br>

* [ ] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
<br> (<https://arxiv.org/abs/2301.12597>)

<br>

* [ ] Flamingo: a Visual Language Model for Few-Shot Learning
<br> (<https://arxiv.org/abs/2204.14198>)

<br>

* [ ] Visual Instruction Tuning
<br> (<https://arxiv.org/abs/2304.08485>)

<br>

### Vision Language Action Model

<br>

* [ ] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances
<br> (<https://arxiv.org/abs/2204.01691>)

<br>

* [ ] RT-1: Robotics Transformer for Real-World Control at Scale
<br> (<https://arxiv.org/abs/2212.06817>)

<br>

* [ ] PaLM-E: An Embodied Multimodal Language Model
<br> (<https://arxiv.org/abs/2303.03378>)

<br>

* [ ] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control
<br> (<https://arxiv.org/abs/2307.15818>)

<br>

* [ ] OpenVLA: An Open-Source Vision-Language-Action Model
<br> (<https://arxiv.org/abs/2406.09246>)

<br>

---

<br>

---
