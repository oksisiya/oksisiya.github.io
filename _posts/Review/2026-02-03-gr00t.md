---
title: "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots"
date: 2026-02-03 14:56:00 +0900
categories: [Review]

published: true
use_math: true
---
---

## Abstract

<br>

범용(general-purpose) 로봇은 다재다능한 몸체(versatile body)와 지능적인 두뇌(intelligent mind)를 필요로 한다. 최근 휴머노이드 로봇의 발전은 인간 세상에서 범용 자율성(generalist autonomy)을 구축하기 위한 하드웨어 플랫폼으로서 큰 가능성을 보이고 있다. 방대하고 다양한 데이터 자원으로 학습된 로봇 기초(foundation) 모델은 로봇이 새로운 상황을 추론하고(reason about novel situations), 실제 환경의 변화에 견고하게 대처하며(robustly handle real-world variability), 새로운 작업을 빠르게 학습할(rapidly learn new tasks) 수 있도록 하는 데 필수적이다. 본 논문에서는 휴머노이드 로봇을 위한 개방형 기초 모델(open foundation model)인 GR00T N1을 소개한다. GR00T N1은 이중 시스템(dual-system) 아키텍처를 갖춘 시각-언어-행동(Vision-Language-Action, VLA) 모델이다. 시각-언어 모듈(vision-language module, System 2)은 시각 및 언어 명령(instructions)을 통해 환경을 해석한다. 그 뒤를 잇는 확산 트랜스포머 모듈(diffusion transformer module, System 1)은 실시간으로 유연한 모터 움직임(fluid motor actions)을 생성한다. 두 모듈은 긴밀하게 결합되어 있으며 엔드 투 엔드로(end-to-end) 공동 학습된다. 본 논문에서는 실제 로봇 궤적(real-robot trajectories), 인간 영상(human videos), 그리고 합성으로 생성된 데이터셋(synthetically generated datasets)이 혼합된 이질적인(heterogeneous) 데이터셋을 사용해 GR00T N1을 학습시켰다. 범용 로봇 모델인 GR00T N1이 다양한 로봇 구조(embodiments)에 걸친 표준 시뮬레이션 벤치마크에서 최첨단(SOTA) 모방학습 베이스라인(baselines)보다 우수함을 보였다. 또한 본 모델을 Fourier GR-1 휴머노이드 로봇에 배포해 언어 조건부 양손 조작(language-conditioned bimanual manipulation) 작업을 수행했으며 높은 데이터 효율성과 함께 강력한 성능을 달성했다.

<br>

## Research Background

<br>

## Task Definition

<br>

GR00T N1은 다양한 데이터 자원으로 학습된 휴머노이드 로봇용 VLA 모델이다.

* **A dual-system model design**: 시각-언어 모델(VLM) 기반의 추론(reasoning) 모듈(System 2)과 확산 트랜스포머(DiT) 기반의 움직임(action) 모듈(system 1)을 하나의 통일된(unified) 학습 프레임워크에 통합한다.

* **Heterogeneous training data**: 일반화(generalization) 및 견고성(robustness)을 위해 인간 영상(human video), 시뮬레이션 및 신경망으로 생성된 데이터(similation and neural generated data), 실제 로봇 시연(real robot demonstrations)을 혼합해 사용한다.

* **Multiple robot embodiments**: 다양한 로봇 구조를 지원하며 데이터 효율적인 사후 학습(post-training)을 통해 새로운 작업에 빠르게 적응(rapid adaption)할 수 있도록 한다.

<br>

## Proposed Method

<br>

![Figure 3](/assets/img/2026-02-03-gr00t/figure-3.png)

<br>

#### Model Architecture

<br>

GR00T N1은 플로우 매칭(flowmatching)을 사용해 동작 생성(action generation)을 학습한다. 확산 트랜스포머(DiT)는 로봇의 고유 감각(proprioceptive) 상태와 동작을 처리하고 이를 Eagle-2 VLM 백본의 이미지 및 텍스트 토큰과 교차 처리해(cross-attended) 노이즈가 제거된 모터 동작(denoised motor actions)을 출력한다.

<br>

**State and Action Encoders**

다양한 로봇 구조(embodiments)에 따라 달라지는 차원에 대한 상태와 행동(states and actions)을 처리하기 위해 각 구조마다 별도의 MLP를 사용하고 이를 공유 임베딩(shared embedding) 차원으로 투영함으로써 DiT의 입력으로 사용한다. **Action Encoder MLP**는 노이즈가 포함된 동작 벡터(noised action vector)와 확산 시간 단계(diffusion timestep)를 인코딩한다.

**Action flow matching**을 사용해 반복적인 노이즈 제거(iterative denoising)를 통해 동작을 샘플링한다. 모델은 노이즈가 포함된 행동(noised actions), 로봇의 고유 감각 상태 인코딩(encodings of the robot's proprioceptive state), 이미지 토큰(image tokens), 텍스트 토큰(text token)을 입력으로 받는다. 동작은 청크 단위로(in chunks) 처리된다. 즉 주어진 시간 $t$에서 모델은 시간 단계 $t$부터 $t+H-1$까지의 동작 벡터가 포함된 $𝐴_𝑡 = [𝑎_𝑡, 𝑎_{𝑡+1}, \cdots , 𝑎_{𝑡+𝐻−1}]$을 사용한다.

<br>

**Vision-Language Module (System 2)**

시각 및 언어 입력 인코딩을 위해 인터넷 규모(internet-scale) 데이터로 사전 학습된 Eagle-2 비전-언어 모델(VLM)을 사용한다. 이미지(images)는 224 × 224 해상도(resolution)로 인코딩된 후 픽셀 셔플(pixel shuffle, 2016)을 거쳐 프레임 당 64개의 이미지 토큰 임베딩(image token embeddings)이 생성된다. 이러한 임베딩은 Eagle-2 VLM의 LLM 구성요소(component)를 통해 텍스트와 함께 추가로 인코딩된다. LLM과 이미지 인코더는 광범위한 시각-언어 작업에 걸쳐 정렬된다(aligned).

정책 학습(policy training) 과정 동안 작업에 대한 텍스트 설명(text description)과 (여러 개의) 이미지가 시각-언어 학습에 사용되는 채팅 형식으로(in the chat format) VLM에 전달된다. 그런 다음 LLM으로부터 (배치 크기 × 시퀀스 길이 × 은닉 차원)((batch size × sequence length × hidden dimension)) 형태의 시각-언어 특징(features)을 추출한다. 중간 계층(middle-layer)에 LLM 임베딩을 사용하는 것이 최종 계층(final-layer)에 사용하는 것보다 추론 속도를 빠르게 하고(faster inference speed) 하위 정책 성공률을 높인다는(higher downstream policy success rate) 것을 발견했다.

<br>

**Diffusion Transformer Module (System 1)**

동작 모델링(modeling actions)을 위해 DiT(Peebles and Xie, 2023)의 변형을 사용한다. 이는 적응형 계층 정규화(adaptive layer normalization)를 통한 노이즈 제거(denoising) 단계를 포함하는 트랜스포머이며 $V_\theta$로 표기한다. $V_\theta$는 Flamingo(Alayrac et al., 2022) 및 VIMA(Jiang et al., 2023)와 유사하게 교차 주의(cross-attention) 블록과 자기 주의(self-attention) 블록이 번갈아(alternating) 나타나는 구조로 이루어져 있다.

* **Self-attention blocks**: 상태 임베딩 $q_t$와 함께 노이즈가 포함된 행동 토큰 임베딩 $A_{t}^{\tau}&에 대해 연산을 수행한다.

* **Cross-attention blocks**: VLM에서 출력된 시각-언어 토큰 임베딩 $\phi_t$에 대한 조건화(conditioning)를 허용한다.

마지막 DiT 블록 이후에는 구조별(embodiment-specific) 최종 $H$ 토큰에 행동 디코더(Action Decoder)(또다른 MLP)를 적용해 행동을 예측한다. 

<br>

#### Training Data Generation

<br>

![Figure 1](/assets/img/2026-02-03-gr00t/figure-1.png)

<br>

시뮬레이션 환경

<br>

## Experiment Results

<br>

## Academic Position

<br>

## Reference

[1] <https://arxiv.org/pdf/2503.14734> (paper)