---
title: "DexterityGen: Foundation Controller for Unprecedented Dexterity (6 Feb 2025)"
date: 2025-11-19 12:10:00 +0900
categories: [Review]
use_math: true
---
---

&nbsp;

## Abstract

<br>

**Teaching Robots Dexterous Manipulation Skills**
* Human teleoperation (for imitation learning)
    * **Hard to produce** safe and dexterous motions on a different embodiment **without touch feedback**
* Sim-to-real reinforcement learning
    * Struggles with the **domain gap** and involves **highly task-specific reward engineering** on complex tasks

<br>

**Key Insight**
* RL - effective at **learning low-level motion primitives**
* Humans - exel at **providing coarse motion commands** for complex, long-horizon tasks

⇨ A combination of both approaches

<br>

**DexterityGen (DexGen)**
* **RL to pretrain** large-scale dexterous **motion primitives**
    * Such as in-hand rotation or translation
    * Leverage this learned dataset to train a dexterous foundational controller
* **Human teleoperation as a prompt** to the controller  
&emsp;&nbsp;&nbsp;Produce highly **dexterous behavior** in the real world
* Evaluate the effectiveness of DexGen in both simulation and real world
    * Realize **input dexterous manipulation commands**
    * Improves **stability** by 10-100x measured as **duration of holding objects**
    * Demonstrate **unprecedented dexterous skills** for the first time

<br>

## Existing Approaches: Challenges and Opportunity

<br>

Review the challenges and opportunities with existing approaches to **dexterous manipulation**

<br>

#### Human Teleoperation for Imitation Learning

<br>

**Challenge**
* Sim-to-real gap
    * Difficult to reproduce real-world sensor observation (mainly for vision input) and physics in simulation
    * Make sim-to-real transfer highly challenging for complex tasks

* Reward specificatioon
    * Notorious challenge of designing reward functions for long-horizon, contact-rich problems
    * Highly engineered rewards or overly complicated learning strategies => task-specific and limit scalability


<br>

**Opportunity: High-level (Semantical) Motion Control**

<br>


#### Sim-to-real Reinforcement Learning

<br>

**Challenge**

<br>

**Opportunity: Low-level (Physical) Action Control**


<br>

## The DexGen Controller

<br>

#### Preliminaries

<br>

**Diffusion Models**
* Diffusion Model
* Denoising Diffusion Probabilistic Model (DDPM)

<br>

**Robot System and Notations**
* Drive the robot hand by a widely used PD controller


<br>

$$\tau = K_p(\tilde{q}_t - q_t) - K_d \dot{q}_t$$

<br>

|$\tilde{q}_t$|a joint target position|
|$q_t$|the current joint position|
|$\dot{q}_t$|the joint velocity|
|$K_p$ and $K_d$|constant scalar gains|

<br>

#### Large-Scale Behavior Dataset Generation

<br>

![Figure 5](/assets/img/2025-11-19-dexgen/figure-5.png)

<br>

**Anygrasp-to-Anygrasp**
* Central **pretraining** task to ensure the dataset **can cover a broad range of potential states**
* Captures the essential part of in-hand manipulation, which is to **move the object to arbitrary configurations**
* Definition of the training task
    1. <U>Generate a set of object grasps</U> using Grasp Analysis and Rapidly-exploring Random Tree (RRT)  
        Each generated grasp is efined as a tuple (hand joint position, object pose).
    2. <U>Initialize the object</U> in the hand <U>with a random grasp</U> in each RL rollout
    3. <U>Set the goal</U> to be a randomly selected <U>nearby grasp</U> using the k Nearest-Neighbor search
    4. <U>Update the goal</U> in the same way after reaching the current grasp goal

<br>

![Figure 3](/assets/img/2025-11-19-dexgen/figure-3.png)

<br>

**Details of the RL training**
* Use a diverse set of random objects and wrist poses
* Include random geometrical objects with different physical properties for each task
* Randomly adjust the wrist to different poses throughout the process, in addition to employing commonly used domain randomizations to enhance the robustness of the policy

<br>

**Generating this dataset (by rolling out trained RL policies)**
* Collect a total of 1 × 10¹⁰ transitions as the simulation dataset, equivalent to 31.7 years of real world experience
* Requires 300 GPU hours

<br>

#### DexGen Model Architecture

<br>

![Figure 4](/assets/img/2025-11-19-dexgen/figure-4.png)

<br>

3D keypoint motions in the robot hand frame
* Use as an intermediate action representation
* particularly advantageous for **integrating guidance from human teleoperation**


$$\Delta x \in \mathbb{R}^{T \times K \times 3}$$


<center>($T$: the future horizon, $K$: the number of finger keypoints)</center>

<br>

**Diffusion model**
* Characterizes the **distribution of robot finger keypoint motions** given current observations
* **UNet-based diffusion model** to fit the complex keypoint motion distribution of the multitask dataset
* Learns to generate several **future finger keypoint offsets** conditioned on the robot **state** at timestep $t$ and a **mode conditioning variable**
    * State: a stack of **historical proprioception** information
    * Mode conditioning variable: a **one-hot vector** to explicitly indicate the **intention of the task**


$$\Delta x_i = x_{t+i} - x_t$$


<br>

Example. <U>Placing an object</U>
* <U>Do not want</U> the model to produce actions that will make <U>the robot hold the object firmly</U>
* <U>Hard to prompt the hand to release</U> the object <U>without introducing a "release object" indicator</U> if most of the actions in the dataset will keep the object in the palm
* In the dataset... labled with
    * A **default** (unconditional) label for the majority of **transitions**
    * A specialized precision **rotation mode** label for **screwdriver**
    * **Disabling DexGen controller** for **releasing obejct**

<br>

**Inverse dynamics model**
* Converts the **keypoint motions** to executable **robot actions** (i.e., target joint position)


$$a_t = \tilde{q}_t$$


* A simple **residual multilayer perceptron**
* Outputs a **normal distribution** to model the actions conditioned on the current robot **state** and **motion command**

<br>

Train these models with
* Generated simulation dataset using
    * **Standard diffusion model loss** function - diffusion model
    * **MSE loss** for regression - inverse dynamics model
* AdamW optimizer for 15 epochs using 96 GPUs, which takes approximately 3 days

<br>

#### Inference: Motion Conditioning with Guided Sampling

<br>

**The goal in this paper**
* Sample a keypoint motion that is both 1) <U>safe (i.e., from the learned distribution $p_\theta(\Delta x \mid o)$)</U> and can 2) <U>maximally preserve the input reference motion</U>
* Can be written as:


$$\Delta x \sim p_\theta(\Delta x \mid o) \, \exp(-\mathrm{Dist}(\Delta x, \Delta x_{\text{input}}))$$


<center>($\Delta x_{\text{input}} \in \mathbb{R}^{K \times 3}$: the input commanded fingertip offset,<br>
$\mathrm{Dist}$: a distance function that quantifies the distance between the predicted sequence and the input reference)</center>


<br>

**Distance function**  
Encourages the generated future fingertip position to **closely match** the commanded fingertip position

$$\mathrm{Dist}(\Delta x, \Delta x_{\text{input}}) = \sum_{i=1}^{T} \|\Delta x_i - \Delta x_{\text{input}}\|^2$$


<br>

**Guided Sampling**
* Naive sampling strategies
    * Computationally intractable
    * Since the action of the robot hand has a high degree of freedom (16 for the Allegro hand)
* **Gradient guidance**
    * **Incorporate motion conditioning** in the diffusion sampling
    * Convert the generated finger keypoint movement to action by the inverse dynamics model
    * Adjust the denoised sample $\Delta x$ by subtracting $\alpha \, \Sigma \, \nabla_{\Delta x} \mathrm{Dist}(\Delta x, \Delta x_{\text{input}})$ as a guide
    <br><br><center>$\Delta x \gets \Delta x - \alpha \, \Sigma \, \nabla_{\Delta x} \mathrm{Dist}(\Delta x, \Delta x_{\text{input}})$</center><center>($\alpha$: a parameter of the strength of the guidance)</center><br>
    * Then converted to action by the inverse dynamics model
* Use DDIM sampler during inference for 10Hz control
* Total sampling time: around 27ms (37Hz) on a Lambda workstation equipped with an NVIDIA RTX 4090 GPU

<br>

## Experiments

<br>

* Validate the effectiveness of DexGen through **simulated experiments** demonstrating its ability to **enhance the robustness and success rate** of extremely suboptimal policies
* Test the system in the **real world** with a focus on its **application in shared autonomy** showing that DexGen can **assist a human operator** in executivng **unprecedented dexterous manipulation skills** with remarkable generalizability

<br>

#### System Setup

<br>

* Allegro Hand  
&emsp;&nbsp;&nbsp;Attached to a Franka-panda robot arm as the **manipulator**
* Retargeting-based system
    * **Control the robot with human hand gestures** in the teleoperation experiments in real world
    * Confidential fast retargeting method that runs at 300Hz (will release in a future report)
* Manus Glove  
&emsp;&nbsp;&nbsp;Capture the human **hand pose**
* Vive tracking system  
&emsp;&nbsp;&nbsp;Obtain the 6D human **wrist pose** (and use it to control the robot arm)

<br>

#### Simulated Experiments

<br>

![Figure 8](/assets/img/2025-11-19-dexgen/figure-8.png)

<br>

**Experimental Setup**
* Test the capability of DexGen in **assisting suboptimal policies** in solving the Anygrasp-to-Anygrasp task in simulation
* Simulate 2 kinds of suboptimal policies with an expert RL policy $\pi_{\exp}$
    * An expert that can perform dangerous suboptimal actions through additive uniform noise
    <br><center>$\pi_{\text{noisy}}(a \mid s) = \pi_{\exp}(a \mid s) + \mathcal{U}(-\alpha, \alpha)$</center>
    * A slowdown version of expert
    <br><center>$\pi_{\text{slow}}(a \mid s) = \mathcal{U}(0, \alpha)\,\pi_{\exp}(a \mid s)$</center>
    * Compare these suboptimal experts $\pi$ to their assisted counterparts $\text{DexGen} \circ \pi$
* Record the **average number of critical failures** (drop the object) and the **number of goal achievements** within a certain time of different policies

<br>

**Main Results**
* **Without** the assistance
    * Much more frequent failures of the noisy expert
    * Can only **hardly achieve** any goals
* **With** the assistance of DexGen
    * Can **partially recover** the performance of the noisy expert
    * Different optimal guidance value for different policies
    * Being a common region working well for all these policies
* When the guidance is **relatively small**
    * Can maintain the object in hand
    * **Can not achieve** the desired goal as well because **DexGen does not know what the goal is**
* When the guidance becomes **too large**
    * Taken over by the potentially suboptimal external motion command
    * Lead to a **lower duration** in some cases

⇨ Demonstrated that the system can **provide effective assistance** through simulated validation


<br>

#### Real World Experiments

<br>

**Benchmarking in the real workd**
* First set: common in-hand dexterous manipulation behavior
    * Ask human teleoperator to **act as an external high-level policy**
    * Evaluate **whether the system can assist humans** to solve diverse dexterous manipulation tasks
    * In-hand object reorientation, functional grasping, in-hand regrasping
* Second set: more realistinc, long-horizon tasks
    * Require the user to **combine the skills above**
    * Screwdriver, syringe

<br>

|Task|Definition|
|---|---|
|In-hand object reorientation|Control the hand to rotate a given object to a specific pose|
|Functional grasping|Perform a power grasp on the tool handle placed either horizontally (normal) <br>or vertically in the air (horizontal functional grasp)|
|In-hand regrasping|Achieve a specific grasp configuration (object pose + finger pose)|
|Screwdriver|Pick up a screwdriver lying on the table and use it to tighten a bolt|
|Syringe|Pick up a syringe and inject some liquid into a target region|

<br>

**Evaluation Protocol**
* Evaluate the **performance of a teleoperation system**
* Measure the **success rate a human user can achieve** when using it to solve certain tasks
* Let users familiarize themselves with each evaluated teleoperation system in 30 minutes
* Two users in this section

<br>

#### Real World Results

<br>

![Table 1](/assets/img/2025-11-19-dexgen/table-1.png)

<br>

**Performance of different approaches**
* Baseline teleoperation system
    * Hardly solve the tasks
    * Drop the object easily during the contact-rich manipulation process
* DexGen
    * Successfully help the user to solve many tasks in various challenging setups
    * Intriguing properties: <u>magnetic effect</u>, <u>intention following</u>

<br>

**Magnetic Effect**
* **Maintain the contact** as if the fingertips are sticking to the object **when the user mistakenly moves** a supporting finger which may drop the object
* Explains why the **user can achieve a much higher success rate in these dexterous tasks**

<br>

**Intention Following**
* **Follow the user's intention (action) well** and move along the user-commanded moving direction although overrides dangerous user action
* Can still have a sense of agency over the robot hand and complete a complex task during the manipulation procedure
* Echoes the simulated result with noisy policies: **DexGen can realize the intention in the noisy suboptimal actions**

<br>

![Table 2](/assets/img/2025-11-19-dexgen/table-2.png)

<br>

**Breakdown analysis of the long-horizon tasks**
* Achievig tool **use**
    * Challenging as it **involves several stages** of complex dexterous
    * Achieve a reasonable stage-wise success rate, but **chaining these skills together is difficult**
* Believe that **improving stage-wise policy** in the future can eventually **close the gap**

<br>

## Related Works

<br>

#### Foundation Models and Pretraining for Robotics

<br>

**Foundation models for robotics**
* Be attracted by the success of large foundation models in natural language processing and computer vision  
* Focus on building a large end-to-end control model by **pretraining them on large real world datasets**

<br>

**DexGen differentiates from these works in ...**
* Exsisting robotic foundation models
    1. Pretraining on **real world datasets** which require extensive human efforts in data collection with teleoperation
    2. Typically consider **parallel jaw gripper problems**
    3. Be conditioned on **discrete language prompts or task embeddings**
* DexGen framework
    1. Pretraining on pure **simulation datasets**
    2. Study **dexterous manipulation with a high DOF robotic hand** and demonstrate the advantage of generative pretraining in this challenging scenario for the first time
    3. Build a low-level foundation controller that can be prompted with **continuous fine-grained guidance** to provide useful actions

⇨ Pretraining framework for building a foundational low-level controller 


<br>

#### Shared Autonomy

<br>

**Shared autonomy research**

Focues on leveraging external action guidance to produce effective actions

<br>

* Some works
    1. Focus on how to train RL agents with external actions (e.g., from teleoperation)
        * In their setup, the external inputs are usually treated as part of observation fed to the RL policy
    2. A limitation in dexterous manipulation
        * Assumes that existence of a few task-specific intentions and goals and reduces the shared autonomy problem to the goal or intent inference
        * Do not allow fine-grained finger control since they only provide a few options for high-dimensional action space
    3. Use some sampled distribution to correct user behavior
* DexGen
    1. Does not involve human actions in the training phase
    2. Samples fine-grained low-level behavior according to user commands in high-dimensional action space
    3. Use a different correction procedure and investigate a more general and challenging dexterous manipulation setup

<br>

## Conclusion

<br>

**DexterityGen**

* Initial attempt towards buildling **a foundational low-level controller for dexterous manipulation**
* Demonstrated that **generative pretraining on diverse multi-task simulated trajectories** yield a powerful generative controller that can **translate coarse motion prompts to effective low-level actions**
* Exibits unprecedented dexterity by combining with external high-level policy

<br>

**Limitations and Future Work**

1. Touch Sensing
    * **Rely on joint angle proprioception** for implicit touch sensing (i.e., **inferring force by reading control errors**)
        * Insufficient and nonrobust for fine-grained problems
        * Impossible to recover contact geometry based on joint angle error
    * **Add touch to pretraining**, which has been shown possible for sim-to-real transfer

2. Vision: Hand-Eye Coordination (leave this to future research)
    * **Does not involve vision**
    * Necessary for producing accurate tool motions for many tasks
    * Should be in the **high-level policy** or part of the proposed **foundation low-level controller**?

3. Real-world Finetuning
    * Deploy the controller in a **zero-shot manner**
    * Necessary to **fine-tune** the controller **with some real world experience**

<br>

---

## References

[1] <https://arxiv.org/abs/2502.04307>