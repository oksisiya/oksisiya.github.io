---
title: "DexterityGen: Foundation Controller for Unprecedented Dexterity (6 Feb 2025)"
date: 2025-11-19 12:10:00 +0900
categories: [Review]
use_math: true
---
---

&nbsp;

## Abstract

<br>

**Teaching Robots Dexterous Manipulation Skills**
* Human teleoperation (for imitation learning)
    * **Hard to produce** safe and dexterous motions on a different embodiment **without touch feedback**
* Sim-to-real reinforcement learning
    * Struggles with the **domain gap** and involves **highly task-specific reward engineering** on complex tasks

<br>

**Key Insight**
* RL - effective at **learning low-level motion primitives**
* Humans - exel at **providing coarse motion commands** for complex, long-horizon tasks

⇨ A combination of both approaches

<br>

**DexterityGen (Dex-Gen)**
* **RL to pretrain** large-scale **dexterous motion primitives**, such as in-hand rotation or translation
* Learned dataset to train a dexterous foundational controller
* **Human teleoperation as a prompt** to the controller to produce highly **dexterous behavior** in the real world
* Evaluate the effectiveness of DexGen in both simulation and real world, demonstrating that ...
    * Realize input dexterous manipulation commands
    * Significantly improves stability by 10-100x measured as duration of holding objects across diverse tasks
* Demonstrate **unprecedented dexterous skills** including diverse object reorientation and dexterous tool use **for the first time**

<br>

## The DexGen Controller

<br>

#### Preliminaries

<br>

**Diffusion Models**
* Diffusion Model
* Denoising Diffusion Probabilistic Model (DDPM)

<br>

**Robot System and Notations**
* Drive the robot hand by a widely used PD controller


<br>

$$\tau = K_p(\tilde{q}_t - q_t) - K_d \dot{q}_t$$

<br>

|$\tilde{q}_t$|a joint target position|
|$q_t$|the current joint position|
|$\dot{q}_t$|the joint velocity|
|$K_p$ and $K_d$|constant scalar gains|

<br>

#### Large-Scale Behavior Dataset Generation

<br>

![Figure 5](/assets/img/2025-11-19-dexgen/figure-5.png)

<br>

**Anygrasp-to-Anygrasp**
* Central **pretraining** task to ensure the dataset **can cover a broad range of potential states**
* Captures the essential part of in-hand manipulation, which is to **move the object to arbitrary configurations**
* Definition of the training task
    1. <U>Generate a set of object grasps</U> using Grasp Analysis and Rapidly-exploring Random Tree (RRT)  
        Each generated grasp is efined as a tuple (hand joint position, object pose).
    2. <U>Initialize the object</U> in the hand <U>with a random grasp</U> in each RL rollout
    3. <U>Set the goal</U> to be a randomly selected <U>nearby grasp</U> using the k Nearest-Neighbor search
    4. <U>Update the goal</U> in the same way after reaching the current grasp goal

<br>

![Figure 3](/assets/img/2025-11-19-dexgen/figure-3.png)

<br>

**Details of the RL training**
* Use a diverse set of random objects and wrist poses
* Include random geometrical objects with different physical properties for each task
* Randomly adjust the wrist to different poses throughout the process, in addition to employing commonly used domain randomizations to enhance the robustness of the policy

<br>

**Generating this dataset (by rolling out trained RL policies)**
* Collect a total of 1 × 10¹⁰ transitions as the simulation dataset, equivalent to 31.7 years of real world experience
* Requires 300 GPU hours

<br>

#### DexGen Model Architecture

<br>

![Figure 4](/assets/img/2025-11-19-dexgen/figure-4.png)

<br>

3D keypoint motions in the robot hand frame
* Use as an intermediate action representation
* particularly advantageous for **integrating guidance from human teleoperation**


$$\Delta x \in \mathbb{R}^{T \times K \times 3}$$


<center>($T$: the future horizon, $K$: the number of finger keypoints)</center>

<br>

**Diffusion model**
* Characterizes the **distribution of robot finger keypoint motions** given current observations
* **UNet-based diffusion model** to fit the complex keypoint motion distribution of the multitask dataset
* Learns to generate several **future finger keypoint offsets** conditioned on the robot **state** at timestep $t$ and a **mode conditioning variable**
    * State: a stack of **historical proprioception** information
    * Mode conditioning variable: a **one-hot vector** to explicitly indicate the **intention of the task**


$$\Delta x_i = x_{t+i} - x_t$$


<br>

Example. <U>Placing an object</U>
* <U>Do not want</U> the model to produce actions that will make <U>the robot hold the object firmly</U>
* <U>Hard to prompt the hand to release</U> the object <U>without introducing a "release object" indicator</U> if most of the actions in the dataset will keep the object in the palm
* In the dataset... labled with
    * A **default** (unconditional) label for the majority of **transitions**
    * A specialized precision **rotation mode** label for **screwdriver**
    * **Disabling DexGen controller** for **releasing obejct**

<br>

**Inverse dynamics model**
* Converts the **keypoint motions** to executable **robot actions** (i.e., target joint position)


$$a_t = \tilde{q}_t$$


* A simple **residual multilayer perceptron**
* Outputs a **normal distribution** to model the actions conditioned on the current robot **state** and **motion command**

<br>

Train these models with
* Generated simulation dataset using
    * **Standard diffusion model loss** function - diffusion model
    * **MSE loss** for regression - inverse dynamics model
* AdamW optimizer for 15 epochs using 96 GPUs, which takes approximately 3 days

<br>

#### Inference: Motion Conditioning with Guided Sampling

<br>

**The goal in this paper**
* Sample a keypoint motion that is both 1) <U>safe (i.e., from the learned distribution $p_\theta(\Delta x \mid o)$)</U> and can 2) <U>maximally preserve the input reference motion</U>
* Can be written as:


$$\Delta x \sim p_\theta(\Delta x \mid o) \, \exp(-\mathrm{Dist}(\Delta x, \Delta x_{\text{input}}))$$


<center>($\Delta x_{\text{input}} \in \mathbb{R}^{K \times 3}$: the input commanded fingertip offset,<br>
$\mathrm{Dist}$: a distance function that quantifies the distance between the predicted sequence and the input reference)</center>


<br>

**Distance function**  
Encourages the generated future fingertip position to **closely match** the commanded fingertip position

$$\mathrm{Dist}(\Delta x, \Delta x_{\text{input}}) = \sum_{i=1}^{T} \|\Delta x_i - \Delta x_{\text{input}}\|^2$$


<br>

**Guided Sampling**
* Naive sampling strategies
    * Computationally intractable
    * Since the action of the robot hand has a high degree of freedom (16 for the Allegro hand)
* **Gradient guidance**
    * **Incorporate motion conditioning** in the diffusion sampling
    * Convert the generated finger keypoint movement to action by the inverse dynamics model
    * Adjust the denoised sample $\Delta x$ by subtracting $\alpha \, \Sigma \, \nabla_{\Delta x} \mathrm{Dist}(\Delta x, \Delta x_{\text{input}})$ as a guide
    <br><br><center>$\Delta x \gets \Delta x - \alpha \, \Sigma \, \nabla_{\Delta x} \mathrm{Dist}(\Delta x, \Delta x_{\text{input}})$</center><center>($\alpha$: a parameter of the strength of the guidance)</center><br>
    * Then converted to action by the inverse dynamics model
* Use DDIM sampler during inference for 10Hz control
* Total sampling time: around 27ms (37Hz) on a Lambda workstation equipped with an NVIDIA RTX 4090 GPU

<br>

## Experiments

<br>

#### System Setup

<br>

#### Simulated Experiments

<br>

![Figure 8](/assets/img/2025-11-19-dexgen/figure-8.png)

<br>

**Experimental Setup**
* Test the capability of DexGen in **assisting suboptimal policies** in solving the Anygrasp-to-Anygrasp task in simulation
* Simulate 2 kinds of suboptimal policies with an expert RL policy $\pi_{\exp}$
    * An expert that can perform dangerous suboptimal actions through additive uniform noise
    <br><center>$\pi_{\text{noisy}}(a \mid s) = \pi_{\exp}(a \mid s) + \mathcal{U}(-\alpha, \alpha)$</center>
    * A slowdown version of expert
    <br><center>$\pi_{\text{slow}}(a \mid s) = \mathcal{U}(0, \alpha)\,\pi_{\exp}(a \mid s)$</center>
    * Compare these suboptimal experts $\pi$ to their assisted counterparts $\text{DexGen} \circ \pi$
* Record the **average number of critical failures** (drop the object) and the **number of goal achievements** within a certain time of different policies

<br>

**Main Results**
* **Without** the assistance
    * Much more frequent failures of the noisy expert
    * Can only **hardly achieve** any goals
* **With** the assistance of DexGen
    * Can **partially recover** the performance of the noisy expert
    * Different optimal guidance value for different policies
    * Being a common region working well for all these policies
* When the guidance is **relatively small**
    * Can maintain the object in hand
    * **Can not achieve** the desired goal as well because **DexGen does not know what the goal is**
* When the guidance becomes **too large**
    * Taken over by the potentially suboptimal external motion command
    * Lead to a **lower duration** in some cases

<br>


te
<br>

## Conclusion

<br>

**DexterityGen**

* Initial attempt towards buildling **a foundational low-level controller for dexterous manipulation**
* Demonstrated that **generative pretraining on diverse multi-task simulated trajectories** yield a powerful generative controller that can **translate coarse motion prompts to effective low-level actions**
* Exibits unprecedented dexterity by combining with external high-level policy

<br>

**Limitations and Future Work**

1. Touch Sensing
    * **Rely on joint angle proprioception** for implicit touch sensing (i.e., inferring force by reading control errors)
        * Insufficient and nonrobust for fine-grained problems
        * Impossible to recover contact geometry based on joint angle error
    * **Add touch to pretraining**, which has been shown possible for sim-to-real transfer

2. Vision: Hand-Eye Coordination (leave this to future research)
    * **Does not involve vision**
    * Necessary for producing accurate tool motions for many tasks
    * Should be in the **high-level policy** or part of the proposed **foundation low-level controller**?

3. Real-world Finetuning
    * Deploy the controller in a **zero-shot manner**
    * Necessary to **fine-tune** the controller **with some real world experience**

&nbsp;

---

## References

[1] <https://arxiv.org/abs/2502.04307>