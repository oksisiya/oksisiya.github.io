---
title: "DexterityGen: Foundation Controller for Unprecedented Dexterity (6 Feb 2025)"
date: 2025-11-19 12:10:00 +0900
categories: [Review]
use_math: true
---

## Abstract

<br>

**Teaching Robots Dexterous Manipulation Skills**
* Human teleoperation (for imitation learning)
    * **Hard to produce** safe and dexterous motions on a different embodiment **without touch feedback**
* Sim-to-real reinforcement learning
    * Struggles with the **domain gap** and involves **highly task-specific reward engineering** on complex tasks

<br>

**Key Insight** 
* Humans - exel at **providing coarse motion commands** for complex, long-horizon tasks
* RL - effective at **learning low-level motion primitives**

â‡¨ A combination of both approaches

<br>

**DexterityGen (Dex-Gen)**
* Uses RL to pretrain large-scale dexterous motion primitives
* Leverage this learned dataset to train a dexterous foundational controller
* Use human teleoperation as a prompt to the controller to produce highly dexterous behavior in the real world
* Evaluate the effectiveness of DexGen in both simulation and real world
    * Realize input dexterous manipulation commands
    * Significantly improves stability by 10-100x measured as duration of holding objects across diverse tasks
* Demonstrate unprecedented dexterous skills including diverse object reorientation and dexterous tool use for the first time

<br>

## The DexGen Controller

<br>

#### Preliminaries

<br>

**Diffusion Models**
* Diffusion Model
* Denoising Diffusion Probabilistic Model (DDPM)

<br>

**Robot System and Notations**
* Drive the robot hand by a widely used PD controller


<br>

$$\tau = K_p(\tilde{q}_t - q_t) - K_d \dot{q}_t$$

<br>

|$\tilde{q}_t$|a joint target position|
|$q_t$|the current joint position|
|$\dot{q}_t$|the joint velocity|
|$K_p$ and $K_d$|constant scalar gains|

<br>

#### Large-Scale Behavior Dataset Generation

<br>



<br>

![Figure 3](/assets/img/2025-11-19-dexgen/figure-3.png)

<br>

#### DexGen Model Architecture

<br>

![Figure 4](/assets/img/2025-11-19-dexgen/figure-4.png)


<br>

**Diffusion model**
* Characterizes the **distribution of robot finger keypoint motions** given current observations
* **UNet-based diffusion model** to fit the complex keypoint motion distribution of the multitask dataset
* Learns to generate several future finger keypoint offsets $\Delta x_i = x_{t+i} - x_t$ conditioned on the robot state at timestep t and a mode conditioning variable
* State: a stack of historical proprioception information
* Mode conditioning variable: a one-hot vector to explicitly indicate the intention of the task

<br>

Example. <U>Placing an object</U>
* <U>Do not want</U> the model to produce actions that will make <U>the robot hold the object firmly</U>
* <U>Hard to prompt the hand to release</U> the object <U>without introducing a "release object" indicator</U> if most of the actions in the dataset will keep the object in the palm
* In the dataset... labled with
    * A **default** (unconditional) label for the majority of **transitions**
    * A specialized precision **rotation** mode label for **screwdriver**
    * **Disabling DexGen controller** for **releasing obejct**

<br>

**Inverse dynamics model**
* Converts the keypoint motions to executable robot actions (i.e., target joint position)
* A simple **residual multilayer perceptron**
* Outputs a **normal distribution** to model the actions conditioned on the current robot state and motion command

<br>

|3D keypoint motions|$\Delta x \in \mathbb{R}^{T \times K \times 3}$|$T$: the future horizon<br>$K$: the number of finger keypoints|


<br>

Train these models with
* Generated simulation dataset using
    * Standard diffusion model loss function - diffusion model
    * MSE loss for regression - inverse dynamics model
* AdamW optimizer for 15 epochs using 96 GPUs, which takes approximately 3 days

<br>

## Experiments

<br>

#### System Setup

<br>

#### Simulated Experiments

<br>


te
<br>

## Conclusion

<br>

**DexterityGen**

* Initial attempt towards buildling **a foundational low-level controller for dexterous manipulation**
* Demonstrated that **generative pretraining on diverse multi-task simulated trajectories** yield a powerful generative controller that can **translate coarse motion prompts to effective low-level actions**
* Exibits unprecedented dexterity by combining with external high-level policy

<br>

**Limitations and Future Work**

1. Touch Sensing
    * **Rely on joint angle proprioception** for implicit touch sensing (i.e., inferring force by reading control errors)
        * Insufficient and nonrobust for fine-grained problems
        * Impossible to recover contact geometry based on joint angle error
    * **Add touch to pretraining**, which has been shown possible for sim-to-real transfer

2. Vision: Hand-Eye Coordination (leave this to future research)
    * **Does not involve vision**
    * Necessary for producing accurate tool motions for many tasks
    * Should be in the **high-level policy** or part of the proposed **foundation low-level controller**?

3. Real-world Finetuning
    * Deploy the controller in a **zero-shot manner**
    * Necessary to **fine-tune** the controller **with some real world experience**

&nbsp;

---

## References

[1] <https://arxiv.org/abs/2502.04307>