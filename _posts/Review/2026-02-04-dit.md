---
title: "Scalable Diffusion Models with Transformers"
date: 2026-02-04 14:45:00 +0900
categories: [Review]

published: true
use_math: true
---
---

## Abstract

<br>

본 연구에서는 트랜스포머 구조 기반의 확산 모델(diffusion models)의 새로운 유형을 탐구한다. 이미지의 잠재 확산 모델(latent diffusion models)을 학습시키며 이때 일반적으로 사용되는 U-Net 백본(backbone)을 잠재 패치(latent patches) 상에서 연산하는 트랜스포머로 대체한다(replacing). Gflops로 측정된 순방향 전달 복잡도(forward pass complexity) 관점에서 확산 변환기(Diffusion Transformers, DiTs)의 확장성(scalability)을 분석한다. 트랜스포머의 깊이/너비 증가 또는 입력 토큰 수 증가로 인해 Gflops가 높아진 DiT가 일관적으로 더 낮은 FID를 갖는다는 것을 발견했다. 확장성이 뛰어날 뿐만 아니라 본 연구의 가장 큰 DiT-XL/2 모델은 클래스 조건부(class-conditional) ImageNet 512×512 및 256×256 벤치마크에서 기존의 모든 확산 모델을 능가하며 후자에서는 2.27의 FID(SOTA)를 달성했다.

<br>

## Research Background

<br>

자연어 처리, 컴퓨터 비전 및 기타 여러 분야의 신경망 구조는 대부분 트랜스포머에 통합되었다(subsumed). 이미지 수준(image-level) 생성 모델의 많은 유형들은 이러한 추세(trend)에서 벗어나 있다. 트랜스포머는 자기회귀(autoregressive) 모델에서 널리 사용되지만 다른 생성 모델링 프레임워크에서는 채택률이 낮다. 예를 들어 확산 모델은 최근 이미지 수준 생성 모델의 발전을 선도했지만 모두 사실상 표준 백본으로 합성곱(convolutional) U-Net 구조를 채택하고 있다.

본 논문에서는 확산 모델에서 아키텍처 선택의 중요성을 명확히 밝히고(demystify) 향후 생성 모델링 연구를 위한 실증적 베이스라인(empirical baselines)을 제시한다. U-Net의 귀납적 편향(inductive bias)(me: 모델이 학습 데이터로부터 일반적인 패턴을 추론할 때 특정한 방식으로 추론하려는 경향)이 확산 모델의 성능에 결정적인 영향을 미치지 않으며 트랜스포머와 같은 표준 설계(standard designs)로 쉽게 대체될 수 있음을 보였다.

<br>

## Task Definition

<br>

확산 트랜스포머(DiTs)는 확산 모델을 위한 트랜스포머 기반의 백본(transformer-based backbone)이다.

<br>

## Proposed Method

<br>

## Experiment Results

<br>

## Academic Position

<br>

## Reference

[1] <https://arxiv.org/pdf/2212.09748> (paper)