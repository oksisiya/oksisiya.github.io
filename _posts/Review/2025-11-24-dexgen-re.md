---
title: "DexGen Q&A"
date: 2025-11-24 16:17:00 +0900
categories: [Review]
use_math: true
---
---

## Q1. Diffusion model에서 "guided"의 의미  

(cmt. Condition을 넣어주는 것이다. Diffusion model 스스로가 어떤 데이터를 생성할지 제어하지 못하기 때문이다. 로봇 제어 분야에서 적용할 때는 task나 constraint 등을 condition으로 넣어주어야 한다.)

<br>

A1. 생성 모델이 아무 행동이나 하지 않도록 조건(condition)을 넣어주는 것이다.
* (Fig. 2 캡션에서) 추론 중에는 teleoperation이나 policy가 만들어낸 위험한 움직임을 **guided sampling을 통해 안전한 동작으로** 되돌릴 수 있다. (☞ During inference, we can project dangerous motion produced by teleoperation or policy back to <span style="color:blue">a high-likelihood action with guided sampling</span>.)
* (Fig. 4 캡션에서) **Motion conditioning**은 diffusion model에 직접 입력되는 것이 아니라 **diffusion sampling 과정에서 gradient guidance로서 사용**된다. (☞ The <span style="color:blue">motion conditioning</span> is not fed into the diffusion model directly but <span style="color:blue">as the gradient guidance during the diffusion sampling</span>.)
* (The DexGen Controller에서) 로봇 손의 행동이 매우 높은 자유도를 갖기 때문에 단순한 sampling 전략은 계산적으로 다루기 힘들어진다. 이를 해결하기 위해 본 논문에서는 motion conditioning을 확립하기 위해 diffusion sampling 과정에서 gradient guidance를 사용하는 것을 제안한다. 각 diffusion 단계에서 노이즈가 제거된 샘플 $\Delta x$에서 **$\alpha \Sigma \nabla_{\Delta x} \mathrm{Dist}(\Delta x, \Delta x_{\text{input}})$를 guide로서 빼서** 조정한다. (☞ Since the action of the robot hand has a high degree of freedom (16 for the Allegro hand used in this paper), naive sampling strategies become computationally intractable. To address this, we propose using gradient guidance in the diffusion sampling process to incorporate motion conditioning. In each diffusion step, we adjust the denoised sample $\Delta x$ <span style="color:blue">by sbtracting $\alpha \Sigma \nabla_{\Delta x} \mathrm{Dist}(\Delta x, \Delta x_{\text{input}})$ as a guide</span>.

<br>

> Fig. 2: Overview of proposed framework. Left (Training): We collect a large multi-task dexterous in-hand manipulation dataset in simulation to pretrain a generative model that can generate diverse actions conditioned on the current state. ... Right (Inference): During inference, we can project dangerous motion produced by teleoperation or policy back to a high-likelihood action with guided sampling.
>
![Figure 2](/assets/img/2025-11-19-dexgen/figure-2.png)

<br>

> Fig. 4: Model: Architecture of the DexGen controller.
>
![Figure 4](/assets/img/2025-11-19-dexgen/figure-4.png)

<br>

## Q2. 본 논문에서 RL을 사용하는 이유?

(cmt. Diffusion 모델을 학습시킬 때 필요한 경로 정보를 RL로 학습시켜서 찾는 것 같다.)

<br>

A2. DexGen이 학습할 기초 동작 데이터셋을 생성하기 위해 사용된다.
* (Abstract에서) RL은 DexGen이 **손 안에서의 회전이나 이동과 같은 대규모의 정교한 기초 동작들**을 사전 학습하기 위해 사용된다. (☞ In this paper, we introduce DexterityGen (DexGen), which uses RL to pretrain <span style="color:blue">large-scale dexterous motion premitives, such as in-hand rotation or translation<span style="color:blue">.)
* (Introduction에서) RL은 **DexGen을 사전 학습시킬 시뮬레이션 데이터셋을 생성하기 위해** 사용된다. (☞ Our main idea is to use a broad, multitask <span style="color:blue">simulation dataset generated via RL to pretrain a generative behavior model (DexGen)</span> that can translate a coarse motion command to safe robot actions which can maximally preserve the motion while guaranteeing safety.)
* (Abstract에서) RL이 **저수준 기초 동작들을 학습하는 데 효과적**이기 때문이다. (☞ Our key insight is that RL is <span style="color:blue">effective at learning low-level motion primitives</span>, while humans excel at providing coarse motion commands for complex, long-horizon tasks.)
* (Introduction에서) 최근의 sim-to-real RL 연구들에서는 **손 안에서의 정교한 물체 조작을 위한 기초 움직임들을 학습시키고 이를 실제 로봇으로 전이하는 것이 가능함**을 보였다. 이는 **RL을 통해 대규모의 정교한 기초 움직임 데이터셋을 생성할 수 있음**을 시사한다. (☞ Specifically, recent sim-to-real RL works have shown that it is <span style="color:blue">possible to train simple dexterous in-hand object manipulation primitives (e.g. rotation) that can be transferred to a robot in the real world</span>. This suggests that <span style="color:blue">RL can be leveraged to generate a large-scale dataset of dexterous manipulation primitives</span>, including in-hand object rotation, translation, and grasp transitions.)

<br>

## **Q3. DexGen 기법을 단일로 사용할 수 있는가? (or just 보조 수단인가?)**

<br>

A3. DexGen은 human teleoperation의 보조 수단으로서 사용된다. (DexGen만 단독으로 사용하지 않는다.)
* (제목에서) DexGen을 전례 없는 섬세한 조작 능력을 위한 **기초 제어기**라고 표현하고 있다. (☞ DEXTERITY GEN: <span style="color:blue">Foundation Controller</span> for Unprecedented Dexterity)
* (Fig. 1 캡션에서) DexGen은 **외부 policy가 생성한 안전하지 않고 거친 움직임 명령을 안전하고 정교한 동작으로 변환**할 수 있는 생성 모델이라고 표현하고 있다. (☞ DexGen is a generative model that can <span style="color:blue">translate an unsafe, coarse motion command produced by external policy to safe and fine actions</span>.)
* (Abstract에서) DexGen이 **입력된 정교한 조작 명령을 실현**하고 안전성을 크게 향상시킬 수 있는 범용적인 제어기라고 표현하고 있다. (☞ We evaluate the effectiveness of DexGen in both simulation and real world, demonstrating that it is a general-purpose controller that can <span style="color:blue">realize input dexterous manipulation commands</span> and significantly improves stability by 10-100x measured as duration of holding objects across diverse tasks.)
* (Introduction에서) DexGen은 **거친 움직임 명령을** 최대한 그 움직임을 보존하고 안전성을 보장하면서 **안전한 로봇 동작으로 변환**하는 생성형 행동 모델이라고 표현하고 있다. (☞ Our main idea is to use a broad, multitask simulation dataset generated via RL to pretrain a generative behavior model (DexGen) that can <span style="color:blue">translate a coarse motion command to safe robot</span> actions which can maximally preserve the motion while guaranteeing safety.)
* (TABLE I 캡션에서) DexGen은 **teleoperation policy가 다양한 환경에서 안정성과 성공률을 동시에 달성할 수 있도록 돕는** 역할을 수행한다는 것을 알 수 있다. 실험에서는 기본적인 teleoperation과 DexGen이 보조한 teleoperation을 비교하고 있다. (☞ The raw teleoperation baseline fails completely on those tasks, while our method can <span style="color:blue">help the teleoperation policy to achieve both stability and success in diverse setups</span>.)

<br>

> TABLE 1: Performance of evaluated methods on the real-world tasks.
>
![Table 1](/assets/img/2025-11-19-dexgen/table-1.png)

<br>

## **Q4. DexGen은 human teleoperation을 어떻게 보조하는가? ("assist"의 의미)**

(cmt. 본 논문에서 사용한 Allegro Hand는 자유도가 높고 무선 장비이다. 또 위치로 ~ 하기 때문에 2지 그리퍼(2-finger gripper)에 비해 노이즈가 쉽게 발생할 수 있다. 그런 것들을 보정하려고 하는 건지...)

<br>

A4. 사람이 제공한 거친 명령을 안전하고 정밀한 행동으로 변환함으로써 teleoperation을 보조한다.
* 

<br>

## **Q5. 각 task에 mode conditioning / motion conditioning 은 어떻게 입력되는가?**

<br>

## **Q6. Distance function은 무엇이고 학습 시 어떻게 활용되는가?**

<br>

A6. 입력으로 들어온 동작 명령과 DexGen이 출력한 행동 사이의 거리(유사성)을 측정하는 함수이고 이 거리를 최소화하는 방향으로(= 동작을 보존하는 방향으로) 모델을 학습시킨다.
* (Introduction에서) DexGen은 **거친 움직임 명령을 최대한 그 모션 명령을 보존한 안전한 로봇 동작으로 변환**한다. (☞ Our main idea is to use a broad, multitask simulation dataset generated via RL to pretrain a generative behavior model (DexGen) that can <span style="color:blue">translate a coarse motion command to safe robot actions which can maximally preserve the motion</span> while guaranteeing safety.)
* (The DexGen Controller에서) 본 논문의 목표는 안전하면서 **입력 레퍼런스 움직임을 최대한 보존하는 keypoint motion을 sampling** 하는 것이다. (☞ Our goal is to <span style="color:blue">sample a keypoint motion</span> that is both safe (i.e. from our learned distribution $p_{\theta}(\Delta x \mid o)\$) and can <span style="color:blue">maximally preserve the input reference motion</span>.)
* (The DexGen Controller에서) 거리 함수는 **예측된 시퀀스와 입력 레퍼런스 사이의 거리를 정량화**한다. (☞ $\mathrm{Dist}$ is a distance function that <span style="color:blue">quantifies the distance between the predicted sequence and the input reference</span>.)
* 논문에서 사용한 거리 함수:
<center>$\mathrm{Dist}(\Delta x, \Delta x_{\text{input}}) = \sum_{i=1}^{T} \|\Delta x_i - \Delta x_{\text{input}}\|^2$</center>

* (The DexGen Controller에서) 위 함수는 **generated future fingertip position이 commanded fingertip position과 매우 가깝게 일치하도록 유도**한다. (☞ The above function <span style="color:blue">encourages the generated future fingertip position to closely match the commanded fingertip position</span>.)

<br>

## **Q7. 본 논문의 최종적 목표가 무엇인가?**

<br>

A7. 사람이 제공한 거친(high-level) 명령을 안전하고 정밀하게(low-level) 실행할 수 있는 기초 제어기를 개발하는 것이다.

<br>

## **\<To-Do List\>**

1. 논문 다시 읽기  
2. Diffusion 공부  
3. Policy 구현 해보기